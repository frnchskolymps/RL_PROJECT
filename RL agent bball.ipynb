{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7faa410",
   "metadata": {},
   "source": [
    "# !!!TEST NEW GAME POV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b3442",
   "metadata": {},
   "source": [
    "## CONFIG AND MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d65e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENU chosen: {'mode': 'Train', 'algorithm': 'Actor-Critic', 'params': {'episodes': 50, 'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.2, 'shooter_speed': 20, 'defender_speed': 20, 'ball_speed': 20}}\n"
     ]
    }
   ],
   "source": [
    "# ----------------- CELL 1: CONFIG & PYGAME MENU -----------------\n",
    "import pygame, sys, os, random, math, time\n",
    "import numpy as np\n",
    "\n",
    "# Basic config defaults (editable in menu)\n",
    "CONFIG = {\n",
    "    \"SCREEN_WIDTH\": 960,\n",
    "    \"SCREEN_HEIGHT\": 640,\n",
    "    \"GRID_SIZE\": 64,             # smaller grid for smoother/finer movement\n",
    "    \"PLAYER_SPEED\": 6,\n",
    "    \"BALL_SPEED\": 14,\n",
    "    \"EPISODES\": 200,\n",
    "    \"ALPHA\": 0.1,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"EPSILON\": 0.2,\n",
    "    \"FPS\": 30,\n",
    "    \"HOOP_COLUMN_OFFSET\": 100   # how far from right edge the hoop column starts\n",
    "}\n",
    "\n",
    "# Menu result container (filled by run_menu())\n",
    "MENU = {\n",
    "    \"mode\": None,         # \"Train\" or \"Play\"\n",
    "    \"algorithm\": None,    # \"Q-Learning\", \"Monte Carlo\", \"Actor-Critic\"\n",
    "    \"params\": {}          # filled with numeric params\n",
    "}\n",
    "\n",
    "# Helper: text input in pygame to enter numeric values\n",
    "def pygame_numeric_input(screen, caption, initial=\"\", pos=(120,300), max_chars=10):\n",
    "    font = pygame.font.SysFont(None, 28)\n",
    "    clock = pygame.time.Clock()\n",
    "    val = initial\n",
    "    active = True\n",
    "    while active:\n",
    "        for ev in pygame.event.get():\n",
    "            if ev.type == pygame.QUIT:\n",
    "                pygame.quit(); sys.exit()\n",
    "            if ev.type == pygame.KEYDOWN:\n",
    "                if ev.key == pygame.K_RETURN:\n",
    "                    return val if val!=\"\" else None\n",
    "                elif ev.key == pygame.K_ESCAPE:\n",
    "                    return None\n",
    "                elif ev.key == pygame.K_BACKSPACE:\n",
    "                    val = val[:-1]\n",
    "                else:\n",
    "                    ch = ev.unicode\n",
    "                    if (ch.isdigit() or ch == \".\") and len(val) < max_chars:\n",
    "                        val += ch\n",
    "        screen.fill((12,12,12))\n",
    "        title = font.render(caption, True, (220,220,220))\n",
    "        screen.blit(title, (pos[0], pos[1]-50))\n",
    "        pygame.draw.rect(screen, (200,200,200), (pos[0]-6, pos[1]-6, 440, 40), 2)\n",
    "        txt = font.render(val, True, (255,255,255))\n",
    "        screen.blit(txt, (pos[0], pos[1]))\n",
    "        hint = font.render(\"Enter=OK  Esc=Cancel\", True, (180,180,180))\n",
    "        screen.blit(hint, (pos[0], pos[1]+60))\n",
    "        pygame.display.flip()\n",
    "        clock.tick(30)\n",
    "\n",
    "# Main menu UI: choose Train / Play / Quit; if Train choose algorithm and numeric params\n",
    "def run_menu():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((640, 420))\n",
    "    pygame.display.set_caption(\"Basketball RL - Menu\")\n",
    "    font = pygame.font.SysFont(None, 36)\n",
    "    small = pygame.font.SysFont(None, 22)\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    options = [\"Train\", \"Play\", \"Quit\"]\n",
    "    sel = 0\n",
    "    while True:\n",
    "        screen.fill((18,18,18))\n",
    "        draw = lambda txt, pos, c: screen.blit(font.render(txt, True, c), pos)\n",
    "        draw(\"Basketball RL - Menu\", (160, 30), (255,220,120))\n",
    "        for i, o in enumerate(options):\n",
    "            color = (255,180,120) if i==sel else (220,220,220)\n",
    "            draw(o, (260, 120 + i*60), color)\n",
    "        draw(\"Use ↑/↓ to move, Enter to select\", (180, 320), (180,180,180))\n",
    "        pygame.display.flip()\n",
    "\n",
    "        for ev in pygame.event.get():\n",
    "            if ev.type == pygame.QUIT:\n",
    "                pygame.quit(); sys.exit()\n",
    "            if ev.type == pygame.KEYDOWN:\n",
    "                if ev.key == pygame.K_UP:\n",
    "                    sel = (sel - 1) % len(options)\n",
    "                elif ev.key == pygame.K_DOWN:\n",
    "                    sel = (sel + 1) % len(options)\n",
    "                elif ev.key == pygame.K_RETURN:\n",
    "                    choice = options[sel]\n",
    "                    if choice == \"Quit\":\n",
    "                        pygame.quit(); sys.exit()\n",
    "                    MENU[\"mode\"] = choice\n",
    "                    break\n",
    "        if MENU[\"mode\"] is not None:\n",
    "            break\n",
    "        clock.tick(30)\n",
    "\n",
    "    # If Train -> choose algorithm\n",
    "    if MENU[\"mode\"] == \"Train\":\n",
    "        algos = [\"Q-Learning\", \"Monte Carlo\", \"Actor-Critic\"]\n",
    "        sel = 0\n",
    "        choosing = True\n",
    "        while choosing:\n",
    "            screen.fill((12,12,30))\n",
    "            screen.blit(font.render(\"Choose Algorithm\", True, (200,200,240)), (200,60))\n",
    "            for i, a in enumerate(algos):\n",
    "                color = (120,255,140) if i==sel else (200,200,200)\n",
    "                screen.blit(font.render(a, True, color), (200, 150 + i*50))\n",
    "            screen.blit(small.render(\"Use ↑/↓ Enter to select\", True, (180,180,180)), (200, 320))\n",
    "            pygame.display.flip()\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    pygame.quit(); sys.exit()\n",
    "                if ev.type == pygame.KEYDOWN:\n",
    "                    if ev.key == pygame.K_UP:\n",
    "                        sel = (sel - 1) % len(algos)\n",
    "                    elif ev.key == pygame.K_DOWN:\n",
    "                        sel = (sel + 1) % len(algos)\n",
    "                    elif ev.key == pygame.K_RETURN:\n",
    "                        MENU[\"algorithm\"] = algos[sel]\n",
    "                        choosing = False\n",
    "            clock.tick(30)\n",
    "\n",
    "        # Parameter editor\n",
    "        params = {}\n",
    "        screen.fill((10,10,10)); pygame.display.flip()\n",
    "        val = pygame_numeric_input(screen, \"Episodes (int):\", str(CONFIG[\"EPISODES\"]), pos=(120,220))\n",
    "        params[\"episodes\"] = int(val) if val and val.isdigit() else CONFIG[\"EPISODES\"]\n",
    "\n",
    "        val = pygame_numeric_input(screen, \"Alpha (learning rate):\", str(CONFIG[\"ALPHA\"]), pos=(120,220))\n",
    "        try: params[\"alpha\"] = float(val) if val is not None else CONFIG[\"ALPHA\"]\n",
    "        except: params[\"alpha\"] = CONFIG[\"ALPHA\"]\n",
    "\n",
    "        val = pygame_numeric_input(screen, \"Gamma (discount):\", str(CONFIG[\"GAMMA\"]), pos=(120,220))\n",
    "        try: params[\"gamma\"] = float(val) if val is not None else CONFIG[\"GAMMA\"]\n",
    "        except: params[\"gamma\"] = CONFIG[\"GAMMA\"]\n",
    "\n",
    "        val = pygame_numeric_input(screen, \"Epsilon (exploration):\", str(CONFIG[\"EPSILON\"]), pos=(120,220))\n",
    "        try: params[\"epsilon\"] = float(val) if val is not None else CONFIG[\"EPSILON\"]\n",
    "        except: params[\"epsilon\"] = CONFIG[\"EPSILON\"]\n",
    "\n",
    "        val = pygame_numeric_input(screen, \"Shooter speed (int):\", str(CONFIG[\"PLAYER_SPEED\"]), pos=(120,220))\n",
    "        params[\"shooter_speed\"] = int(val) if val and val.isdigit() else CONFIG[\"PLAYER_SPEED\"]\n",
    "\n",
    "        val = pygame_numeric_input(screen, \"Defender speed (int):\", str(CONFIG[\"PLAYER_SPEED\"]), pos=(120,220))\n",
    "        params[\"defender_speed\"] = int(val) if val and val.isdigit() else CONFIG[\"PLAYER_SPEED\"]\n",
    "\n",
    "        val = pygame_numeric_input(screen, \"Ball speed (int):\", str(CONFIG[\"BALL_SPEED\"]), pos=(120,220))\n",
    "        params[\"ball_speed\"] = int(val) if val and val.isdigit() else CONFIG[\"BALL_SPEED\"]\n",
    "\n",
    "        MENU[\"params\"] = params\n",
    "\n",
    "    # If Play -> ask whether Defender should be manual (arrow keys) or use agent (trained)\n",
    "    elif MENU[\"mode\"] == \"Play\":\n",
    "        screen.fill((10,10,10)); pygame.display.flip()\n",
    "        choosing = True\n",
    "        font_small = pygame.font.SysFont(None, 26)\n",
    "        manual = False\n",
    "        while choosing:\n",
    "            screen.fill((14,14,14))\n",
    "            screen.blit(font.render(\"Play Mode\", True, (200,220,200)), (240, 60))\n",
    "            screen.blit(font_small.render(\"Defender manual? (Arrow keys)  Y = Yes / N = No (use trained agent if available)\", True, (200,200,200)), (50, 150))\n",
    "            pygame.display.flip()\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    pygame.quit(); sys.exit()\n",
    "                if ev.type == pygame.KEYDOWN:\n",
    "                    if ev.key == pygame.K_y:\n",
    "                        manual = True; choosing = False\n",
    "                    elif ev.key == pygame.K_n:\n",
    "                        manual = False; choosing = False\n",
    "        MENU[\"params\"] = {\n",
    "            \"manual_defender\": manual,\n",
    "            \"shooter_speed\": CONFIG[\"PLAYER_SPEED\"],\n",
    "            \"defender_speed\": CONFIG[\"PLAYER_SPEED\"],\n",
    "            \"ball_speed\": CONFIG[\"BALL_SPEED\"]\n",
    "        }\n",
    "\n",
    "    print(\"MENU chosen:\", MENU)\n",
    "    pygame.quit()\n",
    "    return MENU\n",
    "\n",
    "def draw_text(screen, text, pos, font, color=(255,255,255)):\n",
    "    screen.blit(font.render(text, True, color), pos)\n",
    "\n",
    "MENU = run_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9103edd",
   "metadata": {},
   "source": [
    "## ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3980678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 2: BasketballEnv -----------------\n",
    "import pygame, os, math, random\n",
    "import numpy as np\n",
    "\n",
    "class BasketballEnv:\n",
    "    def __init__(self, cfg, render_fps=None):\n",
    "        self.cfg = cfg\n",
    "        self.W = cfg[\"SCREEN_WIDTH\"]\n",
    "        self.H = cfg[\"SCREEN_HEIGHT\"]\n",
    "        self.grid = cfg[\"GRID_SIZE\"]   # make smaller in CFG for smoother movement\n",
    "        self.hoop_x = self.W - cfg[\"HOOP_COLUMN_OFFSET\"]\n",
    "        self.player_speed_default = cfg[\"PLAYER_SPEED\"]\n",
    "        self.ball_speed_default = cfg[\"BALL_SPEED\"]\n",
    "        self.render_fps = render_fps if render_fps else cfg[\"FPS\"]\n",
    "\n",
    "        # Pygame init and screen (do not call quit here)\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.W, self.H))\n",
    "        pygame.display.set_caption(\"Basketball RL - Game\")\n",
    "\n",
    "        # Load images if available (placeholders otherwise)\n",
    "        def load_or_placeholder(path, size, label):\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    img = pygame.image.load(path).convert_alpha()\n",
    "                    return pygame.transform.smoothscale(img, size)\n",
    "                except:\n",
    "                    pass\n",
    "            surf = pygame.Surface(size, pygame.SRCALPHA)\n",
    "            surf.fill((40,40,40))\n",
    "            pygame.draw.rect(surf, (200,200,200), surf.get_rect(), 2)\n",
    "            f = pygame.font.SysFont(None, 18)\n",
    "            surf.blit(f.render(label, True, (220,220,220)), (4,4))\n",
    "            return surf\n",
    "\n",
    "        # Default placeholders\n",
    "        self.court_img = load_or_placeholder(\"halfcourt.png\", (self.W, self.H), \"Court\")\n",
    "        self.shooter_img = load_or_placeholder(\"shooter.png\", (44,44), \"Shooter\")\n",
    "        self.defender_img = load_or_placeholder(\"defender (2).png\", (44,44), \"Defender\")\n",
    "        self.hoop_img = load_or_placeholder(\"ring.png\", (64,24), \"Hoop\")\n",
    "        self.ball_img = load_or_placeholder(\"basketball.png\", (18,18), \"Ball\")\n",
    "\n",
    "        # Entity rectangles (positions)\n",
    "        self.episode = 0   # will be set by training loop\n",
    "        self.reset()\n",
    "\n",
    "        # --- Global cumulative stats across episodes ---\n",
    "        self.global_shooter_score = 0\n",
    "        self.global_defender_blocks = 0\n",
    "        self.global_reward = 0\n",
    "\n",
    "        # Font\n",
    "        self.font = pygame.font.SysFont(None, 24)\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, shooter_pos=None, defender_pos=None, shooter_speed=None, defender_speed=None, ball_speed=None):\n",
    "        self.shooter_speed = shooter_speed if shooter_speed is not None else self.player_speed_default\n",
    "        self.defender_speed = defender_speed if defender_speed is not None else self.player_speed_default\n",
    "        self.ball_speed = ball_speed if ball_speed is not None else self.ball_speed_default\n",
    "\n",
    "        self.shooter = pygame.Rect(80, self.H//2 - 22, 44, 44)\n",
    "        self.defender = pygame.Rect(self.W//2, self.H//2 - 22, 44, 44)\n",
    "        if shooter_pos: self.shooter.topleft = shooter_pos\n",
    "        if defender_pos: self.defender.topleft = defender_pos\n",
    "\n",
    "        # Ball follows shooter until shot\n",
    "        self.ball_x = self.shooter.centerx\n",
    "        self.ball_y = self.shooter.centery\n",
    "        self.ball_in_motion = False\n",
    "        self.ball_vel_x = 0\n",
    "\n",
    "        # Per-episode stats (reset each ep)\n",
    "        self.shooter_score = 0\n",
    "        self.defender_blocks = 0\n",
    "\n",
    "        self.done = False\n",
    "        self.last_reward = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Discrete state representation\n",
    "        sx = max(0, min(self.W - 1, self.shooter.x)) // self.grid\n",
    "        sy = max(0, min(self.H - 1, self.shooter.y)) // self.grid\n",
    "        dx = max(0, min(self.W - 1, self.defender.x)) // self.grid\n",
    "        dy = max(0, min(self.H - 1, self.defender.y)) // self.grid\n",
    "        ball = 1 if self.ball_in_motion else 0\n",
    "        return (sx, sy, dx, dy, ball)\n",
    "\n",
    "    def _constrain_positions(self):\n",
    "        max_shooter_x = self.hoop_x - 60\n",
    "        if self.shooter.x > max_shooter_x:\n",
    "            self.shooter.x = max_shooter_x\n",
    "        max_def_x = self.hoop_x - 20\n",
    "        if self.defender.x > max_def_x:\n",
    "            self.defender.x = max_def_x\n",
    "\n",
    "        for r in [self.shooter, self.defender]:\n",
    "            r.x = max(0, min(self.W - r.width, r.x))\n",
    "            r.y = max(0, min(self.H - r.height, r.y))\n",
    "\n",
    "    def step(self, action_defender, action_shooter=None, shoot=False):\n",
    "        self.last_reward = 0\n",
    "        self.done = False\n",
    "\n",
    "        # Move shooter\n",
    "        if action_shooter == \"UP\":\n",
    "            self.shooter.y -= self.shooter_speed\n",
    "        elif action_shooter == \"DOWN\":\n",
    "            self.shooter.y += self.shooter_speed\n",
    "        elif action_shooter == \"LEFT\":\n",
    "            self.shooter.x -= self.shooter_speed\n",
    "        elif action_shooter == \"RIGHT\":\n",
    "            self.shooter.x += self.shooter_speed\n",
    "\n",
    "        # Ball follows shooter if not in motion\n",
    "        if not self.ball_in_motion:\n",
    "            self.ball_x = self.shooter.centerx\n",
    "            self.ball_y = self.shooter.centery\n",
    "\n",
    "        # Move defender\n",
    "        if action_defender == \"UP\":\n",
    "            self.defender.y -= self.defender_speed\n",
    "        elif action_defender == \"DOWN\":\n",
    "            self.defender.y += self.defender_speed\n",
    "        elif action_defender == \"LEFT\":\n",
    "            self.defender.x -= self.defender_speed\n",
    "        elif action_defender == \"RIGHT\":\n",
    "            self.defender.x += self.defender_speed\n",
    "\n",
    "        self._constrain_positions()\n",
    "\n",
    "        # Shooting logic\n",
    "        if shoot and not self.ball_in_motion:\n",
    "            self.ball_in_motion = True\n",
    "            self.ball_x = self.shooter.centerx\n",
    "            self.ball_y = self.shooter.centery\n",
    "            self.ball_vel_x = self.ball_speed\n",
    "\n",
    "        if self.ball_in_motion:\n",
    "            self.ball_x += self.ball_vel_x\n",
    "\n",
    "            # Check block\n",
    "            dist_x = abs(self.defender.centerx - self.ball_x)\n",
    "            dist_y = abs(self.defender.centery - self.ball_y)\n",
    "            if dist_x < 36 and dist_y < 36: \n",
    "                self.last_reward = +10\n",
    "                self.defender_blocks += 1\n",
    "                self.global_defender_blocks += 1\n",
    "                self.global_reward += self.last_reward\n",
    "                self.ball_in_motion = False\n",
    "                self.done = True\n",
    "\n",
    "            # Check score only if ball passes hoop *and* within vertical range\n",
    "            elif self.ball_x >= self.hoop_x:\n",
    "                hoop_y = self.H//2 - self.hoop_img.get_height()//2\n",
    "                hoop_top = hoop_y\n",
    "                hoop_bottom = hoop_y + self.hoop_img.get_height()\n",
    "                if hoop_top <= self.ball_y <= hoop_bottom:\n",
    "                    self.last_reward = -10\n",
    "                    self.shooter_score += 1\n",
    "                    self.global_shooter_score += 1\n",
    "                    self.global_reward += self.last_reward\n",
    "                else:\n",
    "                    self.last_reward = 0  # miss = no reward\n",
    "                self.ball_in_motion = False\n",
    "                self.done = True\n",
    "\n",
    "        return self._get_state(), self.last_reward, self.done, {\n",
    "            \"blocks\": self.defender_blocks,\n",
    "            \"shooter_score\": self.shooter_score\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        self.screen.blit(self.court_img, (0,0))\n",
    "        hoop_y = self.H//2 - self.hoop_img.get_height()//2\n",
    "        self.screen.blit(self.hoop_img, (self.hoop_x, hoop_y))\n",
    "        self.screen.blit(self.shooter_img, (self.shooter.x, self.shooter.y))\n",
    "        self.screen.blit(self.defender_img, (self.defender.x, self.defender.y))\n",
    "\n",
    "        if self.ball_in_motion:\n",
    "            self.screen.blit(self.ball_img, (int(self.ball_x - self.ball_img.get_width()/2),\n",
    "                                             int(self.ball_y - self.ball_img.get_height()/2)))\n",
    "        else:\n",
    "            self.screen.blit(self.ball_img, (self.shooter.centerx - self.ball_img.get_width()//2,\n",
    "                                             self.shooter.centery - self.ball_img.get_height()//2))\n",
    "\n",
    "        # --- SCOREBOARD TEXT (CUMULATIVE) ---\n",
    "        txt = (f\"Episode: {self.episode}  |  \"\n",
    "               f\"Blocks (Total): {self.global_defender_blocks}  |  \"\n",
    "               f\"Shooter Score (Total): {self.global_shooter_score}  |  \"\n",
    "               f\"Reward (Total): {self.global_reward}\")\n",
    "        score_surf = self.font.render(txt, True, (255,255,255))\n",
    "        self.screen.blit(score_surf, (16,16))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.render_fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251759b2",
   "metadata": {},
   "source": [
    "## ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41e853df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 3: RL ALGORITHMS -----------------\n",
    "import math, random\n",
    "from collections import defaultdict\n",
    "\n",
    "ACTION_LIST = [\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\",\"STAY\",\"SHOOT\"]\n",
    "# For defender agent we will not include SHOOT action - defender's actions are movement & stay:\n",
    "DEF_ACTIONS = [\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\",\"STAY\"]\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    "        self.alpha = params.get(\"alpha\", 0.1)\n",
    "        self.gamma = params.get(\"gamma\", 0.99)\n",
    "        self.epsilon = params.get(\"epsilon\", 0.2)\n",
    "        self.q = defaultdict(lambda: np.zeros(len(DEF_ACTIONS)))  # tabular\n",
    "\n",
    "    def state_key(self, s):\n",
    "        # s is the tuple from env._get_state()\n",
    "        return tuple(s)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        key = self.state_key(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(DEF_ACTIONS)\n",
    "        vals = self.q[key]\n",
    "        return DEF_ACTIONS[int(np.argmax(vals))]\n",
    "\n",
    "    def learn_step(self, state, action, reward, next_state):\n",
    "        sk = self.state_key(state)\n",
    "        nk = self.state_key(next_state)\n",
    "        a_idx = DEF_ACTIONS.index(action)\n",
    "        best_next = np.max(self.q[nk])\n",
    "        td = reward + self.gamma * best_next - self.q[sk][a_idx]\n",
    "        self.q[sk][a_idx] += self.alpha * td\n",
    "\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    "        self.returns = defaultdict(list)\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(DEF_ACTIONS)))\n",
    "        self.gamma = params.get(\"gamma\", 0.99)\n",
    "        self.epsilon = params.get(\"epsilon\", 0.2)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(DEF_ACTIONS)\n",
    "        return DEF_ACTIONS[int(np.argmax(self.Q[state]))]\n",
    "\n",
    "    def learn_episode(self, episode):  # episode = list of (s,a,r)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for s,a,r in reversed(episode):\n",
    "            G = self.gamma * G + r\n",
    "            if (s,a) not in visited:\n",
    "                self.returns[(s,a)].append(G)\n",
    "                self.Q[s][DEF_ACTIONS.index(a)] = np.mean(self.returns[(s,a)])\n",
    "                visited.add((s,a))\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    "        self.actor = defaultdict(lambda: np.zeros(len(DEF_ACTIONS)))  # preferences\n",
    "        self.critic = defaultdict(float)\n",
    "        self.alpha = params.get(\"alpha\", 0.001)\n",
    "        self.beta = params.get(\"beta\", 0.01)\n",
    "        self.gamma = params.get(\"gamma\", 0.99)\n",
    "        self.epsilon = params.get(\"epsilon\", 0.2)\n",
    "\n",
    "    def softmax(self, prefs):\n",
    "        ex = np.exp(prefs - np.max(prefs))\n",
    "        probs = ex / np.sum(ex)\n",
    "        return probs\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        prefs = self.actor[state]\n",
    "        probs = self.softmax(prefs)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(DEF_ACTIONS)\n",
    "        return np.random.choice(DEF_ACTIONS, p=probs)\n",
    "\n",
    "    def learn_step(self, state, action, reward, next_state):\n",
    "        s = state; ns = next_state\n",
    "        a_idx = DEF_ACTIONS.index(action)\n",
    "        td_target = reward + self.gamma * self.critic[ns]\n",
    "        td_error = td_target - self.critic[s]\n",
    "        # update critic\n",
    "        self.critic[s] += self.beta * td_error\n",
    "        # update actor preferences\n",
    "        prefs = self.actor[s]\n",
    "        probs = self.softmax(prefs)\n",
    "        for i in range(len(prefs)):\n",
    "            if i == a_idx:\n",
    "                prefs[i] += self.alpha * td_error * (1 - probs[i])\n",
    "            else:\n",
    "                prefs[i] -= self.alpha * td_error * probs[i]\n",
    "        self.actor[s] = prefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c40ee1",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "123ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 4: TRAINING LOOP -----------------\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def scripted_shooter_policy(env):\n",
    "    \"\"\"\n",
    "    Simple scripted shooter for training:\n",
    "    - Move up/down randomly, sometimes move toward hoop then shoot when forward enough.\n",
    "    - Returns tuple (action_shooter_str, shoot_boolean)\n",
    "    \"\"\"\n",
    "    shoot_prob = 0.08\n",
    "    move = random.choice([\"STAY\",\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\",\"STAY\"])\n",
    "    if random.random() < 0.25:\n",
    "        move = \"RIGHT\"\n",
    "    shoot = False\n",
    "    if env.shooter.centerx > env.hoop_x - 220 and random.random() < shoot_prob:\n",
    "        shoot = True\n",
    "    return move, shoot\n",
    "\n",
    "def train(env, agent, episodes=200, render=True):\n",
    "    rewards_history = []\n",
    "\n",
    "    # keep cumulative stats here (not reset every env.reset)\n",
    "    total_shooter_score = 0\n",
    "    total_defender_blocks = 0\n",
    "    cumulative_reward = 0   # <-- NEW: running reward tracker\n",
    "\n",
    "    for ep in range(1, episodes+1):\n",
    "        # reset env (but do NOT wipe global counters)\n",
    "        env.reset(shooter_speed=MENU[\"params\"].get(\"shooter_speed\"),\n",
    "                  defender_speed=MENU[\"params\"].get(\"defender_speed\"),\n",
    "                  ball_speed=MENU[\"params\"].get(\"ball_speed\"))\n",
    "\n",
    "        env.episode = ep  # ensure correct episode sync\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        mc_episode = []\n",
    "\n",
    "        while not done:\n",
    "            s = env._get_state()\n",
    "            action_def = agent.choose_action(s)\n",
    "\n",
    "            action_shooter, shoot = scripted_shooter_policy(env)\n",
    "\n",
    "            next_s, reward, done, info = env.step(\n",
    "                action_defender=action_def,\n",
    "                action_shooter=action_shooter,\n",
    "                shoot=shoot\n",
    "            )\n",
    "            ep_reward += reward\n",
    "            cumulative_reward += reward   # <-- Add to running total\n",
    "\n",
    "            if isinstance(agent, QLearningAgent):\n",
    "                agent.learn_step(s, action_def, reward, next_s)\n",
    "            elif isinstance(agent, ActorCriticAgent):\n",
    "                agent.learn_step(s, action_def, reward, next_s)\n",
    "            elif isinstance(agent, MonteCarloAgent):\n",
    "                mc_episode.append((s, action_def, reward))\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    pygame.quit(); sys.exit()\n",
    "\n",
    "        if isinstance(agent, MonteCarloAgent) and mc_episode:\n",
    "            agent.learn_episode(mc_episode)\n",
    "\n",
    "        rewards_history.append(ep_reward)\n",
    "\n",
    "        # update cumulative totals\n",
    "        total_shooter_score += env.shooter_score\n",
    "        total_defender_blocks += env.defender_blocks\n",
    "\n",
    "        # --- Notebook logging output (sync with UI scoreboard) ---\n",
    "        print(f\"Episode {ep}/{episodes} | \"\n",
    "              f\"Shooter Total Score: {total_shooter_score} | \"\n",
    "              f\"Defender Total Blocks: {total_defender_blocks} | \"\n",
    "              f\"Reward: {cumulative_reward} | \"   # <-- running reward\n",
    "              f\"Last Reward: {env.last_reward}\")  # <-- per-step reward\n",
    "\n",
    "        if ep % 20 == 0:\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # plot rewards after training\n",
    "    plt.figure()\n",
    "    plt.plot(rewards_history)\n",
    "    plt.title(\"Rewards per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "\n",
    "    return agent, rewards_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95e073",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "351c6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: Actor-Critic for 50 episodes\n",
      "Episode 1/50 | Shooter Total Score: 0 | Defender Total Blocks: 0 | Reward: 0 | Last Reward: 0\n",
      "Episode 2/50 | Shooter Total Score: 0 | Defender Total Blocks: 0 | Reward: 0 | Last Reward: 0\n",
      "Episode 3/50 | Shooter Total Score: 0 | Defender Total Blocks: 0 | Reward: 0 | Last Reward: 0\n",
      "Episode 4/50 | Shooter Total Score: 0 | Defender Total Blocks: 0 | Reward: 0 | Last Reward: 0\n",
      "Episode 5/50 | Shooter Total Score: 0 | Defender Total Blocks: 0 | Reward: 0 | Last Reward: 0\n",
      "Episode 6/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: -10\n",
      "Episode 7/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 8/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 9/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 10/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 11/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 12/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 13/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 14/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 15/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 16/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 17/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 18/50 | Shooter Total Score: 1 | Defender Total Blocks: 0 | Reward: -10 | Last Reward: 0\n",
      "Episode 19/50 | Shooter Total Score: 2 | Defender Total Blocks: 0 | Reward: -20 | Last Reward: -10\n",
      "Episode 20/50 | Shooter Total Score: 2 | Defender Total Blocks: 0 | Reward: -20 | Last Reward: 0\n",
      "Episode 21/50 | Shooter Total Score: 2 | Defender Total Blocks: 0 | Reward: -20 | Last Reward: 0\n",
      "Episode 22/50 | Shooter Total Score: 2 | Defender Total Blocks: 0 | Reward: -20 | Last Reward: 0\n",
      "Episode 23/50 | Shooter Total Score: 2 | Defender Total Blocks: 0 | Reward: -20 | Last Reward: 0\n",
      "Episode 24/50 | Shooter Total Score: 2 | Defender Total Blocks: 0 | Reward: -20 | Last Reward: 0\n",
      "Episode 25/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 10\n",
      "Episode 26/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 27/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 28/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 29/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 30/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 31/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 32/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 33/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 34/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 35/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 36/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 37/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 38/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 39/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 40/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 41/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 42/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n",
      "Episode 43/50 | Shooter Total Score: 2 | Defender Total Blocks: 1 | Reward: -10 | Last Reward: 0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chescake\\anaconda3\\envs\\olympiaCOM222ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- CELL 5: MAIN RUNNER -----------------\n",
    "import pygame\n",
    "\n",
    "# Create environment\n",
    "env = BasketballEnv(CONFIG)\n",
    "\n",
    "trained_agent = None\n",
    "\n",
    "if MENU[\"mode\"] == \"Train\":\n",
    "    # instantiate the chosen agent\n",
    "    params = {\n",
    "        \"alpha\": MENU[\"params\"].get(\"alpha\", CONFIG[\"ALPHA\"]),\n",
    "        \"gamma\": MENU[\"params\"].get(\"gamma\", CONFIG[\"GAMMA\"]),\n",
    "        \"epsilon\": MENU[\"params\"].get(\"epsilon\", CONFIG[\"EPSILON\"])\n",
    "    }\n",
    "    alg = MENU[\"algorithm\"]\n",
    "    print(f\"Starting training: {alg} for {MENU['params'].get('episodes')} episodes\")\n",
    "    if alg == \"Q-Learning\":\n",
    "        agent = QLearningAgent(env, {\"alpha\":params[\"alpha\"], \"gamma\":params[\"gamma\"], \"epsilon\":params[\"epsilon\"]})\n",
    "    elif alg == \"Monte Carlo\":\n",
    "        agent = MonteCarloAgent(env, {\"gamma\":params[\"gamma\"], \"epsilon\":params[\"epsilon\"]})\n",
    "    elif alg == \"Actor-Critic\":\n",
    "        agent = ActorCriticAgent(env, {\"alpha\":params[\"alpha\"], \"gamma\":params[\"gamma\"], \"epsilon\":params[\"epsilon\"], \"beta\":0.01})\n",
    "    else:\n",
    "        raise ValueError(\"Unknown algorithm\")\n",
    "\n",
    "    trained_agent, rewards = train(env, agent, episodes=MENU[\"params\"].get(\"episodes\", 100), render=True)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # After training you can proceed to manual Play or stop. We'll keep trained_agent in variable.\n",
    "\n",
    "elif MENU[\"mode\"] == \"Play\":\n",
    "    manual_def = MENU[\"params\"].get(\"manual_defender\", False)\n",
    "    print(\"Starting Play mode. Controls: Shooter = WASD, Shoot=SPACE. Defender = Arrow keys (if manual) or Agent (if trained).\")\n",
    "    # If an agent was trained earlier in this session, use it; otherwise fallback to random agent\n",
    "    if trained_agent is None:\n",
    "        # create a simple random Q agent (no training) as fallback\n",
    "        trained_agent = QLearningAgent(env, {\"alpha\":0.1,\"gamma\":0.99,\"epsilon\":0.0})\n",
    "\n",
    "    running = True\n",
    "    clock = pygame.time.Clock()\n",
    "    while running:\n",
    "        s = env._get_state()\n",
    "        # handle events\n",
    "        move_shooter = None\n",
    "        shoot_flag = False\n",
    "        move_defender = None\n",
    "        for ev in pygame.event.get():\n",
    "            if ev.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if ev.type == pygame.KEYDOWN:\n",
    "                if ev.key == pygame.K_SPACE:\n",
    "                    shoot_flag = True\n",
    "\n",
    "        keys = pygame.key.get_pressed()\n",
    "        # Shooter (human) WASD\n",
    "        if keys[pygame.K_w]: move_shooter = \"UP\"\n",
    "        elif keys[pygame.K_s]: move_shooter = \"DOWN\"\n",
    "        elif keys[pygame.K_a]: move_shooter = \"LEFT\"\n",
    "        elif keys[pygame.K_d]: move_shooter = \"RIGHT\"\n",
    "        else: move_shooter = \"STAY\"\n",
    "\n",
    "        # Defender manual or agent\n",
    "        if manual_def:\n",
    "            if keys[pygame.K_UP]: move_defender = \"UP\"\n",
    "            elif keys[pygame.K_DOWN]: move_defender = \"DOWN\"\n",
    "            elif keys[pygame.K_LEFT]: move_defender = \"LEFT\"\n",
    "            elif keys[pygame.K_RIGHT]: move_defender = \"RIGHT\"\n",
    "            else: move_defender = \"STAY\"\n",
    "        else:\n",
    "            # agent action\n",
    "            move_defender = trained_agent.choose_action(s)\n",
    "\n",
    "        # step env\n",
    "        next_s, reward, done, info = env.step(action_defender=move_defender, action_shooter=move_shooter, shoot=shoot_flag)\n",
    "        env.render(episode=None)\n",
    "\n",
    "        if done:\n",
    "            # Print episode outcome to notebook output (you requested prints go only to cell output)\n",
    "            print(f\"Play round result -> reward: {reward} | Blocks(this round): {info.get('blocks')} | Shooter score(this round): {info.get('shooter_score')}\")\n",
    "            # short pause then reset\n",
    "            time.sleep(0.6)\n",
    "            env.reset()\n",
    "        clock.tick(CONFIG[\"FPS\"])\n",
    "\n",
    "    pygame.quit()\n",
    "else:\n",
    "    raise ValueError(\"MENU mode not set correctly. Re-run Cell 1.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympiaCOM222ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
