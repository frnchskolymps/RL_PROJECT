{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3066cb63",
   "metadata": {},
   "source": [
    "# PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9aaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (1.2.1)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipywidgets) (5.1.1)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (72.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\chescake\\anaconda3\\envs\\olympiacom222ml\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3a805",
   "metadata": {},
   "source": [
    "# LOCKANDKEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e17e48",
   "metadata": {},
   "source": [
    "## 1 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f294dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Imports & helpers\n",
    "# Paste in first cell\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Import your external env (must be in same folder)\n",
    "from lock_key_env import LockKeyEnv\n",
    "\n",
    "# Pygame used for menu + visualization\n",
    "import pygame\n",
    "\n",
    "# Small safe initializer used before creating fonts/displays\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "    if not pygame.font.get_init():\n",
    "        pygame.font.init()\n",
    "\n",
    "# small helper: moving average\n",
    "def moving_avg(arr, window):\n",
    "    if len(arr) < window:\n",
    "        return np.array(arr)\n",
    "    return np.convolve(arr, np.ones(window)/window, mode='valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9178f7",
   "metadata": {},
   "source": [
    "## 2 RL AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ebb7e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 2: RL Agents (Q-Learning, Monte Carlo, Actor-Critic) -----------------\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Utility to make states hashable ---\n",
    "def hashable_state(s):\n",
    "    \"\"\"\n",
    "    Convert dict, ndarray, or list observation into a hashable key.\n",
    "    \"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return tuple(sorted((k, hashable_state(v)) for k, v in s.items()))\n",
    "    elif isinstance(s, np.ndarray):\n",
    "        return tuple(map(float, s.flatten()))\n",
    "    elif isinstance(s, (list, tuple)):\n",
    "        return tuple(map(hashable_state, s))\n",
    "    else:\n",
    "        try:\n",
    "            hash(s)\n",
    "            return s\n",
    "        except TypeError:\n",
    "            return str(s)\n",
    "\n",
    "\n",
    "# ----------------- Q-Learning Agent -----------------\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states=None, n_actions=4, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)\n",
    "        current = self.Q[state][action]\n",
    "        self.Q[state][action] = current + self.alpha * (target - current)\n",
    "\n",
    "\n",
    "# ----------------- Monte Carlo Agent -----------------\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, n_states=None, n_actions=4, gamma=0.9, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, episode):\n",
    "        \"\"\"Episode is a list of (state, action, reward).\"\"\"\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            state = hashable_state(state)\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                self.returns[(state, action)].append(G)\n",
    "                self.Q[state][action] = np.mean(self.returns[(state, action)])\n",
    "                visited.add((state, action))\n",
    "\n",
    "\n",
    "# ----------------- Actor-Critic Agent -----------------\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, n_states=None, n_actions=4, alpha=0.1, beta=0.01, gamma=0.9):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # critic lr\n",
    "        self.beta = beta    # actor lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.V = defaultdict(float)\n",
    "        self.pi = defaultdict(lambda: np.ones(self.n_actions) / self.n_actions)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        probs = self.pi[state]\n",
    "        return np.random.choice(self.n_actions, p=probs)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "\n",
    "        # TD error\n",
    "        td_target = reward + self.gamma * self.V[next_state] * (1 - done)\n",
    "        td_error = td_target - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "\n",
    "        # Actor update\n",
    "        probs = self.pi[state]\n",
    "        one_hot = np.zeros_like(probs)\n",
    "        one_hot[action] = 1.0\n",
    "        self.pi[state] += self.beta * td_error * (one_hot - probs)\n",
    "\n",
    "        # Normalize\n",
    "        self.pi[state] = np.clip(self.pi[state], 1e-5, 1.0)\n",
    "        self.pi[state] /= np.sum(self.pi[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf779",
   "metadata": {},
   "source": [
    "## 3 PYGAME MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65e30676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Pygame menu & parameter UI (paste into third cell)\n",
    "# Usage: algo, params = menu_and_params()\n",
    "def draw_button(screen, rect, text, font, color=(100,200,150), border=2):\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, border)\n",
    "    txt = font.render(text, True, (0,0,0))\n",
    "    tx = rect.x + (rect.width - txt.get_width())//2\n",
    "    ty = rect.y + (rect.height - txt.get_height())//2\n",
    "    screen.blit(txt, (tx, ty))\n",
    "\n",
    "def menu_and_params(initial_params=None):\n",
    "    pygame_safe_init()\n",
    "    w,h = 980, 620\n",
    "    screen = pygame.display.set_mode((w,h))\n",
    "    pygame.display.set_caption('Locke N Key — Choose Algorithm & Parameters')\n",
    "    font = pygame.font.SysFont('arial', 22)\n",
    "    small = pygame.font.SysFont('arial', 16)\n",
    "\n",
    "    algo_choices = ['Q-Learning', 'Monte Carlo', 'Actor-Critic']\n",
    "    algo_idx = 0\n",
    "    params = {\n",
    "        'episodes': 800,\n",
    "        'gamma': 0.9,\n",
    "        'epsilon': 0.2,\n",
    "        'alpha': 0.1,\n",
    "        'actor_lr': 0.001,\n",
    "        'critic_lr': 0.005,\n",
    "        'phase': 5\n",
    "    }\n",
    "    if initial_params:\n",
    "        params.update(initial_params)\n",
    "\n",
    "    input_active = None\n",
    "    input_text = ''\n",
    "    clock = pygame.time.Clock()\n",
    "    start_sim = False\n",
    "\n",
    "    boxes = {\n",
    "        'episodes': (50,220,220,44),\n",
    "        'gamma':    (300,220,160,44),\n",
    "        'epsilon':  (480,220,160,44),\n",
    "        'alpha':    (50,300,160,44),\n",
    "        'phase':    (240,300,120,44),\n",
    "        'actor_lr': (380,300,160,44),\n",
    "        'critic_lr':(560,300,160,44)\n",
    "    }\n",
    "\n",
    "    # placeholder style vars (easy to change)\n",
    "    bg_color = (245,245,245)\n",
    "    box_color = (255,255,255)\n",
    "    title_color = (30,30,30)\n",
    "\n",
    "    running = True\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit(); raise SystemExit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if input_active is not None:\n",
    "                    if event.key == pygame.K_RETURN:\n",
    "                        try:\n",
    "                            val = float(input_text) if '.' in input_text else int(input_text)\n",
    "                            params[input_active] = val\n",
    "                        except:\n",
    "                            pass\n",
    "                        input_active = None; input_text = ''\n",
    "                    elif event.key == pygame.K_BACKSPACE:\n",
    "                        input_text = input_text[:-1]\n",
    "                    else:\n",
    "                        input_text += event.unicode\n",
    "                else:\n",
    "                    if event.key == pygame.K_RIGHT:\n",
    "                        algo_idx = (algo_idx + 1) % len(algo_choices)\n",
    "                    elif event.key == pygame.K_LEFT:\n",
    "                        algo_idx = (algo_idx - 1) % len(algo_choices)\n",
    "                    elif event.key == pygame.K_RETURN:\n",
    "                        start_sim = True\n",
    "                    elif event.key == pygame.K_ESCAPE:\n",
    "                        pygame.quit(); raise SystemExit()\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                for k,rect in boxes.items():\n",
    "                    x,y,wbox,hbox = rect\n",
    "                    if x <= mx <= x+wbox and y <= my <= y+hbox:\n",
    "                        input_active = k\n",
    "                        input_text = str(params.get(k, ''))\n",
    "                        break\n",
    "\n",
    "        screen.fill(bg_color)\n",
    "        title = font.render('Locke N Key — Choose Algorithm and Parameters', True, title_color)\n",
    "        screen.blit(title, (40,30))\n",
    "\n",
    "        # algorithm selector as big button\n",
    "        draw_button(screen, pygame.Rect(50,80,520,80), f'<<  {algo_choices[algo_idx]}  >>', font)\n",
    "\n",
    "        info = small.render('LEFT/RIGHT to change algorithm. Click parameter boxes to edit. ENTER to start training. ESC to quit.', True, (10,10,10))\n",
    "        screen.blit(info, (50,180))\n",
    "\n",
    "        # draw parameter boxes and labels\n",
    "        # boxes layout (see boxes dict)\n",
    "        for k,rect in boxes.items():\n",
    "            x,y,wbox,hbox = rect\n",
    "            pygame.draw.rect(screen, box_color, (x,y,wbox,hbox))\n",
    "            pygame.draw.rect(screen, (0,0,0), (x,y,wbox,hbox), 1)\n",
    "        # labels\n",
    "        screen.blit(small.render('Episodes (int)', True, (0,0,0)), (50,200))\n",
    "        screen.blit(small.render('Gamma (γ)', True, (0,0,0)), (300,200))\n",
    "        screen.blit(small.render('Epsilon (ε)', True, (0,0,0)), (480,200))\n",
    "        screen.blit(small.render('Alpha (Q lr)', True, (0,0,0)), (50,280))\n",
    "        screen.blit(small.render('Phase (1-5)', True, (0,0,0)), (240,280))\n",
    "        screen.blit(small.render('Actor LR', True, (0,0,0)), (380,280))\n",
    "        screen.blit(small.render('Critic LR', True, (0,0,0)), (560,280))\n",
    "\n",
    "        # values displayed\n",
    "        screen.blit(font.render(str(params['episodes']), True, (0,0,0)), (boxes['episodes'][0]+10, boxes['episodes'][1]+6))\n",
    "        screen.blit(font.render(str(params['gamma']), True, (0,0,0)), (boxes['gamma'][0]+10, boxes['gamma'][1]+6))\n",
    "        screen.blit(font.render(str(params['epsilon']), True, (0,0,0)), (boxes['epsilon'][0]+10, boxes['epsilon'][1]+6))\n",
    "        screen.blit(font.render(str(params['alpha']), True, (0,0,0)), (boxes['alpha'][0]+10, boxes['alpha'][1]+6))\n",
    "        screen.blit(font.render(str(params['phase']), True, (0,0,0)), (boxes['phase'][0]+10, boxes['phase'][1]+6))\n",
    "        screen.blit(font.render(str(params['actor_lr']), True, (0,0,0)), (boxes['actor_lr'][0]+10, boxes['actor_lr'][1]+6))\n",
    "        screen.blit(font.render(str(params['critic_lr']), True, (0,0,0)), (boxes['critic_lr'][0]+10, boxes['critic_lr'][1]+6))\n",
    "\n",
    "        # input editing indicator\n",
    "        if input_active is not None:\n",
    "            pygame.draw.rect(screen, (200,220,255), (40,520,900,72), 3)\n",
    "            screen.blit(small.render(f'Editing: {input_active} -> {input_text}', True, (0,0,0)), (50,540))\n",
    "\n",
    "        start_hint = font.render('Press ENTER to START SIMULATION (training)', True, (180,30,30))\n",
    "        screen.blit(start_hint, (50,420))\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(30)\n",
    "\n",
    "        if start_sim:\n",
    "            pygame.quit()\n",
    "            chosen_algo = algo_choices[algo_idx]\n",
    "            return chosen_algo, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1d161",
   "metadata": {},
   "source": [
    "## 4 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f0a1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 4: Training loop + visualization with HUD -----------------\n",
    "# ✅ CELL 4: Training loop + visualization (final fixed version)\n",
    "\n",
    "def train_and_visualize(algo_name, params, save_path=\"lockkey_policy.pkl\"):\n",
    "    pygame_safe_init()\n",
    "\n",
    "    # --- Environment setup ---\n",
    "    env = LockKeyEnv(render_mode='human', size=6, phase=int(params.get('phase', 5)))\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    episodes = int(params.get('episodes', 800))\n",
    "    gamma = float(params.get('gamma', 0.9))\n",
    "\n",
    "    # --- Agent creation ---\n",
    "    if algo_name == 'Q-Learning':\n",
    "        agent = QLearningAgent(n_states, n_actions,\n",
    "                               alpha=float(params.get('alpha', 0.1)),\n",
    "                               gamma=gamma,\n",
    "                               epsilon=float(params.get('epsilon', 0.2)))\n",
    "    elif algo_name == 'Monte Carlo':\n",
    "        agent = MonteCarloAgent(n_states, n_actions,\n",
    "                                gamma=gamma,\n",
    "                                epsilon=float(params.get('epsilon', 0.1)))\n",
    "    else:  # Actor-Critic\n",
    "        agent = ActorCriticAgent(n_states, n_actions,\n",
    "                                 actor_lr=float(params.get('actor_lr', 1e-3)),\n",
    "                                 critic_lr=float(params.get('critic_lr', 5e-3)),\n",
    "                                 gamma=gamma)\n",
    "\n",
    "    episode_rewards, successes = [], []\n",
    "\n",
    "    pygame_safe_init()\n",
    "    font = pygame.font.SysFont('arial', 18)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        if hasattr(env, 'start_pygame'):\n",
    "            try:\n",
    "                env.start_pygame()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # --- Main training loop ---\n",
    "        for ep in range(1, episodes + 1):\n",
    "            obs = env.reset()\n",
    "            env.current_episode = ep\n",
    "            done = False\n",
    "            total_r = 0\n",
    "            steps = 0\n",
    "            episode_hist = []\n",
    "\n",
    "            while not done and steps < 400:\n",
    "                # Allow window events\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "                    elif event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "\n",
    "                # Agent selects an action\n",
    "                s = obs\n",
    "                a = agent.policy(s) if isinstance(agent, MonteCarloAgent) else agent.select_action(s)\n",
    "                obs2, r, done, trunc, info = env.step(a)\n",
    "                total_r += r\n",
    "                steps += 1\n",
    "\n",
    "                if isinstance(agent, MonteCarloAgent):\n",
    "                    episode_hist.append((s, a, r))\n",
    "                else:\n",
    "                    agent.update(s, a, r, obs2, done)\n",
    "\n",
    "                obs = obs2\n",
    "\n",
    "                # 🟢 Right-side overlay panel (live info)\n",
    "                try:\n",
    "                    screen = env.window\n",
    "                    screen = env.window\n",
    "                    if screen:\n",
    "                        grid_px = env.size * env.cell_size\n",
    "                        panel_width = 220\n",
    "                        total_width = grid_px + panel_width\n",
    "\n",
    "                        # Resize window once if not yet adjusted\n",
    "                        if screen.get_width() < total_width:\n",
    "                            env.window = pygame.display.set_mode((total_width, grid_px))\n",
    "\n",
    "                        # Draw dark side panel\n",
    "                        panel_rect = pygame.Rect(grid_px, 0, panel_width, grid_px)\n",
    "                        pygame.draw.rect(screen, (20, 20, 30), panel_rect)  # darker blue-gray\n",
    "                        pygame.draw.rect(screen, (80, 80, 120), panel_rect, 2)  # subtle border\n",
    "\n",
    "                        lines = [\n",
    "                            f\"Episode: {ep}/{episodes}\",\n",
    "                            f\"Steps: {steps}\",\n",
    "                            f\"Reward: {total_r:.2f}\",\n",
    "                            f\"Epsilon: {getattr(agent, 'epsilon', 0):.2f}\",\n",
    "                            f\"Elapsed: {time.time() - start_time:.1f}s\"\n",
    "                        ]\n",
    "                        y = 30\n",
    "                        for line in lines:\n",
    "                            text_surface = font.render(line, True, (255, 255, 255))\n",
    "                            screen.blit(text_surface, (grid_px + 20, y))\n",
    "                            y += 30\n",
    "\n",
    "                        # Optional live reward bar\n",
    "                        bar_width = int((min(1.0, max(0.0, total_r / 20))) * (panel_width - 40))\n",
    "                        pygame.draw.rect(screen, (0, 200, 0), (grid_px + 20, y + 10, bar_width, 20))\n",
    "\n",
    "                        pygame.draw.rect(screen, (25, 25, 25), panel_rect)  # dark background\n",
    "                        lines = [\n",
    "                            f\"Episode: {ep}/{episodes}\",\n",
    "                            f\"Steps: {steps}\",\n",
    "                            f\"Reward: {total_r:.2f}\",\n",
    "                            f\"Epsilon: {getattr(agent, 'epsilon', 0):.2f}\",\n",
    "                            f\"Elapsed: {time.time() - start_time:.1f}s\"\n",
    "                        ]\n",
    "                        y = 20\n",
    "                        for line in lines:\n",
    "                            text_surface = font.render(line, True, (255, 255, 255))\n",
    "                            screen.blit(text_surface, (env.size * env.cell_size + 10, y))\n",
    "                            y += 25\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Render environment\n",
    "                try:\n",
    "                    env.render(font=font, episode=ep, ep_reward=total_r)\n",
    "                except TypeError:\n",
    "                    env.render()\n",
    "\n",
    "                # 💬 Terminal live updates every few steps\n",
    "                if steps % 40 == 0:\n",
    "                    print(f\"Ep {ep} | Step {steps:03d} | Reward={total_r:.2f}\", end='\\r')\n",
    "\n",
    "            # End of episode updates\n",
    "            if isinstance(agent, MonteCarloAgent):\n",
    "                agent.update(episode_hist)\n",
    "\n",
    "            if hasattr(agent, 'epsilon'):\n",
    "                agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
    "\n",
    "            episode_rewards.append(total_r)\n",
    "            successes.append(1 if done and total_r > 0 else 0)\n",
    "            print(f\"\\n✅ Episode {ep}/{episodes} finished | Reward={total_r:.2f} | Steps={steps}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            env.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Save policy safely ---\n",
    "    try:\n",
    "        def safe_convert(obj):\n",
    "            \"\"\"Recursively convert defaultdicts with lambdas to plain dicts.\"\"\"\n",
    "            if isinstance(obj, defaultdict):\n",
    "                obj = {k: safe_convert(v) for k, v in obj.items()}\n",
    "            return obj\n",
    "\n",
    "        data_to_save = None\n",
    "        if algo_name == 'Q-Learning':\n",
    "            data_to_save = safe_convert(agent.Q)\n",
    "        elif algo_name == 'Monte Carlo':\n",
    "            data_to_save = safe_convert(agent.Q)\n",
    "        else:  # Actor-Critic\n",
    "            data_to_save = {\n",
    "                'V': safe_convert(agent.V),\n",
    "                'pi': safe_convert(agent.pi)\n",
    "            }\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)\n",
    "\n",
    "        print(f\"✅ Saved {algo_name} policy successfully to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not save policy due to {e}\")\n",
    "\n",
    "    # --- Plot training results ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = min(50, max(1, len(successes)))\n",
    "    if len(successes) >= window:\n",
    "        mov = moving_avg(successes, window)\n",
    "        x = np.arange(window - 1, window - 1 + len(mov))\n",
    "        plt.plot(x, mov)\n",
    "    else:\n",
    "        plt.plot(successes)\n",
    "    plt.title(f'Success Rate (moving avg window={window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    plt.show()\n",
    "\n",
    "    return episode_rewards, successes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20c6e5",
   "metadata": {},
   "source": [
    "## 5 RUN WHOLE FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a67d7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen algorithm: Q-Learning\n",
      "Parameters: {'episodes': 800, 'gamma': 0.9, 'epsilon': 0.2, 'alpha': 0.1, 'actor_lr': 0.001, 'critic_lr': 0.005, 'phase': 5}\n",
      "\n",
      "✅ Episode 1/800 finished | Reward=-51.00 | Steps=23\n",
      "Ep 2 | Step 120 | Reward=-263.00\n",
      "✅ Episode 2/800 finished | Reward=-420.00 | Steps=159\n",
      "\n",
      "✅ Episode 3/800 finished | Reward=-12.00 | Steps=12\n",
      "Ep 4 | Step 080 | Reward=-221.00\n",
      "✅ Episode 4/800 finished | Reward=-221.00 | Steps=80\n",
      "Ep 5 | Step 040 | Reward=-94.00\n",
      "✅ Episode 5/800 finished | Reward=-161.00 | Steps=61\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Run the whole flow (paste last)\n",
    "# Run this cell to open the Pygame menu, choose algorithm & params, then train & plot.\n",
    "\n",
    "# 1) show menu\n",
    "algo, params = menu_and_params()\n",
    "print(\"Chosen algorithm:\", algo)\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# 2) optional visuals placeholder (we rely on env.render so keep empty or pass any mapping if your env accepts it)\n",
    "visuals = {\n",
    "    'agent_color': (50,100,255),\n",
    "    'key_color': (255,215,0),\n",
    "    'lock_color': (0,180,0),\n",
    "    'wall_color': (80,80,80),\n",
    "    'enemy_color': (180,0,180),\n",
    "    'bg_color': (230,230,230)\n",
    "}\n",
    "\n",
    "# 3) train & visualize (this will open the env.render Pygame window)\n",
    "rewards, successes = train_and_visualize(algo, params, save_path='lockkey_policy.pkl')\n",
    "print(\"Training complete. Saved policy and plotted results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympiaCOM222ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
