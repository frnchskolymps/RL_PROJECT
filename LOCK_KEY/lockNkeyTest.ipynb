{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3066cb63",
   "metadata": {},
   "source": [
    "# PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9aaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnasium ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3a805",
   "metadata": {},
   "source": [
    "# LOCKANDKEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e17e48",
   "metadata": {},
   "source": [
    "## 1 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f294dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Imports & helpers\n",
    "# Paste in first cell\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Pygame used for menu + visualization\n",
    "import pygame\n",
    "\n",
    "# Small safe initializer used before creating fonts/displays\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "    if not pygame.font.get_init():\n",
    "        pygame.font.init()\n",
    "\n",
    "# small helper: moving average\n",
    "def moving_avg(arr, window):\n",
    "    if len(arr) < window:\n",
    "        return np.array(arr)\n",
    "    return np.convolve(arr, np.ones(window)/window, mode='valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9178f7",
   "metadata": {},
   "source": [
    "## 2 RL AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb7e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: RL Agents (Q-Learning, Monte Carlo, Actor-Critic)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Utility to make states hashable ---\n",
    "def hashable_state(s):\n",
    "    \"\"\"\n",
    "    Convert dict, ndarray, or list observation into a hashable key.\n",
    "    For our env the observation is an integer, so hashing is trivial.\n",
    "    This helper keeps the agent code general.\n",
    "    \"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return tuple(sorted((k, hashable_state(v)) for k, v in s.items()))\n",
    "    elif isinstance(s, np.ndarray):\n",
    "        return tuple(map(float, s.flatten()))\n",
    "    elif isinstance(s, (list, tuple)):\n",
    "        return tuple(map(hashable_state, s))\n",
    "    else:\n",
    "        try:\n",
    "            hash(s)\n",
    "            return s\n",
    "        except TypeError:\n",
    "            return str(s)\n",
    "\n",
    "\n",
    "# ----------------- Q-Learning Agent -----------------\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)\n",
    "        current = self.Q[state][action]\n",
    "        self.Q[state][action] = current + self.alpha * (target - current)\n",
    "\n",
    "\n",
    "# ----------------- Monte Carlo Agent -----------------\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, gamma=0.9, epsilon=0.1, alpha=0.05):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # NEW: learning rate\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, episode):\n",
    "        \"\"\"Episode is a list of (state, action, reward).\"\"\"\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            state = hashable_state(state)\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                old_value = self.Q[state][action]\n",
    "                # Instead of averaging, apply incremental update\n",
    "                self.Q[state][action] = old_value + self.alpha * (G - old_value)\n",
    "                visited.add((state, action))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- Actor-Critic Agent -----------------\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, alpha=0.1, beta=0.01, gamma=0.9):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # critic lr\n",
    "        self.beta = beta    # actor lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.V = defaultdict(float)\n",
    "        self.pi = defaultdict(lambda: np.ones(self.n_actions) / self.n_actions)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        probs = self.pi[state]\n",
    "        return np.random.choice(self.n_actions, p=probs)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "\n",
    "        # TD error\n",
    "        td_target = reward + self.gamma * self.V[next_state] * (1 - done)\n",
    "        td_error = td_target - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "\n",
    "        # Actor update\n",
    "        probs = self.pi[state]\n",
    "        one_hot = np.zeros_like(probs)\n",
    "        one_hot[action] = 1.0\n",
    "        self.pi[state] += self.beta * td_error * (one_hot - probs)\n",
    "\n",
    "        # Normalize\n",
    "        self.pi[state] = np.clip(self.pi[state], 1e-5, 1.0)\n",
    "        self.pi[state] /= np.sum(self.pi[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf779",
   "metadata": {},
   "source": [
    "## 3 PYGAME MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e30676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 3: Stable UI with Accurate Sliders (2-decimal, Updated Beta + MC LR) -----------------\n",
    "def draw_button(screen, rect, text, font, color=(180,220,255), border=2):\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, border)\n",
    "    txt = font.render(text, True, (0,0,0))\n",
    "    screen.blit(txt, (rect.x + (rect.width - txt.get_width())//2, rect.y + (rect.height - txt.get_height())//2))\n",
    "\n",
    "def draw_slider(screen, x, y, w, val, min_val, max_val, label, font, knob_r=10):\n",
    "    \"\"\"\n",
    "    Draws a slider and returns the knob rectangle.\n",
    "    - Ensures knob is computed from a clamped value.\n",
    "    - Displays value with 2 decimals.\n",
    "    \"\"\"\n",
    "    range_span = max_val - min_val if (max_val - min_val) != 0 else 1e-6\n",
    "    clamped_val = max(min_val, min(max_val, float(val)))\n",
    "    frac = (clamped_val - min_val) / range_span\n",
    "    knob_x = int(x + frac * w)\n",
    "\n",
    "    # track\n",
    "    track_rect = pygame.Rect(x, y, w, 6)\n",
    "    pygame.draw.rect(screen, (220,220,220), track_rect)\n",
    "\n",
    "    # knob\n",
    "    pygame.draw.circle(screen, (80,80,200), (knob_x, y+3), knob_r)\n",
    "\n",
    "    # label (2 decimals)\n",
    "    txt = font.render(f\"{label}: {clamped_val:.2f}\", True, (0,0,0))\n",
    "    screen.blit(txt, (x, y - 28))\n",
    "\n",
    "    return pygame.Rect(knob_x - knob_r, y+3 - knob_r, knob_r*2, knob_r*2)\n",
    "\n",
    "def draw_input_box(screen, rect, text, font, active):\n",
    "    color = (200,255,200) if active else (255,255,255)\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, 2)\n",
    "    txt_surface = font.render(str(text), True, (0,0,0))\n",
    "    screen.blit(txt_surface, (rect.x+5, rect.y+5))\n",
    "\n",
    "def menu_and_params(initial_params=None):\n",
    "    pygame_safe_init()\n",
    "    w,h = 980, 640\n",
    "    screen = pygame.display.set_mode((w,h))\n",
    "    pygame.display.set_caption('Locke N Key — Setup (Phase → Algo → Params)')\n",
    "    font = pygame.font.SysFont('arial', 22)\n",
    "    small = pygame.font.SysFont('arial', 16)\n",
    "\n",
    "    algo_choices = ['Q-Learning', 'Monte Carlo', 'Actor-Critic']\n",
    "    algo_idx = 0\n",
    "    phase_idx = 1\n",
    "\n",
    "    # --- Default parameters (MC_LR included) ---\n",
    "    params = {\n",
    "        'episodes': '100',\n",
    "        'alpha': 0.10,\n",
    "        'beta': 0.10,\n",
    "        'gamma': 0.90,\n",
    "        'epsilon': 0.20,\n",
    "        'MC_LR': 0.05,\n",
    "        'phase': phase_idx\n",
    "    }\n",
    "    if initial_params:\n",
    "        params.update(initial_params)\n",
    "\n",
    "    stage = 0\n",
    "    dragging = None\n",
    "    drag_offset = 0\n",
    "    active_box = False\n",
    "    input_rect = pygame.Rect(760, 250, 150, 36)\n",
    "    clock = pygame.time.Clock()\n",
    "    start_sim = False\n",
    "    running = True\n",
    "    knobs = {}\n",
    "\n",
    "    def clamp(v, a, b): return max(a, min(b, v))\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit(); raise SystemExit()\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_RIGHT:\n",
    "                    if stage == 0:\n",
    "                        phase_idx = min(5, phase_idx + 1)\n",
    "                    elif stage == 1:\n",
    "                        algo_idx = (algo_idx + 1) % len(algo_choices)\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    if stage == 0:\n",
    "                        phase_idx = max(1, phase_idx - 1)\n",
    "                    elif stage == 1:\n",
    "                        algo_idx = (algo_idx - 1) % len(algo_choices)\n",
    "                elif event.key == pygame.K_RETURN:\n",
    "                    if stage < 2:\n",
    "                        stage += 1\n",
    "                    else:\n",
    "                        start_sim = True\n",
    "                elif event.key == pygame.K_BACKSPACE:\n",
    "                    if active_box:\n",
    "                        params['episodes'] = params['episodes'][:-1]\n",
    "                    elif stage > 0:\n",
    "                        stage -= 1\n",
    "                elif active_box and event.unicode.isdigit():\n",
    "                    params['episodes'] += event.unicode\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                if input_rect.collidepoint(mx,my):\n",
    "                    active_box = True\n",
    "                else:\n",
    "                    active_box = False\n",
    "\n",
    "                if event.button == 1:\n",
    "                    for k, meta in knobs.items():\n",
    "                        if meta['rect'].collidepoint(mx, my):\n",
    "                            dragging = k\n",
    "                            knob_center_x = meta['rect'].centerx\n",
    "                            drag_offset = mx - knob_center_x\n",
    "                            break\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONUP:\n",
    "                dragging = None\n",
    "                drag_offset = 0\n",
    "\n",
    "            elif event.type == pygame.MOUSEMOTION and dragging:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                meta = knobs.get(dragging)\n",
    "                if meta is None:\n",
    "                    dragging = None\n",
    "                    continue\n",
    "\n",
    "                x, w = meta['x'], meta['w']\n",
    "                min_v, max_v = meta['min'], meta['max']\n",
    "                stick_x = clamp(mx - drag_offset, x, x + w)\n",
    "                rel = (stick_x - x) / float(w) if w != 0 else 0.0\n",
    "                new_val = round(min_v + rel * (max_v - min_v), 2)\n",
    "                new_val = clamp(new_val, min_v, max_v)\n",
    "                params[meta['param_key']] = new_val\n",
    "\n",
    "                knob_x = int(x + ((new_val - min_v) / (max_v - min_v if (max_v-min_v)!=0 else 1e-6)) * w)\n",
    "                knob_r = meta.get('knob_r', 10)\n",
    "                meta_rect = pygame.Rect(knob_x - knob_r, meta['y_center'] - knob_r, knob_r*2, knob_r*2)\n",
    "                meta['rect'] = meta_rect\n",
    "                knobs[dragging] = meta\n",
    "\n",
    "        # --- Draw UI ---\n",
    "        screen.fill((245,245,245))\n",
    "        title = font.render('Locke N Key — Setup (Phase → Algo → Params)', True, (20,20,20))\n",
    "        screen.blit(title, (40,20))\n",
    "\n",
    "        stage_texts = ['Select Phase (ENTER next)', 'Select Algorithm (ENTER next)', 'Tune Params / Start (ENTER start)']\n",
    "        screen.blit(small.render(stage_texts[stage], True, (60,60,60)), (40,60))\n",
    "\n",
    "        # Phase box\n",
    "        phase_box = pygame.Rect(50, 100, 300, 80)\n",
    "        pygame.draw.rect(screen, (200,230,200) if stage==0 else (220,220,220), phase_box)\n",
    "        pygame.draw.rect(screen, (0,0,0), phase_box, 2)\n",
    "        ptxt = font.render(f\"Phase: {phase_idx}\", True, (0,0,0))\n",
    "        screen.blit(ptxt, (phase_box.x + 20, phase_box.y + 20))\n",
    "        descs = {\n",
    "            1: \"Fixed positions (no enemy)\",\n",
    "            2: \"Random agent/key, fixed lock\",\n",
    "            3: \"Random agent/key/lock\",\n",
    "            4: \"Randomized tuning (like Phase 3)\",\n",
    "            5: \"Adds moving enemy hazard\"\n",
    "        }\n",
    "        screen.blit(small.render(descs[phase_idx], True, (40,40,40)), (phase_box.x + 20, phase_box.y + 48))\n",
    "\n",
    "        # Algorithm selection\n",
    "        algo_box = pygame.Rect(380, 100, 520, 80)\n",
    "        pygame.draw.rect(screen, (200,230,255) if stage==1 else (240,240,240), algo_box)\n",
    "        pygame.draw.rect(screen, (0,0,0), algo_box, 2)\n",
    "        algo_txt = font.render(f\"Algorithm: {algo_choices[algo_idx]}\", True, (0,0,0))\n",
    "        screen.blit(algo_txt, (algo_box.x + 20, algo_box.y + 20))\n",
    "\n",
    "        # Sliders\n",
    "        algo = algo_choices[algo_idx]\n",
    "        knobs.clear()\n",
    "        if stage == 2:\n",
    "            y = 220\n",
    "            track_x = 180\n",
    "            track_w = 480\n",
    "\n",
    "            if algo == 'Q-Learning':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params['alpha'], 0.001, 1.0, \"Alpha (Learning Rate)\", font)\n",
    "                knobs['alpha'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'alpha', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['epsilon'], 0.0, 1.0, \"Epsilon (Exploration)\", font)\n",
    "                knobs['epsilon'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.0, 'max':1.0, 'param_key':'epsilon', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "            elif algo == 'Monte Carlo':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params.get('MC_LR', 0.05), 0.001, 1.0, \"MC Learning Rate\", font)\n",
    "                knobs['MC_LR'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'MC_LR', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['epsilon'], 0.0, 1.0, \"Epsilon (Exploration)\", font)\n",
    "                knobs['epsilon'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.0, 'max':1.0, 'param_key':'epsilon', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "            elif algo == 'Actor-Critic':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params['alpha'], 0.001, 1.0, \"Alpha (Critic LR)\", font)\n",
    "                knobs['alpha'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'alpha', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['beta'], 0.001, 1.0, \"Beta (Actor LR)\", font)\n",
    "                knobs['beta'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'beta', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "        # Episodes input\n",
    "        draw_input_box(screen, input_rect, params['episodes'], font, active_box)\n",
    "        screen.blit(small.render(\"Episodes:\", True, (0,0,0)), (680, 260))\n",
    "\n",
    "        # Hint text\n",
    "        screen.blit(small.render('←/→ to change • ENTER = Next/Start • BACKSPACE = Back • Drag sliders to adjust', True, (60,60,60)), (40, 580))\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(30)\n",
    "\n",
    "        if start_sim:\n",
    "            pygame.quit()\n",
    "            params['episodes'] = int(params['episodes']) if str(params['episodes']).isdigit() else 500\n",
    "            params['phase'] = phase_idx\n",
    "            return algo_choices[algo_idx], params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231edaf",
   "metadata": {},
   "source": [
    "## 4 ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642a05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 4: Simplified In-notebook Environment (with Image Placeholders + Speed Control + Distance Reward + Trailing Enemy) -----------------\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os, sys, random\n",
    "from collections import deque\n",
    "\n",
    "# Safe init for pygame (avoids repeated initialization errors in Jupyter)\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "        pygame.display.init()\n",
    "\n",
    "class LockKeyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Lock & Key Puzzle Environment\n",
    "    Now includes an enemy that follows the agent's trail (2–3 steps behind).\n",
    "    Reward shaping includes:\n",
    "      - Moving closer to the key (before pickup)\n",
    "      - Moving closer to the door (after having key)\n",
    "      - Staying farther from the enemy\n",
    "      - Major rewards for key pickup & door unlock\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 5}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\", size=6, phase=1, seed=None):\n",
    "        super().__init__()\n",
    "        assert phase in (1,2,3,4,5), \"Phase must be 1–5\"\n",
    "        self.size = size\n",
    "        self.phase = phase\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Window and grid parameters\n",
    "        self.window_size = 800\n",
    "        self.grid_size = 500\n",
    "        self.info_width = self.window_size - self.grid_size\n",
    "        self.cell_size = self.grid_size // self.size\n",
    "\n",
    "        # Spaces\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Discrete(self.size**6)\n",
    "\n",
    "        # Default object states\n",
    "        self._default_agent_pos = np.array([5, 0])\n",
    "        self._default_key_pos = np.array([5, 3])\n",
    "        self._default_lock_pos = np.array([0, 5])\n",
    "        self.walls = {(0,2),(1,1),(1,4),(2,3),(3,0),(3,2),(4,4)}\n",
    "\n",
    "        # Enemy\n",
    "        self.enemy_pos = None\n",
    "        self.enemy_active = (phase == 5)\n",
    "        self.trail = deque(maxlen=4)  # store last few agent positions for enemy following\n",
    "\n",
    "        # State variables\n",
    "        self.agent_pos = None\n",
    "        self.key_pos = None\n",
    "        self.lock_pos = None\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.last_reward = 0\n",
    "        self.current_episode = 0\n",
    "\n",
    "        # Track distances for shaping\n",
    "        self.prev_enemy_dist = None\n",
    "        self.prev_key_dist = None\n",
    "        self.prev_door_dist = None\n",
    "\n",
    "        # Rendering setup\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # --- Speed control ---\n",
    "        self.framerate = 5\n",
    "        self.unlimited_fps = False\n",
    "        self.speed_multiplier = 1.0\n",
    "        self.speed_levels = [0.5, 1, 2, 5, 10, 20, 30, 40, 50, 60]\n",
    "        self.current_speed_idx = 1  # index in speed_levels (1 => ×1)\n",
    "\n",
    "        # Episode control\n",
    "        self.max_steps = 400\n",
    "\n",
    "        # --- IMAGE PLACEHOLDERS ---\n",
    "        self.player_img = None\n",
    "        self.key_img = None\n",
    "        self.door_img = None\n",
    "        self.enemy_img = None\n",
    "        self.obstacle_img = None\n",
    "\n",
    "        # --- Reward configuration ---\n",
    "        self.STEP_PENALTY = -0.1\n",
    "        self.KEY_REWARD = 100.0\n",
    "        self.DOOR_REWARD = 300.0\n",
    "        self.CAUGHT_PENALTY = -200.0\n",
    "        self.SURVIVAL_BONUS = 0.00\n",
    "        self.DIST_SCALE = 0.05      # enemy distance shaping\n",
    "        self.APPROACH_SCALE = 0.1   # key/door distance shaping\n",
    "\n",
    "\n",
    "    # --- IMAGE LOADING SECTION ---\n",
    "    def _load_images(self):\n",
    "        pygame_safe_init()\n",
    "        if not pygame.display.get_init() or pygame.display.get_surface() is None:\n",
    "            pygame.display.set_mode((1, 1))\n",
    "\n",
    "        cwd = os.getcwd()\n",
    "        print(f\"[INFO] Looking for images in: {cwd}\")\n",
    "\n",
    "        def load_img(name):\n",
    "            try:\n",
    "                if os.path.exists(name):\n",
    "                    img = pygame.image.load(name).convert_alpha()\n",
    "                    print(f\"[OK] Loaded {name}\")\n",
    "                    return img\n",
    "                else:\n",
    "                    print(f\"[WARNING] {name} not found — using placeholder.\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to load {name}: {e}\")\n",
    "                return None\n",
    "\n",
    "        self.player_img = load_img(\"player.png\")\n",
    "        self.key_img = load_img(\"key.png\")\n",
    "        self.door_img = load_img(\"door.png\")\n",
    "        self.enemy_img = load_img(\"enemy.png\")\n",
    "        self.obstacle_img = load_img(\"obstacle.png\")\n",
    "\n",
    "\n",
    "    def handle_events(self, events):\n",
    "        for event in events:\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "                sys.exit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    self._increase_speed()\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self._decrease_speed()\n",
    "                elif event.key == pygame.K_ESCAPE:\n",
    "                    self.close()\n",
    "                    sys.exit()\n",
    "\n",
    "    # ---------- RESET ----------\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(seed)\n",
    "        if self.phase == 1:\n",
    "            self.agent_pos = self._default_agent_pos.copy()\n",
    "            self.key_pos = self._default_key_pos.copy()\n",
    "            self.lock_pos = self._default_lock_pos.copy()\n",
    "        elif self.phase == 2:\n",
    "            self.lock_pos = self._default_lock_pos.copy()\n",
    "            self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "            self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "        elif self.phase in (3,4,5):\n",
    "            self.lock_pos = self._random_free_cell()\n",
    "            self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "            self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "\n",
    "        if self.phase == 5:\n",
    "            exclude = {tuple(self.lock_pos), tuple(self.agent_pos), tuple(self.key_pos)}.union(self.walls)\n",
    "            self.enemy_pos = self._random_free_cell(exclude=exclude)\n",
    "            self.enemy_active = True\n",
    "            self.trail.clear()\n",
    "\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.last_reward = 0\n",
    "\n",
    "        # Initialize distance trackers\n",
    "        self.prev_enemy_dist = self._manhattan(self.agent_pos, self.enemy_pos) if self.enemy_active else None\n",
    "        self.prev_key_dist = self._manhattan(self.agent_pos, self.key_pos)\n",
    "        self.prev_door_dist = self._manhattan(self.agent_pos, self.lock_pos)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return obs, {}\n",
    "\n",
    "    # ---------- STEP ----------\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        reward = self.STEP_PENALTY\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Agent movement\n",
    "        move_map = {0:(1,0), 1:(-1,0), 2:(0,1), 3:(0,-1)}\n",
    "        if action in move_map:\n",
    "            dr, dc = move_map[action]\n",
    "            new_pos = self.agent_pos + np.array([dr, dc])\n",
    "            if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and (tuple(new_pos) not in self.walls):\n",
    "                # record current position before moving\n",
    "                self.trail.appendleft(self.agent_pos.copy())\n",
    "                self.agent_pos = new_pos\n",
    "\n",
    "        # Distance-based shaping: approach key/door\n",
    "        key_dist = self._manhattan(self.agent_pos, self.key_pos)\n",
    "        door_dist = self._manhattan(self.agent_pos, self.lock_pos)\n",
    "\n",
    "        if not self.has_key:\n",
    "            delta = self.prev_key_dist - key_dist\n",
    "            reward += self.APPROACH_SCALE * delta\n",
    "            self.prev_key_dist = key_dist\n",
    "        else:\n",
    "            delta = self.prev_door_dist - door_dist\n",
    "            reward += self.APPROACH_SCALE * delta\n",
    "            self.prev_door_dist = door_dist\n",
    "\n",
    "        # Pick up key\n",
    "        if not self.has_key and np.array_equal(self.agent_pos, self.key_pos):\n",
    "            self.has_key = True\n",
    "            reward += self.KEY_REWARD\n",
    "\n",
    "        # Unlock door (terminal success)\n",
    "        if self.has_key and np.array_equal(self.agent_pos, self.lock_pos):\n",
    "            reward += self.DOOR_REWARD\n",
    "            terminated = True\n",
    "\n",
    "        # Enemy pattern: follow agent trail\n",
    "        if self.phase == 5 and self.enemy_active:\n",
    "            if len(self.trail) >= 3:\n",
    "                target = self.trail[2]  # roughly 2–3 steps behind\n",
    "                self.enemy_pos = target.copy()\n",
    "            else:\n",
    "                self._move_enemy_random()  # fallback random movement early\n",
    "\n",
    "            # Enemy distance shaping\n",
    "            dist_now = self._manhattan(self.agent_pos, self.enemy_pos)\n",
    "            if self.prev_enemy_dist is not None:\n",
    "                delta = dist_now - self.prev_enemy_dist\n",
    "                reward += self.DIST_SCALE * delta\n",
    "            self.prev_enemy_dist = dist_now\n",
    "\n",
    "            # Collision\n",
    "            if np.array_equal(self.enemy_pos, self.agent_pos):\n",
    "                reward += self.CAUGHT_PENALTY\n",
    "                terminated = True\n",
    "\n",
    "        # Survival bonus\n",
    "        if not terminated:\n",
    "            reward += self.SURVIVAL_BONUS\n",
    "\n",
    "        # Timeout\n",
    "        info = {}\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "            info['timeout'] = True\n",
    "\n",
    "        # Outcome info\n",
    "        if terminated:\n",
    "            if self.phase == 5 and self.enemy_active and np.array_equal(self.enemy_pos, self.agent_pos):\n",
    "                info['caught'] = True\n",
    "            elif self.has_key and np.array_equal(self.agent_pos, self.lock_pos):\n",
    "                info['unlocked'] = True\n",
    "\n",
    "        self.last_reward = reward\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    # ---------- HELPERS ----------\n",
    "    def _random_free_cell(self, exclude=None):\n",
    "        if exclude is None:\n",
    "            exclude = set()\n",
    "        while True:\n",
    "            r, c = self._rng.integers(0, self.size, size=2)\n",
    "            if (r, c) not in self.walls and (r, c) not in exclude:\n",
    "                return np.array([r, c])\n",
    "\n",
    "    def _manhattan(self, p1, p2):\n",
    "        if p1 is None or p2 is None:\n",
    "            return 0\n",
    "        return abs(p1[0]-p2[0]) + abs(p1[1]-p2[1])\n",
    "\n",
    "    def _move_enemy_random(self):\n",
    "        directions = [(1,0),(-1,0),(0,1),(0,-1),(0,0)]\n",
    "        self._rng.shuffle(directions)\n",
    "        for dr, dc in directions:\n",
    "            cand = self.enemy_pos + np.array([dr, dc])\n",
    "            rr, cc = int(cand[0]), int(cand[1])\n",
    "            if 0 <= rr < self.size and 0 <= cc < self.size and (rr, cc) not in self.walls:\n",
    "                self.enemy_pos = np.array([rr, cc])\n",
    "                return\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            *self.agent_pos,\n",
    "            *self.key_pos,\n",
    "            *self.lock_pos,\n",
    "            int(self.has_key)\n",
    "        ])\n",
    "\n",
    "    # ---------- RENDER ----------\n",
    "    def _render_frame(self):\n",
    "        pygame_safe_init()\n",
    "        if self.player_img is None:\n",
    "            self._load_images()\n",
    "        if self.window is None:\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.grid_size))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size, self.grid_size))\n",
    "        canvas.fill((220, 220, 220))\n",
    "\n",
    "        for x in range(self.size+1):\n",
    "            pygame.draw.line(canvas, (0,0,0), (0, x*self.cell_size), (self.grid_size, x*self.cell_size), 1)\n",
    "            pygame.draw.line(canvas, (0,0,0), (x*self.cell_size, 0), (x*self.cell_size, self.grid_size), 1)\n",
    "\n",
    "        for r, c in self.walls:\n",
    "            rect = pygame.Rect(c*self.cell_size, r*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.obstacle_img:\n",
    "                scaled = pygame.transform.scale(self.obstacle_img, (self.cell_size, self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.rect(canvas, (100,100,100), rect)\n",
    "\n",
    "        if not self.has_key:\n",
    "            rect = pygame.Rect(self.key_pos[1]*self.cell_size, self.key_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.key_img:\n",
    "                scaled = pygame.transform.scale(self.key_img, (self.cell_size, self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.circle(canvas, (255,215,0), rect.center, max(6, self.cell_size//4))\n",
    "\n",
    "        rect = pygame.Rect(self.lock_pos[1]*self.cell_size, self.lock_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "        if self.door_img:\n",
    "            scaled = pygame.transform.scale(self.door_img, (self.cell_size, self.cell_size))\n",
    "            canvas.blit(scaled, rect.topleft)\n",
    "        else:\n",
    "            pygame.draw.rect(canvas, (200,50,50), rect)\n",
    "\n",
    "        rect = pygame.Rect(self.agent_pos[1]*self.cell_size, self.agent_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "        if self.player_img:\n",
    "            scaled = pygame.transform.scale(self.player_img, (self.cell_size, self.cell_size))\n",
    "            canvas.blit(scaled, rect.topleft)\n",
    "        else:\n",
    "            pygame.draw.circle(canvas, (50,100,255), rect.center, max(6, self.cell_size//3))\n",
    "\n",
    "        if self.phase == 5 and self.enemy_active and self.enemy_pos is not None:\n",
    "            rect = pygame.Rect(self.enemy_pos[1]*self.cell_size, self.enemy_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.enemy_img:\n",
    "                scaled = pygame.transform.scale(self.enemy_img, (self.cell_size, self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.circle(canvas, (255,0,0), rect.center, max(6, self.cell_size//3))\n",
    "\n",
    "        info_panel = pygame.Surface((self.info_width, self.grid_size))\n",
    "        info_panel.fill((245,245,245))\n",
    "        font = pygame.font.SysFont(\"arial\", 18)\n",
    "        lines = [\n",
    "            f\"Phase: {self.phase}\",\n",
    "            f\"Steps: {self.steps}\",\n",
    "            f\"Reward: {round(self.last_reward, 2)}\",\n",
    "            f\"Has Key: {'Yes' if self.has_key else 'No'}\",\n",
    "            f\"Speed: ×{self.speed_multiplier}\"\n",
    "        ]\n",
    "        for i, text in enumerate(lines):\n",
    "            info_panel.blit(font.render(text, True, (0,0,0)), (10, 20 + i*28))\n",
    "\n",
    "        self.window.blit(canvas, (0,0))\n",
    "        self.window.blit(info_panel, (self.grid_size, 0))\n",
    "        pygame.display.flip()\n",
    "\n",
    "        if not self.unlimited_fps:\n",
    "            effective_fps = max(1, int(self.framerate * self.speed_multiplier))\n",
    "            self.clock.tick(effective_fps)\n",
    "\n",
    "    # ---------- SPEED CONTROL ----------\n",
    "    def _increase_speed(self):\n",
    "        if self.current_speed_idx < len(self.speed_levels) - 1:\n",
    "            self.current_speed_idx += 1\n",
    "            self.speed_multiplier = self.speed_levels[self.current_speed_idx]\n",
    "            print(f\"[INFO] Increased speed to ×{self.speed_multiplier}\")\n",
    "\n",
    "    def _decrease_speed(self):\n",
    "        if self.current_speed_idx > 0:\n",
    "            self.current_speed_idx -= 1\n",
    "            self.speed_multiplier = self.speed_levels[self.current_speed_idx]\n",
    "            print(f\"[INFO] Decreased speed to ×{self.speed_multiplier}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1d161",
   "metadata": {},
   "source": [
    "## 5 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 5: Run the whole flow (menu -> train). Training loop no plotting. -----------------\n",
    "def train_and_visualize(algo_name, params, save_path=\"lockkey_policy.pkl\"):\n",
    "    pygame_safe_init()\n",
    "\n",
    "    # --- Environment setup (now using the in-notebook LockKeyEnv) ---\n",
    "    env = LockKeyEnv(render_mode='human', size=6, phase=int(params.get('phase', 5)))\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    episodes = int(params.get('episodes', 800))\n",
    "    gamma = float(params.get('gamma', 0.9))\n",
    "\n",
    "    # --- Agent creation ---\n",
    "    if algo_name == 'Q-Learning':\n",
    "        agent = QLearningAgent(n_states, n_actions,\n",
    "                               alpha=float(params.get('alpha', 0.1)),\n",
    "                               gamma=gamma,\n",
    "                               epsilon=float(params.get('epsilon', 0.2)))\n",
    "    elif algo_name == 'Monte Carlo':\n",
    "        agent = MonteCarloAgent(n_states, n_actions,\n",
    "                                gamma=gamma,\n",
    "                                epsilon=float(params.get('epsilon', 0.1)))\n",
    "        # if MC has a learning rate param, store it on agent (optional usage)\n",
    "        if 'MC_LR' in params:\n",
    "            agent.mc_lr = float(params.get('MC_LR', 0.05))\n",
    "    else:  # Actor-Critic\n",
    "        agent = ActorCriticAgent(n_states, n_actions,\n",
    "                                 alpha=float(params.get('alpha', 0.1)),   # critic lr\n",
    "                                 beta=float(params.get('beta', 0.01)),    # actor lr\n",
    "                                 gamma=gamma)\n",
    "\n",
    "    episode_rewards, successes, episode_lengths = [], [], []\n",
    "\n",
    "    pygame_safe_init()\n",
    "    font = pygame.font.SysFont('arial', 18)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # --- Main training loop ---\n",
    "        for ep in range(1, episodes + 1):\n",
    "            # fetch events once now; pass them into env later each frame\n",
    "            obs, _ = env.reset()\n",
    "            env.current_episode = ep\n",
    "            done = False\n",
    "            total_r = 0\n",
    "            steps = 0\n",
    "            episode_hist = []\n",
    "\n",
    "            if isinstance(obs, tuple) and len(obs) == 2:\n",
    "                obs = obs[0]\n",
    "\n",
    "            while not done and steps < 200:\n",
    "                # fetch events once per frame\n",
    "                events = pygame.event.get()\n",
    "                # handle quit/escape centrally (avoid consuming events)\n",
    "                for ev in events:\n",
    "                    if ev.type == pygame.QUIT:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "                    elif ev.type == pygame.KEYDOWN and ev.key == pygame.K_ESCAPE:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "\n",
    "                # let environment consume events it needs (speed keys)\n",
    "                env.handle_events(events)\n",
    "\n",
    "                # Agent selects action\n",
    "                s = obs\n",
    "                a = agent.select_action(s) if hasattr(agent, 'select_action') else agent.policy(s)\n",
    "                obs2, r, done, trunc, info = env.step(a)\n",
    "                total_r += r\n",
    "                steps += 1\n",
    "\n",
    "                if isinstance(agent, MonteCarloAgent):\n",
    "                    episode_hist.append((s, a, r))\n",
    "                else:\n",
    "                    agent.update(s, a, r, obs2, done)\n",
    "\n",
    "                obs = obs2\n",
    "\n",
    "                # Render env (HUD handled by env._render_frame)\n",
    "                try:\n",
    "                    env.render()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Console updates occasionally\n",
    "                if steps % 30 == 0:\n",
    "                    print(f\"Ep {ep} | Step {steps:03d} | Reward={total_r:.2f} | Eps={getattr(agent, 'epsilon', 0):.2f}\", end='\\r')\n",
    "\n",
    "            # End of episode\n",
    "            if isinstance(agent, MonteCarloAgent):\n",
    "                agent.update(episode_hist)\n",
    "\n",
    "            if hasattr(agent, 'epsilon'):\n",
    "                agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
    "\n",
    "            episode_rewards.append(total_r)\n",
    "            successes.append(1 if done and total_r > 0 else 0)\n",
    "            episode_lengths.append(steps)\n",
    "            \n",
    "            # Detect episode outcome\n",
    "            if env.phase == 5 and env.enemy_active and np.array_equal(env.agent_pos, env.enemy_pos):\n",
    "                outcome = \"❌ Caught by Enemy\"\n",
    "            elif np.array_equal(env.agent_pos, env.lock_pos) and env.has_key:\n",
    "                outcome = \"🏁 Door Unlocked\"\n",
    "            elif not env.has_key and np.array_equal(env.agent_pos, env.key_pos):\n",
    "                outcome = \"🔑 Picked up Key\"\n",
    "            else:\n",
    "                outcome = \"🌀 Episode Ended (Timeout or Random Termination)\"\n",
    "\n",
    "            print(f\"✅ Episode {ep}/{episodes} finished | Reward={total_r:.2f} | Steps={steps} | {outcome}\")\n",
    "\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            env.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Save policy (unchanged) ---\n",
    "    try:\n",
    "        def safe_convert(obj):\n",
    "            if isinstance(obj, defaultdict):\n",
    "                obj = {k: safe_convert(v) for k, v in obj.items()}\n",
    "            return obj\n",
    "\n",
    "        data_to_save = None\n",
    "        if algo_name in ['Q-Learning', 'Monte Carlo']:\n",
    "            data_to_save = safe_convert(agent.Q)\n",
    "        else:\n",
    "            data_to_save = {'V': safe_convert(agent.V), 'pi': safe_convert(agent.pi)}\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)\n",
    "        print(f\"✅ Saved {algo_name} policy successfully to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not save policy due to {e}\")\n",
    "\n",
    "    return episode_rewards, successes, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20c6e5",
   "metadata": {},
   "source": [
    "## 6 RUN WHOLE FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen algorithm: Q-Learning\n",
      "Parameters: {'episodes': 100, 'alpha': 0.1, 'beta': 0.1, 'gamma': 0.9, 'epsilon': 0.2, 'MC_LR': 0.05, 'phase': 1}\n",
      "[INFO] Looking for images in: c:\\Users\\chescake\\Documents\\NU\\SY 2025 - 2026\\1ST TERM\\REINFORCEMENT LEARNING\\RL TEST\\LOCK_KEY\n",
      "[OK] Loaded player.png\n",
      "[OK] Loaded key.png\n",
      "[OK] Loaded door.png\n",
      "[OK] Loaded enemy.png\n",
      "[OK] Loaded obstacle.png\n",
      "[INFO] Increased speed to ×2\n",
      "[INFO] Increased speed to ×5\n",
      "[INFO] Increased speed to ×10\n",
      "[INFO] Increased speed to ×20\n",
      "[INFO] Increased speed to ×30\n",
      "[INFO] Increased speed to ×400 | Eps=0.20\n",
      "✅ Episode 1/100 finished | Reward=397.10 | Steps=42 | 🏁 Door Unlocked\n",
      "✅ Episode 2/100 finished | Reward=399.80 | Steps=15 | 🏁 Door Unlocked\n",
      "[INFO] Increased speed to ×50\n",
      "✅ Episode 3/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 4/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "[INFO] Increased speed to ×60\n",
      "✅ Episode 5/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 6/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 7/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 8/100 finished | Reward=399.60 | Steps=17 | 🏁 Door Unlocked\n",
      "✅ Episode 9/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 10/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 11/100 finished | Reward=399.70 | Steps=16 | 🏁 Door Unlocked\n",
      "✅ Episode 12/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 13/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 14/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 15/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 16/100 finished | Reward=399.70 | Steps=16 | 🏁 Door Unlocked\n",
      "✅ Episode 17/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 18/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 19/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 20/100 finished | Reward=399.70 | Steps=16 | 🏁 Door Unlocked\n",
      "✅ Episode 21/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 22/100 finished | Reward=399.60 | Steps=17 | 🏁 Door Unlocked\n",
      "✅ Episode 23/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 24/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 25/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 26/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 27/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 28/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 29/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 30/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 31/100 finished | Reward=399.80 | Steps=15 | 🏁 Door Unlocked\n",
      "✅ Episode 32/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 33/100 finished | Reward=399.80 | Steps=15 | 🏁 Door Unlocked\n",
      "✅ Episode 34/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 35/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 36/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 37/100 finished | Reward=399.70 | Steps=16 | 🏁 Door Unlocked\n",
      "✅ Episode 38/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 39/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 40/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 41/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 42/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 43/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 44/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 45/100 finished | Reward=399.80 | Steps=15 | 🏁 Door Unlocked\n",
      "✅ Episode 46/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 47/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 48/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 49/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 50/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 51/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 52/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 53/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 54/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 55/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 56/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 57/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 58/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 59/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 60/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 61/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 62/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 63/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 64/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 65/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 66/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 67/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 68/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 69/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 70/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 71/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 72/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 73/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 74/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 75/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 76/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 77/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 78/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 79/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 80/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 81/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 82/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 83/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 84/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 85/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 86/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 87/100 finished | Reward=400.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 88/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 89/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 90/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 91/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 92/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 93/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 94/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 95/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 96/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 97/100 finished | Reward=400.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 98/100 finished | Reward=399.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 99/100 finished | Reward=400.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 100/100 finished | Reward=400.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Saved Q-Learning policy successfully to lockkey_policy.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ec9333ddb0e3>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train (this opens the environment window)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_visualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lockkey_policy.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training complete. Saved policy.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# CELL 6: Run the menu and training\n",
    "# Run this cell to open the Pygame menu, choose phase & algorithm & params, then train.\n",
    "\n",
    "algo_name, params = menu_and_params()\n",
    "print(\"Chosen algorithm:\", algo_name)\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Visuals mapping placeholder (not currently used in the env code,\n",
    "# but retained if you want to pass custom colors or images later)\n",
    "visuals = {\n",
    "    'agent_color': (50,100,255),\n",
    "    'key_color': (255,215,0),\n",
    "    'lock_color': (200,50,50),\n",
    "    'wall_color': (80,80,80),\n",
    "    'enemy_color': (0,0,255),\n",
    "    'bg_color': (230,230,230)\n",
    "}\n",
    "\n",
    "# Train (this opens the environment window)\n",
    "rewards, successes, episode_lengths = train_and_visualize(algo_name, params, save_path='lockkey_policy.pkl')\n",
    "print(\"Training complete. Saved policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51965fa",
   "metadata": {},
   "source": [
    "## 7 RESULTS GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5646c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-42b3569feb43>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m# === 3) Run plotting and metrics (use variables from CELL 6) ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mplot_learning_curves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[0manalyze_best_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------- CELL 7: Results & Comparison (plots + optional summary) -----------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# === 1) Plot Learning Curves (Raw + Smoothed Rewards + Success Rate + Episode Lengths) ===\n",
    "def plot_learning_curves(rewards, successes, episode_lengths, algo_name, smooth_window=10):\n",
    "    episodes = np.arange(1, len(rewards) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.suptitle(f\"{algo_name} - Training Performance\", fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 1️⃣ Raw + Smoothed Reward Curve\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(episodes, rewards, marker='o', alpha=0.6, label='Raw Reward')\n",
    "    if len(rewards) >= smooth_window:\n",
    "        smooth_rewards = np.convolve(rewards, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "        x = np.arange(smooth_window, len(rewards)+1)\n",
    "        plt.plot(x, smooth_rewards, color='orange', linewidth=2, label=f'Smoothed (window={smooth_window})')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Rewards (Raw + Smoothed)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # 2️⃣ Success Rate Curve\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if len(successes) > 0:\n",
    "        success_rate = np.cumsum(successes) / np.arange(1, len(successes)+1)\n",
    "        plt.plot(np.arange(1, len(successes)+1), success_rate, color='green', label='Success Rate')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Success Rate\")\n",
    "    plt.title(\"Success Rate Over Time\")\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 3️⃣ Episode Length Curve\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(episodes, episode_lengths, color='purple', marker='x', alpha=0.6, label='Episode Length')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.title(\"Episode Lengths\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === 2) Extract Best Metrics ===\n",
    "def analyze_best_metrics(rewards, successes, episode_lengths, algo_name, smooth_window=10):\n",
    "    # Raw rewards\n",
    "    best_raw = np.max(rewards)\n",
    "    best_raw_ep = np.argmax(rewards) + 1\n",
    "\n",
    "    # Smoothed rewards\n",
    "    if len(rewards) >= smooth_window:\n",
    "        smooth_rewards = np.convolve(rewards, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "        best_smooth = np.max(smooth_rewards)\n",
    "        best_smooth_ep = np.argmax(smooth_rewards) + smooth_window\n",
    "    else:\n",
    "        best_smooth = np.max(rewards)\n",
    "        best_smooth_ep = np.argmax(rewards) + 1\n",
    "\n",
    "    # Success rate\n",
    "    success_rate = np.cumsum(successes) / np.arange(1, len(successes)+1)\n",
    "    best_success = np.max(success_rate)\n",
    "    best_success_ep = np.argmax(success_rate) + 1\n",
    "\n",
    "    # Episode length\n",
    "    best_length = np.max(episode_lengths)\n",
    "    best_length_ep = np.argmax(episode_lengths) + 1\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "\n",
    "    print(f\"\\n📈 {algo_name} - Best Performance Metrics\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(f\"  🟦 Best Raw Reward: {best_raw:.3f}  (Episode {best_raw_ep})\")\n",
    "    print(f\"  🟧 Best Smoothed Reward: {best_smooth:.3f}  (Episode {best_smooth_ep})\")\n",
    "    print(f\"  🟩 Best Success Rate: {best_success:.3f}  (Episode {best_success_ep})\")\n",
    "    print(f\"  🟪 Longest Episode: {best_length} steps  (Episode {best_length_ep})\")\n",
    "    print(f\"  Average Episode Length: {avg_length:.2f} steps\")\n",
    "\n",
    "\n",
    "# === 3) Run plotting and metrics (use variables from CELL 6) ===\n",
    "plot_learning_curves(rewards, successes, episode_lengths, algo_name)\n",
    "analyze_best_metrics(rewards, successes, episode_lengths, algo_name)\n",
    "\n",
    "# added here episode lengths and average length to the metrics summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc90e5",
   "metadata": {},
   "source": [
    "# FOR PERCENTAGE OF EVAL (TO BE EDITED PER PHASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Converted Performance to Percentage Form ===\n",
      "       Setting  Best Raw Reward (%)  Best Smoothed Reward (%)  \\\n",
      "0      Default                99.35                     93.04   \n",
      "1  High LR (α)               100.00                     92.39   \n",
      "2   Low LR (α)                99.02                     93.56   \n",
      "3  High ER (ε)                96.08                     84.64   \n",
      "4   Low ER (ε)                99.35                     95.98   \n",
      "5  High DF (γ)               100.00                     94.74   \n",
      "6   Low DF (γ)                99.35                     92.88   \n",
      "\n",
      "   Best Success Rate (%)  Average Episode Length (%)  \n",
      "0                  100.0                       44.13  \n",
      "1                  100.0                       52.63  \n",
      "2                   89.0                       35.74  \n",
      "3                   76.0                        0.00  \n",
      "4                  100.0                       46.09  \n",
      "5                  100.0                       43.86  \n",
      "6                  100.0                       44.66  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Define your max constants (adjust if needed) ===\n",
    "MAX_REWARD = 30.600   # highest achievable reward observed\n",
    "MAX_STEPS = 200     # your environment's timeout step\n",
    "\n",
    "# === 2. Input your metrics per parameter setting ===\n",
    "data = [\n",
    "    {\n",
    "        \"Setting\": \"Default\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 28.470,\n",
    "        \"Best Success Rate\": 1.000,   # already 0–1\n",
    "        \"Average Episode Length\": 56.880\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High LR (α)\",\n",
    "        \"Best Raw Reward\": 30.600,\n",
    "        \"Best Smoothed Reward\": 28.270,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 48.22\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low LR (α)\",\n",
    "        \"Best Raw Reward\": 30.300,\n",
    "        \"Best Smoothed Reward\": 28.630,\n",
    "        \"Best Success Rate\": 0.890,\n",
    "        \"Average Episode Length\": 65.42\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High ER (ε)\",\n",
    "        \"Best Raw Reward\": 29.400,\n",
    "        \"Best Smoothed Reward\": 25.900,\n",
    "        \"Best Success Rate\": 0.760,\n",
    "        \"Average Episode Length\": 101.80\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low ER (ε)\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 29.370,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 54.88\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High DF (γ)\",\n",
    "        \"Best Raw Reward\": 30.600,\n",
    "        \"Best Smoothed Reward\": 28.990,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 57.15\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low DF (γ)\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 28.420,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 56.34\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === 3. Convert to percentage values ===\n",
    "df[\"Best Raw Reward (%)\"] = (df[\"Best Raw Reward\"] / MAX_REWARD) * 100\n",
    "df[\"Best Smoothed Reward (%)\"] = (df[\"Best Smoothed Reward\"] / MAX_REWARD) * 100\n",
    "df[\"Best Success Rate (%)\"] = df[\"Best Success Rate\"] * 100\n",
    "df[\"Average Episode Length (%)\"] = (1 - (df[\"Average Episode Length\"] / MAX_STEPS)) * 100  # efficiency\n",
    "\n",
    "# === 4. Select and display percentage columns only ===\n",
    "percent_df = df[[\"Setting\",\n",
    "                 \"Best Raw Reward (%)\",\n",
    "                 \"Best Smoothed Reward (%)\",\n",
    "                 \"Best Success Rate (%)\",\n",
    "                 \"Average Episode Length (%)\"]]\n",
    "\n",
    "print(\"\\n=== Converted Performance to Percentage Form ===\")\n",
    "print(percent_df.round(2))\n",
    "\n",
    "# for evaluation only; not part of main flow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympiaCOM222ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
