{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3066cb63",
   "metadata": {},
   "source": [
    "# PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9aaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnasium ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3a805",
   "metadata": {},
   "source": [
    "# LOCKANDKEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e17e48",
   "metadata": {},
   "source": [
    "## 1 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f294dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Imports & helpers\n",
    "# Paste in first cell\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Pygame used for menu + visualization\n",
    "import pygame\n",
    "\n",
    "# Small safe initializer used before creating fonts/displays\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "    if not pygame.font.get_init():\n",
    "        pygame.font.init()\n",
    "\n",
    "# small helper: moving average\n",
    "def moving_avg(arr, window):\n",
    "    if len(arr) < window:\n",
    "        return np.array(arr)\n",
    "    return np.convolve(arr, np.ones(window)/window, mode='valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9178f7",
   "metadata": {},
   "source": [
    "## 2 RL AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb7e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: RL Agents (Q-Learning, Monte Carlo, Actor-Critic)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Utility to make states hashable ---\n",
    "def hashable_state(s):\n",
    "    \"\"\"\n",
    "    Convert dict, ndarray, or list observation into a hashable key.\n",
    "    For our env the observation is an integer, so hashing is trivial.\n",
    "    This helper keeps the agent code general.\n",
    "    \"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return tuple(sorted((k, hashable_state(v)) for k, v in s.items()))\n",
    "    elif isinstance(s, np.ndarray):\n",
    "        return tuple(map(float, s.flatten()))\n",
    "    elif isinstance(s, (list, tuple)):\n",
    "        return tuple(map(hashable_state, s))\n",
    "    else:\n",
    "        try:\n",
    "            hash(s)\n",
    "            return s\n",
    "        except TypeError:\n",
    "            return str(s)\n",
    "\n",
    "\n",
    "# ----------------- Q-Learning Agent -----------------\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)\n",
    "        current = self.Q[state][action]\n",
    "        self.Q[state][action] = current + self.alpha * (target - current)\n",
    "\n",
    "\n",
    "# ----------------- Monte Carlo Agent -----------------\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, gamma=0.9, epsilon=0.1, alpha=0.05):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # NEW: learning rate\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, episode):\n",
    "        \"\"\"Episode is a list of (state, action, reward).\"\"\"\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            state = hashable_state(state)\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                old_value = self.Q[state][action]\n",
    "                # Instead of averaging, apply incremental update\n",
    "                self.Q[state][action] = old_value + self.alpha * (G - old_value)\n",
    "                visited.add((state, action))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- Actor-Critic Agent -----------------\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, alpha=0.1, beta=0.01, gamma=0.9):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # critic lr\n",
    "        self.beta = beta    # actor lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.V = defaultdict(float)\n",
    "        self.pi = defaultdict(lambda: np.ones(self.n_actions) / self.n_actions)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        probs = self.pi[state]\n",
    "        return np.random.choice(self.n_actions, p=probs)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "\n",
    "        # TD error\n",
    "        td_target = reward + self.gamma * self.V[next_state] * (1 - done)\n",
    "        td_error = td_target - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "\n",
    "        # Actor update\n",
    "        probs = self.pi[state]\n",
    "        one_hot = np.zeros_like(probs)\n",
    "        one_hot[action] = 1.0\n",
    "        self.pi[state] += self.beta * td_error * (one_hot - probs)\n",
    "\n",
    "        # Normalize\n",
    "        self.pi[state] = np.clip(self.pi[state], 1e-5, 1.0)\n",
    "        self.pi[state] /= np.sum(self.pi[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf779",
   "metadata": {},
   "source": [
    "## 3 PYGAME MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e30676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 3: Stable UI with Accurate Sliders (2-decimal, Updated Beta + MC LR) -----------------\n",
    "def draw_button(screen, rect, text, font, color=(180,220,255), border=2):\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, border)\n",
    "    txt = font.render(text, True, (0,0,0))\n",
    "    screen.blit(txt, (rect.x + (rect.width - txt.get_width())//2, rect.y + (rect.height - txt.get_height())//2))\n",
    "\n",
    "def draw_slider(screen, x, y, w, val, min_val, max_val, label, font, knob_r=10):\n",
    "    \"\"\"\n",
    "    Draws a slider and returns the knob rectangle.\n",
    "    - Ensures knob is computed from a clamped value.\n",
    "    - Displays value with 2 decimals.\n",
    "    \"\"\"\n",
    "    range_span = max_val - min_val if (max_val - min_val) != 0 else 1e-6\n",
    "    clamped_val = max(min_val, min(max_val, float(val)))\n",
    "    frac = (clamped_val - min_val) / range_span\n",
    "    knob_x = int(x + frac * w)\n",
    "\n",
    "    # track\n",
    "    track_rect = pygame.Rect(x, y, w, 6)\n",
    "    pygame.draw.rect(screen, (220,220,220), track_rect)\n",
    "\n",
    "    # knob\n",
    "    pygame.draw.circle(screen, (80,80,200), (knob_x, y+3), knob_r)\n",
    "\n",
    "    # label (2 decimals)\n",
    "    txt = font.render(f\"{label}: {clamped_val:.2f}\", True, (0,0,0))\n",
    "    screen.blit(txt, (x, y - 28))\n",
    "\n",
    "    return pygame.Rect(knob_x - knob_r, y+3 - knob_r, knob_r*2, knob_r*2)\n",
    "\n",
    "def draw_input_box(screen, rect, text, font, active):\n",
    "    color = (200,255,200) if active else (255,255,255)\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, 2)\n",
    "    txt_surface = font.render(str(text), True, (0,0,0))\n",
    "    screen.blit(txt_surface, (rect.x+5, rect.y+5))\n",
    "\n",
    "def menu_and_params(initial_params=None):\n",
    "    pygame_safe_init()\n",
    "    w,h = 980, 640\n",
    "    screen = pygame.display.set_mode((w,h))\n",
    "    pygame.display.set_caption('Locke N Key â€” Setup (Phase â†’ Algo â†’ Params)')\n",
    "    font = pygame.font.SysFont('arial', 22)\n",
    "    small = pygame.font.SysFont('arial', 16)\n",
    "\n",
    "    algo_choices = ['Q-Learning', 'Monte Carlo', 'Actor-Critic']\n",
    "    algo_idx = 0\n",
    "    phase_idx = 1\n",
    "\n",
    "    # --- Default parameters (MC_LR included) ---\n",
    "    params = {\n",
    "        'episodes': '100',\n",
    "        'alpha': 0.10,\n",
    "        'beta': 0.10,\n",
    "        'gamma': 0.90,\n",
    "        'epsilon': 0.20,\n",
    "        'MC_LR': 0.05,\n",
    "        'phase': phase_idx\n",
    "    }\n",
    "    if initial_params:\n",
    "        params.update(initial_params)\n",
    "\n",
    "    stage = 0\n",
    "    dragging = None\n",
    "    drag_offset = 0\n",
    "    active_box = False\n",
    "    input_rect = pygame.Rect(760, 250, 150, 36)\n",
    "    clock = pygame.time.Clock()\n",
    "    start_sim = False\n",
    "    running = True\n",
    "    knobs = {}\n",
    "\n",
    "    def clamp(v, a, b): return max(a, min(b, v))\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit(); raise SystemExit()\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_RIGHT:\n",
    "                    if stage == 0:\n",
    "                        phase_idx = min(5, phase_idx + 1)\n",
    "                    elif stage == 1:\n",
    "                        algo_idx = (algo_idx + 1) % len(algo_choices)\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    if stage == 0:\n",
    "                        phase_idx = max(1, phase_idx - 1)\n",
    "                    elif stage == 1:\n",
    "                        algo_idx = (algo_idx - 1) % len(algo_choices)\n",
    "                elif event.key == pygame.K_RETURN:\n",
    "                    if stage < 2:\n",
    "                        stage += 1\n",
    "                    else:\n",
    "                        start_sim = True\n",
    "                elif event.key == pygame.K_BACKSPACE:\n",
    "                    if active_box:\n",
    "                        params['episodes'] = params['episodes'][:-1]\n",
    "                    elif stage > 0:\n",
    "                        stage -= 1\n",
    "                elif active_box and event.unicode.isdigit():\n",
    "                    params['episodes'] += event.unicode\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                if input_rect.collidepoint(mx,my):\n",
    "                    active_box = True\n",
    "                else:\n",
    "                    active_box = False\n",
    "\n",
    "                if event.button == 1:\n",
    "                    for k, meta in knobs.items():\n",
    "                        if meta['rect'].collidepoint(mx, my):\n",
    "                            dragging = k\n",
    "                            knob_center_x = meta['rect'].centerx\n",
    "                            drag_offset = mx - knob_center_x\n",
    "                            break\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONUP:\n",
    "                dragging = None\n",
    "                drag_offset = 0\n",
    "\n",
    "            elif event.type == pygame.MOUSEMOTION and dragging:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                meta = knobs.get(dragging)\n",
    "                if meta is None:\n",
    "                    dragging = None\n",
    "                    continue\n",
    "\n",
    "                x, w = meta['x'], meta['w']\n",
    "                min_v, max_v = meta['min'], meta['max']\n",
    "                stick_x = clamp(mx - drag_offset, x, x + w)\n",
    "                rel = (stick_x - x) / float(w) if w != 0 else 0.0\n",
    "                new_val = round(min_v + rel * (max_v - min_v), 2)\n",
    "                new_val = clamp(new_val, min_v, max_v)\n",
    "                params[meta['param_key']] = new_val\n",
    "\n",
    "                knob_x = int(x + ((new_val - min_v) / (max_v - min_v if (max_v-min_v)!=0 else 1e-6)) * w)\n",
    "                knob_r = meta.get('knob_r', 10)\n",
    "                meta_rect = pygame.Rect(knob_x - knob_r, meta['y_center'] - knob_r, knob_r*2, knob_r*2)\n",
    "                meta['rect'] = meta_rect\n",
    "                knobs[dragging] = meta\n",
    "\n",
    "        # --- Draw UI ---\n",
    "        screen.fill((245,245,245))\n",
    "        title = font.render('Locke N Key â€” Setup (Phase â†’ Algo â†’ Params)', True, (20,20,20))\n",
    "        screen.blit(title, (40,20))\n",
    "\n",
    "        stage_texts = ['Select Phase (ENTER next)', 'Select Algorithm (ENTER next)', 'Tune Params / Start (ENTER start)']\n",
    "        screen.blit(small.render(stage_texts[stage], True, (60,60,60)), (40,60))\n",
    "\n",
    "        # Phase box\n",
    "        phase_box = pygame.Rect(50, 100, 300, 80)\n",
    "        pygame.draw.rect(screen, (200,230,200) if stage==0 else (220,220,220), phase_box)\n",
    "        pygame.draw.rect(screen, (0,0,0), phase_box, 2)\n",
    "        ptxt = font.render(f\"Phase: {phase_idx}\", True, (0,0,0))\n",
    "        screen.blit(ptxt, (phase_box.x + 20, phase_box.y + 20))\n",
    "        descs = {\n",
    "            1: \"Fixed positions (no enemy)\",\n",
    "            2: \"Random agent/key, fixed lock\",\n",
    "            3: \"Random agent/key/lock\",\n",
    "            4: \"Randomized tuning (like Phase 3)\",\n",
    "            5: \"Adds moving enemy hazard\"\n",
    "        }\n",
    "        screen.blit(small.render(descs[phase_idx], True, (40,40,40)), (phase_box.x + 20, phase_box.y + 48))\n",
    "\n",
    "        # Algorithm selection\n",
    "        algo_box = pygame.Rect(380, 100, 520, 80)\n",
    "        pygame.draw.rect(screen, (200,230,255) if stage==1 else (240,240,240), algo_box)\n",
    "        pygame.draw.rect(screen, (0,0,0), algo_box, 2)\n",
    "        algo_txt = font.render(f\"Algorithm: {algo_choices[algo_idx]}\", True, (0,0,0))\n",
    "        screen.blit(algo_txt, (algo_box.x + 20, algo_box.y + 20))\n",
    "\n",
    "        # Sliders\n",
    "        algo = algo_choices[algo_idx]\n",
    "        knobs.clear()\n",
    "        if stage == 2:\n",
    "            y = 220\n",
    "            track_x = 180\n",
    "            track_w = 480\n",
    "\n",
    "            if algo == 'Q-Learning':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params['alpha'], 0.001, 1.0, \"Alpha (Learning Rate)\", font)\n",
    "                knobs['alpha'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'alpha', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['epsilon'], 0.0, 1.0, \"Epsilon (Exploration)\", font)\n",
    "                knobs['epsilon'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.0, 'max':1.0, 'param_key':'epsilon', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "            elif algo == 'Monte Carlo':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params.get('MC_LR', 0.05), 0.001, 1.0, \"MC Learning Rate\", font)\n",
    "                knobs['MC_LR'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'MC_LR', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['epsilon'], 0.0, 1.0, \"Epsilon (Exploration)\", font)\n",
    "                knobs['epsilon'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.0, 'max':1.0, 'param_key':'epsilon', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "            elif algo == 'Actor-Critic':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params['alpha'], 0.001, 1.0, \"Alpha (Critic LR)\", font)\n",
    "                knobs['alpha'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'alpha', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['beta'], 0.001, 1.0, \"Beta (Actor LR)\", font)\n",
    "                knobs['beta'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'beta', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "        # Episodes input\n",
    "        draw_input_box(screen, input_rect, params['episodes'], font, active_box)\n",
    "        screen.blit(small.render(\"Episodes:\", True, (0,0,0)), (680, 260))\n",
    "\n",
    "        # Hint text\n",
    "        screen.blit(small.render('â†/â†’ to change â€¢ ENTER = Next/Start â€¢ BACKSPACE = Back â€¢ Drag sliders to adjust', True, (60,60,60)), (40, 580))\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(30)\n",
    "\n",
    "        if start_sim:\n",
    "            pygame.quit()\n",
    "            params['episodes'] = int(params['episodes']) if str(params['episodes']).isdigit() else 500\n",
    "            params['phase'] = phase_idx\n",
    "            return algo_choices[algo_idx], params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231edaf",
   "metadata": {},
   "source": [
    "## 4 ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642a05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 4: Simplified In-notebook Environment (with Image Placeholders + Speed Control + Distance Reward + Trailing Enemy) -----------------\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os, sys, random\n",
    "from collections import deque\n",
    "\n",
    "# Safe init for pygame (avoids repeated initialization errors in Jupyter)\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "        pygame.display.init()\n",
    "\n",
    "class LockKeyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Lock & Key Puzzle Environment\n",
    "    Now includes an enemy that follows the agent's trail (2â€“3 steps behind).\n",
    "    Reward shaping includes:\n",
    "      - Moving closer to the key (before pickup)\n",
    "      - Moving closer to the door (after having key)\n",
    "      - Staying farther from the enemy\n",
    "      - Major rewards for key pickup & door unlock\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 5}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\", size=6, phase=1, seed=None):\n",
    "        super().__init__()\n",
    "        assert phase in (1,2,3,4,5), \"Phase must be 1â€“5\"\n",
    "        self.size = size\n",
    "        self.phase = phase\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Window and grid parameters\n",
    "        self.window_size = 800\n",
    "        self.grid_size = 500\n",
    "        self.info_width = self.window_size - self.grid_size\n",
    "        self.cell_size = self.grid_size // self.size\n",
    "\n",
    "        # Spaces\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Discrete(self.size**6)\n",
    "\n",
    "        # Default object states\n",
    "        self._default_agent_pos = np.array([5, 0])\n",
    "        self._default_key_pos = np.array([5, 3])\n",
    "        self._default_lock_pos = np.array([0, 5])\n",
    "        self.walls = {(0,2),(1,1),(1,4),(2,3),(3,0),(3,2),(4,4)}\n",
    "\n",
    "        # Enemy\n",
    "        self.enemy_pos = None\n",
    "        self.enemy_active = (phase == 5)\n",
    "        self.trail = deque(maxlen=4)  # store last few agent positions for enemy following\n",
    "\n",
    "        # State variables\n",
    "        self.agent_pos = None\n",
    "        self.key_pos = None\n",
    "        self.lock_pos = None\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.last_reward = 0\n",
    "        self.current_episode = 0\n",
    "\n",
    "        # Track distances for shaping\n",
    "        self.prev_enemy_dist = None\n",
    "        self.prev_key_dist = None\n",
    "        self.prev_door_dist = None\n",
    "\n",
    "        # Rendering setup\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # --- Speed control ---\n",
    "        self.framerate = 5\n",
    "        self.unlimited_fps = False\n",
    "        self.speed_multiplier = 1.0\n",
    "        self.speed_levels = [0.5, 1, 2, 5, 10, 20, 30, 40, 50, 60]\n",
    "        self.current_speed_idx = 1  # index in speed_levels (1 => Ã—1)\n",
    "\n",
    "        # Episode control\n",
    "        self.max_steps = 400\n",
    "\n",
    "        # --- IMAGE PLACEHOLDERS ---\n",
    "        self.player_img = None\n",
    "        self.key_img = None\n",
    "        self.door_img = None\n",
    "        self.enemy_img = None\n",
    "        self.obstacle_img = None\n",
    "\n",
    "        # --- Reward configuration ---\n",
    "        self.STEP_PENALTY = -0.1\n",
    "        self.KEY_REWARD = 100.0\n",
    "        self.DOOR_REWARD = 300.0\n",
    "        self.CAUGHT_PENALTY = -200.0\n",
    "        self.SURVIVAL_BONUS = 0.00\n",
    "        self.DIST_SCALE = 0.05      # enemy distance shaping\n",
    "        self.APPROACH_SCALE = 0.1   # key/door distance shaping\n",
    "\n",
    "\n",
    "    # --- IMAGE LOADING SECTION ---\n",
    "    def _load_images(self):\n",
    "        pygame_safe_init()\n",
    "        if not pygame.display.get_init() or pygame.display.get_surface() is None:\n",
    "            pygame.display.set_mode((1, 1))\n",
    "\n",
    "        cwd = os.getcwd()\n",
    "        print(f\"[INFO] Looking for images in: {cwd}\")\n",
    "\n",
    "        def load_img(name):\n",
    "            try:\n",
    "                if os.path.exists(name):\n",
    "                    img = pygame.image.load(name).convert_alpha()\n",
    "                    print(f\"[OK] Loaded {name}\")\n",
    "                    return img\n",
    "                else:\n",
    "                    print(f\"[WARNING] {name} not found â€” using placeholder.\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to load {name}: {e}\")\n",
    "                return None\n",
    "\n",
    "        self.player_img = load_img(\"player.png\")\n",
    "        self.key_img = load_img(\"key.png\")\n",
    "        self.door_img = load_img(\"door.png\")\n",
    "        self.enemy_img = load_img(\"enemy.png\")\n",
    "        self.obstacle_img = load_img(\"obstacle.png\")\n",
    "\n",
    "\n",
    "    def handle_events(self, events):\n",
    "        for event in events:\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "                sys.exit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    self._increase_speed()\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self._decrease_speed()\n",
    "                elif event.key == pygame.K_ESCAPE:\n",
    "                    self.close()\n",
    "                    sys.exit()\n",
    "\n",
    "    # ---------- RESET ----------\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(seed)\n",
    "        if self.phase == 1:\n",
    "            self.agent_pos = self._default_agent_pos.copy()\n",
    "            self.key_pos = self._default_key_pos.copy()\n",
    "            self.lock_pos = self._default_lock_pos.copy()\n",
    "        elif self.phase == 2:\n",
    "            self.lock_pos = self._default_lock_pos.copy()\n",
    "            self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "            self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "        elif self.phase in (3,4,5):\n",
    "            self.lock_pos = self._random_free_cell()\n",
    "            self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "            self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "\n",
    "        if self.phase == 5:\n",
    "            exclude = {tuple(self.lock_pos), tuple(self.agent_pos), tuple(self.key_pos)}.union(self.walls)\n",
    "            self.enemy_pos = self._random_free_cell(exclude=exclude)\n",
    "            self.enemy_active = True\n",
    "            self.trail.clear()\n",
    "\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.last_reward = 0\n",
    "\n",
    "        # Initialize distance trackers\n",
    "        self.prev_enemy_dist = self._manhattan(self.agent_pos, self.enemy_pos) if self.enemy_active else None\n",
    "        self.prev_key_dist = self._manhattan(self.agent_pos, self.key_pos)\n",
    "        self.prev_door_dist = self._manhattan(self.agent_pos, self.lock_pos)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return obs, {}\n",
    "\n",
    "    # ---------- STEP ----------\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        reward = self.STEP_PENALTY\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Agent movement\n",
    "        move_map = {0:(1,0), 1:(-1,0), 2:(0,1), 3:(0,-1)}\n",
    "        if action in move_map:\n",
    "            dr, dc = move_map[action]\n",
    "            new_pos = self.agent_pos + np.array([dr, dc])\n",
    "            if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and (tuple(new_pos) not in self.walls):\n",
    "                # record current position before moving\n",
    "                self.trail.appendleft(self.agent_pos.copy())\n",
    "                self.agent_pos = new_pos\n",
    "\n",
    "        # Distance-based shaping: approach key/door\n",
    "        key_dist = self._manhattan(self.agent_pos, self.key_pos)\n",
    "        door_dist = self._manhattan(self.agent_pos, self.lock_pos)\n",
    "\n",
    "        if not self.has_key:\n",
    "            delta = self.prev_key_dist - key_dist\n",
    "            reward += self.APPROACH_SCALE * delta\n",
    "            self.prev_key_dist = key_dist\n",
    "        else:\n",
    "            delta = self.prev_door_dist - door_dist\n",
    "            reward += self.APPROACH_SCALE * delta\n",
    "            self.prev_door_dist = door_dist\n",
    "\n",
    "        # Pick up key\n",
    "        if not self.has_key and np.array_equal(self.agent_pos, self.key_pos):\n",
    "            self.has_key = True\n",
    "            reward += self.KEY_REWARD\n",
    "\n",
    "        # Unlock door (terminal success)\n",
    "        if self.has_key and np.array_equal(self.agent_pos, self.lock_pos):\n",
    "            reward += self.DOOR_REWARD\n",
    "            terminated = True\n",
    "\n",
    "        # Enemy pattern: follow agent trail\n",
    "        if self.phase == 5 and self.enemy_active:\n",
    "            if len(self.trail) >= 3:\n",
    "                target = self.trail[2]  # roughly 2â€“3 steps behind\n",
    "                self.enemy_pos = target.copy()\n",
    "            else:\n",
    "                self._move_enemy_random()  # fallback random movement early\n",
    "\n",
    "            # Enemy distance shaping\n",
    "            dist_now = self._manhattan(self.agent_pos, self.enemy_pos)\n",
    "            if self.prev_enemy_dist is not None:\n",
    "                delta = dist_now - self.prev_enemy_dist\n",
    "                reward += self.DIST_SCALE * delta\n",
    "            self.prev_enemy_dist = dist_now\n",
    "\n",
    "            # Collision\n",
    "            if np.array_equal(self.enemy_pos, self.agent_pos):\n",
    "                reward += self.CAUGHT_PENALTY\n",
    "                terminated = True\n",
    "\n",
    "        # Survival bonus\n",
    "        if not terminated:\n",
    "            reward += self.SURVIVAL_BONUS\n",
    "\n",
    "        # Timeout\n",
    "        info = {}\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "            info['timeout'] = True\n",
    "\n",
    "        # Outcome info\n",
    "        if terminated:\n",
    "            if self.phase == 5 and self.enemy_active and np.array_equal(self.enemy_pos, self.agent_pos):\n",
    "                info['caught'] = True\n",
    "            elif self.has_key and np.array_equal(self.agent_pos, self.lock_pos):\n",
    "                info['unlocked'] = True\n",
    "\n",
    "        self.last_reward = reward\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    # ---------- HELPERS ----------\n",
    "    def _random_free_cell(self, exclude=None):\n",
    "        if exclude is None:\n",
    "            exclude = set()\n",
    "        while True:\n",
    "            r, c = self._rng.integers(0, self.size, size=2)\n",
    "            if (r, c) not in self.walls and (r, c) not in exclude:\n",
    "                return np.array([r, c])\n",
    "\n",
    "    def _manhattan(self, p1, p2):\n",
    "        if p1 is None or p2 is None:\n",
    "            return 0\n",
    "        return abs(p1[0]-p2[0]) + abs(p1[1]-p2[1])\n",
    "\n",
    "    def _move_enemy_random(self):\n",
    "        directions = [(1,0),(-1,0),(0,1),(0,-1),(0,0)]\n",
    "        self._rng.shuffle(directions)\n",
    "        for dr, dc in directions:\n",
    "            cand = self.enemy_pos + np.array([dr, dc])\n",
    "            rr, cc = int(cand[0]), int(cand[1])\n",
    "            if 0 <= rr < self.size and 0 <= cc < self.size and (rr, cc) not in self.walls:\n",
    "                self.enemy_pos = np.array([rr, cc])\n",
    "                return\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            *self.agent_pos,\n",
    "            *self.key_pos,\n",
    "            *self.lock_pos,\n",
    "            int(self.has_key)\n",
    "        ])\n",
    "\n",
    "    # ---------- RENDER ----------\n",
    "    def _render_frame(self):\n",
    "        pygame_safe_init()\n",
    "        if self.player_img is None:\n",
    "            self._load_images()\n",
    "        if self.window is None:\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.grid_size))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size, self.grid_size))\n",
    "        canvas.fill((220, 220, 220))\n",
    "\n",
    "        for x in range(self.size+1):\n",
    "            pygame.draw.line(canvas, (0,0,0), (0, x*self.cell_size), (self.grid_size, x*self.cell_size), 1)\n",
    "            pygame.draw.line(canvas, (0,0,0), (x*self.cell_size, 0), (x*self.cell_size, self.grid_size), 1)\n",
    "\n",
    "        for r, c in self.walls:\n",
    "            rect = pygame.Rect(c*self.cell_size, r*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.obstacle_img:\n",
    "                scaled = pygame.transform.scale(self.obstacle_img, (self.cell_size, self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.rect(canvas, (100,100,100), rect)\n",
    "\n",
    "        if not self.has_key:\n",
    "            rect = pygame.Rect(self.key_pos[1]*self.cell_size, self.key_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.key_img:\n",
    "                scaled = pygame.transform.scale(self.key_img, (self.cell_size, self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.circle(canvas, (255,215,0), rect.center, max(6, self.cell_size//4))\n",
    "\n",
    "        rect = pygame.Rect(self.lock_pos[1]*self.cell_size, self.lock_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "        if self.door_img:\n",
    "            scaled = pygame.transform.scale(self.door_img, (self.cell_size, self.cell_size))\n",
    "            canvas.blit(scaled, rect.topleft)\n",
    "        else:\n",
    "            pygame.draw.rect(canvas, (200,50,50), rect)\n",
    "\n",
    "        rect = pygame.Rect(self.agent_pos[1]*self.cell_size, self.agent_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "        if self.player_img:\n",
    "            scaled = pygame.transform.scale(self.player_img, (self.cell_size, self.cell_size))\n",
    "            canvas.blit(scaled, rect.topleft)\n",
    "        else:\n",
    "            pygame.draw.circle(canvas, (50,100,255), rect.center, max(6, self.cell_size//3))\n",
    "\n",
    "        if self.phase == 5 and self.enemy_active and self.enemy_pos is not None:\n",
    "            rect = pygame.Rect(self.enemy_pos[1]*self.cell_size, self.enemy_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.enemy_img:\n",
    "                scaled = pygame.transform.scale(self.enemy_img, (self.cell_size, self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.circle(canvas, (255,0,0), rect.center, max(6, self.cell_size//3))\n",
    "\n",
    "        info_panel = pygame.Surface((self.info_width, self.grid_size))\n",
    "        info_panel.fill((245,245,245))\n",
    "        font = pygame.font.SysFont(\"arial\", 18)\n",
    "        lines = [\n",
    "            f\"Phase: {self.phase}\",\n",
    "            f\"Steps: {self.steps}\",\n",
    "            f\"Reward: {round(self.last_reward, 2)}\",\n",
    "            f\"Has Key: {'Yes' if self.has_key else 'No'}\",\n",
    "            f\"Speed: Ã—{self.speed_multiplier}\"\n",
    "        ]\n",
    "        for i, text in enumerate(lines):\n",
    "            info_panel.blit(font.render(text, True, (0,0,0)), (10, 20 + i*28))\n",
    "\n",
    "        self.window.blit(canvas, (0,0))\n",
    "        self.window.blit(info_panel, (self.grid_size, 0))\n",
    "        pygame.display.flip()\n",
    "\n",
    "        if not self.unlimited_fps:\n",
    "            effective_fps = max(1, int(self.framerate * self.speed_multiplier))\n",
    "            self.clock.tick(effective_fps)\n",
    "\n",
    "    # ---------- SPEED CONTROL ----------\n",
    "    def _increase_speed(self):\n",
    "        if self.current_speed_idx < len(self.speed_levels) - 1:\n",
    "            self.current_speed_idx += 1\n",
    "            self.speed_multiplier = self.speed_levels[self.current_speed_idx]\n",
    "            print(f\"[INFO] Increased speed to Ã—{self.speed_multiplier}\")\n",
    "\n",
    "    def _decrease_speed(self):\n",
    "        if self.current_speed_idx > 0:\n",
    "            self.current_speed_idx -= 1\n",
    "            self.speed_multiplier = self.speed_levels[self.current_speed_idx]\n",
    "            print(f\"[INFO] Decreased speed to Ã—{self.speed_multiplier}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1d161",
   "metadata": {},
   "source": [
    "## 5 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 5: Run the whole flow (menu -> train). Training loop no plotting. -----------------\n",
    "def train_and_visualize(algo_name, params, save_path=\"lockkey_policy.pkl\"):\n",
    "    pygame_safe_init()\n",
    "\n",
    "    # --- Environment setup (now using the in-notebook LockKeyEnv) ---\n",
    "    env = LockKeyEnv(render_mode='human', size=6, phase=int(params.get('phase', 5)))\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    episodes = int(params.get('episodes', 800))\n",
    "    gamma = float(params.get('gamma', 0.9))\n",
    "\n",
    "    # --- Agent creation ---\n",
    "    if algo_name == 'Q-Learning':\n",
    "        agent = QLearningAgent(n_states, n_actions,\n",
    "                               alpha=float(params.get('alpha', 0.1)),\n",
    "                               gamma=gamma,\n",
    "                               epsilon=float(params.get('epsilon', 0.2)))\n",
    "    elif algo_name == 'Monte Carlo':\n",
    "        agent = MonteCarloAgent(n_states, n_actions,\n",
    "                                gamma=gamma,\n",
    "                                epsilon=float(params.get('epsilon', 0.1)))\n",
    "        # if MC has a learning rate param, store it on agent (optional usage)\n",
    "        if 'MC_LR' in params:\n",
    "            agent.mc_lr = float(params.get('MC_LR', 0.05))\n",
    "    else:  # Actor-Critic\n",
    "        agent = ActorCriticAgent(n_states, n_actions,\n",
    "                                 alpha=float(params.get('alpha', 0.1)),   # critic lr\n",
    "                                 beta=float(params.get('beta', 0.01)),    # actor lr\n",
    "                                 gamma=gamma)\n",
    "\n",
    "    episode_rewards, successes, episode_lengths = [], [], []\n",
    "\n",
    "    pygame_safe_init()\n",
    "    font = pygame.font.SysFont('arial', 18)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # --- Main training loop ---\n",
    "        for ep in range(1, episodes + 1):\n",
    "            # fetch events once now; pass them into env later each frame\n",
    "            obs, _ = env.reset()\n",
    "            env.current_episode = ep\n",
    "            done = False\n",
    "            total_r = 0\n",
    "            steps = 0\n",
    "            episode_hist = []\n",
    "\n",
    "            if isinstance(obs, tuple) and len(obs) == 2:\n",
    "                obs = obs[0]\n",
    "\n",
    "            while not done and steps < 200:\n",
    "                # fetch events once per frame\n",
    "                events = pygame.event.get()\n",
    "                # handle quit/escape centrally (avoid consuming events)\n",
    "                for ev in events:\n",
    "                    if ev.type == pygame.QUIT:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "                    elif ev.type == pygame.KEYDOWN and ev.key == pygame.K_ESCAPE:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "\n",
    "                # let environment consume events it needs (speed keys)\n",
    "                env.handle_events(events)\n",
    "\n",
    "                # Agent selects action\n",
    "                s = obs\n",
    "                a = agent.select_action(s) if hasattr(agent, 'select_action') else agent.policy(s)\n",
    "                obs2, r, done, trunc, info = env.step(a)\n",
    "                total_r += r\n",
    "                steps += 1\n",
    "\n",
    "                if isinstance(agent, MonteCarloAgent):\n",
    "                    episode_hist.append((s, a, r))\n",
    "                else:\n",
    "                    agent.update(s, a, r, obs2, done)\n",
    "\n",
    "                obs = obs2\n",
    "\n",
    "                # Render env (HUD handled by env._render_frame)\n",
    "                try:\n",
    "                    env.render()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Console updates occasionally\n",
    "                if steps % 30 == 0:\n",
    "                    print(f\"Ep {ep} | Step {steps:03d} | Reward={total_r:.2f} | Eps={getattr(agent, 'epsilon', 0):.2f}\", end='\\r')\n",
    "\n",
    "            # End of episode\n",
    "            if isinstance(agent, MonteCarloAgent):\n",
    "                agent.update(episode_hist)\n",
    "\n",
    "            if hasattr(agent, 'epsilon'):\n",
    "                agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
    "\n",
    "            episode_rewards.append(total_r)\n",
    "            successes.append(1 if done and total_r > 0 else 0)\n",
    "            episode_lengths.append(steps)\n",
    "            \n",
    "            # Detect episode outcome\n",
    "            if env.phase == 5 and env.enemy_active and np.array_equal(env.agent_pos, env.enemy_pos):\n",
    "                outcome = \"âŒ Caught by Enemy\"\n",
    "            elif np.array_equal(env.agent_pos, env.lock_pos) and env.has_key:\n",
    "                outcome = \"ğŸ Door Unlocked\"\n",
    "            elif not env.has_key and np.array_equal(env.agent_pos, env.key_pos):\n",
    "                outcome = \"ğŸ”‘ Picked up Key\"\n",
    "            else:\n",
    "                outcome = \"ğŸŒ€ Episode Ended (Timeout or Random Termination)\"\n",
    "\n",
    "            print(f\"âœ… Episode {ep}/{episodes} finished | Reward={total_r:.2f} | Steps={steps} | {outcome}\")\n",
    "\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            env.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Save policy (unchanged) ---\n",
    "    try:\n",
    "        def safe_convert(obj):\n",
    "            if isinstance(obj, defaultdict):\n",
    "                obj = {k: safe_convert(v) for k, v in obj.items()}\n",
    "            return obj\n",
    "\n",
    "        data_to_save = None\n",
    "        if algo_name in ['Q-Learning', 'Monte Carlo']:\n",
    "            data_to_save = safe_convert(agent.Q)\n",
    "        else:\n",
    "            data_to_save = {'V': safe_convert(agent.V), 'pi': safe_convert(agent.pi)}\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)\n",
    "        print(f\"âœ… Saved {algo_name} policy successfully to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Warning: Could not save policy due to {e}\")\n",
    "\n",
    "    return episode_rewards, successes, episode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20c6e5",
   "metadata": {},
   "source": [
    "## 6 RUN WHOLE FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen algorithm: Q-Learning\n",
      "Parameters: {'episodes': 100, 'alpha': 0.1, 'beta': 0.1, 'gamma': 0.9, 'epsilon': 0.2, 'MC_LR': 0.05, 'phase': 1}\n",
      "[INFO] Looking for images in: c:\\Users\\chescake\\Documents\\NU\\SY 2025 - 2026\\1ST TERM\\REINFORCEMENT LEARNING\\RL TEST\\LOCK_KEY\n",
      "[OK] Loaded player.png\n",
      "[OK] Loaded key.png\n",
      "[OK] Loaded door.png\n",
      "[OK] Loaded enemy.png\n",
      "[OK] Loaded obstacle.png\n",
      "[INFO] Increased speed to Ã—2\n",
      "[INFO] Increased speed to Ã—5\n",
      "[INFO] Increased speed to Ã—10\n",
      "[INFO] Increased speed to Ã—20\n",
      "[INFO] Increased speed to Ã—30\n",
      "[INFO] Increased speed to Ã—400 | Eps=0.20\n",
      "âœ… Episode 1/100 finished | Reward=397.10 | Steps=42 | ğŸ Door Unlocked\n",
      "âœ… Episode 2/100 finished | Reward=399.80 | Steps=15 | ğŸ Door Unlocked\n",
      "[INFO] Increased speed to Ã—50\n",
      "âœ… Episode 3/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 4/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "[INFO] Increased speed to Ã—60\n",
      "âœ… Episode 5/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 6/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 7/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 8/100 finished | Reward=399.60 | Steps=17 | ğŸ Door Unlocked\n",
      "âœ… Episode 9/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 10/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 11/100 finished | Reward=399.70 | Steps=16 | ğŸ Door Unlocked\n",
      "âœ… Episode 12/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 13/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 14/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 15/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 16/100 finished | Reward=399.70 | Steps=16 | ğŸ Door Unlocked\n",
      "âœ… Episode 17/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 18/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 19/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 20/100 finished | Reward=399.70 | Steps=16 | ğŸ Door Unlocked\n",
      "âœ… Episode 21/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 22/100 finished | Reward=399.60 | Steps=17 | ğŸ Door Unlocked\n",
      "âœ… Episode 23/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 24/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 25/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 26/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 27/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 28/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 29/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 30/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 31/100 finished | Reward=399.80 | Steps=15 | ğŸ Door Unlocked\n",
      "âœ… Episode 32/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 33/100 finished | Reward=399.80 | Steps=15 | ğŸ Door Unlocked\n",
      "âœ… Episode 34/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 35/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 36/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 37/100 finished | Reward=399.70 | Steps=16 | ğŸ Door Unlocked\n",
      "âœ… Episode 38/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 39/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 40/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 41/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 42/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 43/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 44/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 45/100 finished | Reward=399.80 | Steps=15 | ğŸ Door Unlocked\n",
      "âœ… Episode 46/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 47/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 48/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 49/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 50/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 51/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 52/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 53/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 54/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 55/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 56/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 57/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 58/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 59/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 60/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 61/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 62/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 63/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 64/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 65/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 66/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 67/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 68/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 69/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 70/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 71/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 72/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 73/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 74/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 75/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 76/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 77/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 78/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 79/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 80/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 81/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 82/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 83/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 84/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 85/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 86/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 87/100 finished | Reward=400.00 | Steps=13 | ğŸ Door Unlocked\n",
      "âœ… Episode 88/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 89/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 90/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Episode 91/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 92/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 93/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 94/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 95/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 96/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 97/100 finished | Reward=400.30 | Steps=10 | ğŸ Door Unlocked\n",
      "âœ… Episode 98/100 finished | Reward=399.90 | Steps=14 | ğŸ Door Unlocked\n",
      "âœ… Episode 99/100 finished | Reward=400.20 | Steps=11 | ğŸ Door Unlocked\n",
      "âœ… Episode 100/100 finished | Reward=400.10 | Steps=12 | ğŸ Door Unlocked\n",
      "âœ… Saved Q-Learning policy successfully to lockkey_policy.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ec9333ddb0e3>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train (this opens the environment window)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_visualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lockkey_policy.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training complete. Saved policy.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# CELL 6: Run the menu and training\n",
    "# Run this cell to open the Pygame menu, choose phase & algorithm & params, then train.\n",
    "\n",
    "algo_name, params = menu_and_params()\n",
    "print(\"Chosen algorithm:\", algo_name)\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Visuals mapping placeholder (not currently used in the env code,\n",
    "# but retained if you want to pass custom colors or images later)\n",
    "visuals = {\n",
    "    'agent_color': (50,100,255),\n",
    "    'key_color': (255,215,0),\n",
    "    'lock_color': (200,50,50),\n",
    "    'wall_color': (80,80,80),\n",
    "    'enemy_color': (0,0,255),\n",
    "    'bg_color': (230,230,230)\n",
    "}\n",
    "\n",
    "# Train (this opens the environment window)\n",
    "rewards, successes, episode_lengths = train_and_visualize(algo_name, params, save_path='lockkey_policy.pkl')\n",
    "print(\"Training complete. Saved policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51965fa",
   "metadata": {},
   "source": [
    "## 7 RESULTS GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5646c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-42b3569feb43>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m# === 3) Run plotting and metrics (use variables from CELL 6) ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mplot_learning_curves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[0manalyze_best_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------- CELL 7: Results & Comparison (plots + optional summary) -----------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# === 1) Plot Learning Curves (Raw + Smoothed Rewards + Success Rate + Episode Lengths) ===\n",
    "def plot_learning_curves(rewards, successes, episode_lengths, algo_name, smooth_window=10):\n",
    "    episodes = np.arange(1, len(rewards) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.suptitle(f\"{algo_name} - Training Performance\", fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 1ï¸âƒ£ Raw + Smoothed Reward Curve\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(episodes, rewards, marker='o', alpha=0.6, label='Raw Reward')\n",
    "    if len(rewards) >= smooth_window:\n",
    "        smooth_rewards = np.convolve(rewards, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "        x = np.arange(smooth_window, len(rewards)+1)\n",
    "        plt.plot(x, smooth_rewards, color='orange', linewidth=2, label=f'Smoothed (window={smooth_window})')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Rewards (Raw + Smoothed)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # 2ï¸âƒ£ Success Rate Curve\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if len(successes) > 0:\n",
    "        success_rate = np.cumsum(successes) / np.arange(1, len(successes)+1)\n",
    "        plt.plot(np.arange(1, len(successes)+1), success_rate, color='green', label='Success Rate')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Success Rate\")\n",
    "    plt.title(\"Success Rate Over Time\")\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 3ï¸âƒ£ Episode Length Curve\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(episodes, episode_lengths, color='purple', marker='x', alpha=0.6, label='Episode Length')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.title(\"Episode Lengths\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === 2) Extract Best Metrics ===\n",
    "def analyze_best_metrics(rewards, successes, episode_lengths, algo_name, smooth_window=10):\n",
    "    # Raw rewards\n",
    "    best_raw = np.max(rewards)\n",
    "    best_raw_ep = np.argmax(rewards) + 1\n",
    "\n",
    "    # Smoothed rewards\n",
    "    if len(rewards) >= smooth_window:\n",
    "        smooth_rewards = np.convolve(rewards, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "        best_smooth = np.max(smooth_rewards)\n",
    "        best_smooth_ep = np.argmax(smooth_rewards) + smooth_window\n",
    "    else:\n",
    "        best_smooth = np.max(rewards)\n",
    "        best_smooth_ep = np.argmax(rewards) + 1\n",
    "\n",
    "    # Success rate\n",
    "    success_rate = np.cumsum(successes) / np.arange(1, len(successes)+1)\n",
    "    best_success = np.max(success_rate)\n",
    "    best_success_ep = np.argmax(success_rate) + 1\n",
    "\n",
    "    # Episode length\n",
    "    best_length = np.max(episode_lengths)\n",
    "    best_length_ep = np.argmax(episode_lengths) + 1\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ {algo_name} - Best Performance Metrics\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(f\"  ğŸŸ¦ Best Raw Reward: {best_raw:.3f}  (Episode {best_raw_ep})\")\n",
    "    print(f\"  ğŸŸ§ Best Smoothed Reward: {best_smooth:.3f}  (Episode {best_smooth_ep})\")\n",
    "    print(f\"  ğŸŸ© Best Success Rate: {best_success:.3f}  (Episode {best_success_ep})\")\n",
    "    print(f\"  ğŸŸª Longest Episode: {best_length} steps  (Episode {best_length_ep})\")\n",
    "    print(f\"  Average Episode Length: {avg_length:.2f} steps\")\n",
    "\n",
    "\n",
    "# === 3) Run plotting and metrics (use variables from CELL 6) ===\n",
    "plot_learning_curves(rewards, successes, episode_lengths, algo_name)\n",
    "analyze_best_metrics(rewards, successes, episode_lengths, algo_name)\n",
    "\n",
    "# added here episode lengths and average length to the metrics summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc90e5",
   "metadata": {},
   "source": [
    "# FOR PERCENTAGE OF EVAL (TO BE EDITED PER PHASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Converted Performance to Percentage Form ===\n",
      "       Setting  Best Raw Reward (%)  Best Smoothed Reward (%)  \\\n",
      "0      Default                99.35                     93.04   \n",
      "1  High LR (Î±)               100.00                     92.39   \n",
      "2   Low LR (Î±)                99.02                     93.56   \n",
      "3  High ER (Îµ)                96.08                     84.64   \n",
      "4   Low ER (Îµ)                99.35                     95.98   \n",
      "5  High DF (Î³)               100.00                     94.74   \n",
      "6   Low DF (Î³)                99.35                     92.88   \n",
      "\n",
      "   Best Success Rate (%)  Average Episode Length (%)  \n",
      "0                  100.0                       44.13  \n",
      "1                  100.0                       52.63  \n",
      "2                   89.0                       35.74  \n",
      "3                   76.0                        0.00  \n",
      "4                  100.0                       46.09  \n",
      "5                  100.0                       43.86  \n",
      "6                  100.0                       44.66  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Define your max constants (adjust if needed) ===\n",
    "MAX_REWARD = 30.600   # highest achievable reward observed\n",
    "MAX_STEPS = 200     # your environment's timeout step\n",
    "\n",
    "# === 2. Input your metrics per parameter setting ===\n",
    "data = [\n",
    "    {\n",
    "        \"Setting\": \"Default\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 28.470,\n",
    "        \"Best Success Rate\": 1.000,   # already 0â€“1\n",
    "        \"Average Episode Length\": 56.880\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High LR (Î±)\",\n",
    "        \"Best Raw Reward\": 30.600,\n",
    "        \"Best Smoothed Reward\": 28.270,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 48.22\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low LR (Î±)\",\n",
    "        \"Best Raw Reward\": 30.300,\n",
    "        \"Best Smoothed Reward\": 28.630,\n",
    "        \"Best Success Rate\": 0.890,\n",
    "        \"Average Episode Length\": 65.42\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High ER (Îµ)\",\n",
    "        \"Best Raw Reward\": 29.400,\n",
    "        \"Best Smoothed Reward\": 25.900,\n",
    "        \"Best Success Rate\": 0.760,\n",
    "        \"Average Episode Length\": 101.80\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low ER (Îµ)\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 29.370,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 54.88\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High DF (Î³)\",\n",
    "        \"Best Raw Reward\": 30.600,\n",
    "        \"Best Smoothed Reward\": 28.990,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 57.15\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low DF (Î³)\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 28.420,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 56.34\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === 3. Convert to percentage values ===\n",
    "df[\"Best Raw Reward (%)\"] = (df[\"Best Raw Reward\"] / MAX_REWARD) * 100\n",
    "df[\"Best Smoothed Reward (%)\"] = (df[\"Best Smoothed Reward\"] / MAX_REWARD) * 100\n",
    "df[\"Best Success Rate (%)\"] = df[\"Best Success Rate\"] * 100\n",
    "df[\"Average Episode Length (%)\"] = (1 - (df[\"Average Episode Length\"] / MAX_STEPS)) * 100  # efficiency\n",
    "\n",
    "# === 4. Select and display percentage columns only ===\n",
    "percent_df = df[[\"Setting\",\n",
    "                 \"Best Raw Reward (%)\",\n",
    "                 \"Best Smoothed Reward (%)\",\n",
    "                 \"Best Success Rate (%)\",\n",
    "                 \"Average Episode Length (%)\"]]\n",
    "\n",
    "print(\"\\n=== Converted Performance to Percentage Form ===\")\n",
    "print(percent_df.round(2))\n",
    "\n",
    "# for evaluation only; not part of main flow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympiaCOM222ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
