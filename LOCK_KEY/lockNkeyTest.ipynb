{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3066cb63",
   "metadata": {},
   "source": [
    "# PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d9aaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\renza\\facerecog\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\renza\\facerecog\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from gymnasium) (2.2.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from gymnasium) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\renza\\facerecog\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\renza\\facerecog\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\renza\\facerecog\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\renza\\facerecog\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\renza\\facerecog\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\renza\\facerecog\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3a805",
   "metadata": {},
   "source": [
    "# LOCKANDKEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e17e48",
   "metadata": {},
   "source": [
    "## 1 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f294dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Imports & helpers\n",
    "# Paste in first cell\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Pygame used for menu + visualization\n",
    "import pygame\n",
    "\n",
    "# Small safe initializer used before creating fonts/displays\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "    if not pygame.font.get_init():\n",
    "        pygame.font.init()\n",
    "\n",
    "# small helper: moving average\n",
    "def moving_avg(arr, window):\n",
    "    if len(arr) < window:\n",
    "        return np.array(arr)\n",
    "    return np.convolve(arr, np.ones(window)/window, mode='valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9178f7",
   "metadata": {},
   "source": [
    "## 2 RL AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb7e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: RL Agents (Q-Learning, Monte Carlo, Actor-Critic)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Utility to make states hashable ---\n",
    "def hashable_state(s):\n",
    "    \"\"\"\n",
    "    Convert dict, ndarray, or list observation into a hashable key.\n",
    "    For our env the observation is an integer, so hashing is trivial.\n",
    "    This helper keeps the agent code general.\n",
    "    \"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return tuple(sorted((k, hashable_state(v)) for k, v in s.items()))\n",
    "    elif isinstance(s, np.ndarray):\n",
    "        return tuple(map(float, s.flatten()))\n",
    "    elif isinstance(s, (list, tuple)):\n",
    "        return tuple(map(hashable_state, s))\n",
    "    else:\n",
    "        try:\n",
    "            hash(s)\n",
    "            return s\n",
    "        except TypeError:\n",
    "            return str(s)\n",
    "\n",
    "\n",
    "# ----------------- Q-Learning Agent -----------------\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "        target = reward + self.gamma * np.max(self.Q[next_state]) * (1 - done)\n",
    "        current = self.Q[state][action]\n",
    "        self.Q[state][action] = current + self.alpha * (target - current)\n",
    "\n",
    "\n",
    "# ----------------- Monte Carlo Agent -----------------\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, gamma=0.9, epsilon=0.1, alpha=0.05):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # NEW: learning rate\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "\n",
    "    def update(self, episode):\n",
    "        \"\"\"Episode is a list of (state, action, reward).\"\"\"\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            state = hashable_state(state)\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                old_value = self.Q[state][action]\n",
    "                # Instead of averaging, apply incremental update\n",
    "                self.Q[state][action] = old_value + self.alpha * (G - old_value)\n",
    "                visited.add((state, action))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- Actor-Critic Agent -----------------\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, n_states=None, n_actions=6, alpha=0.1, beta=0.01, gamma=0.9):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # critic lr\n",
    "        self.beta = beta    # actor lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.V = defaultdict(float)\n",
    "        self.pi = defaultdict(lambda: np.ones(self.n_actions) / self.n_actions)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = hashable_state(state)\n",
    "        probs = self.pi[state]\n",
    "        return np.random.choice(self.n_actions, p=probs)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = hashable_state(state)\n",
    "        next_state = hashable_state(next_state)\n",
    "\n",
    "        # TD error\n",
    "        td_target = reward + self.gamma * self.V[next_state] * (1 - done)\n",
    "        td_error = td_target - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "\n",
    "        # Actor update\n",
    "        probs = self.pi[state]\n",
    "        one_hot = np.zeros_like(probs)\n",
    "        one_hot[action] = 1.0\n",
    "        self.pi[state] += self.beta * td_error * (one_hot - probs)\n",
    "\n",
    "        # Normalize\n",
    "        self.pi[state] = np.clip(self.pi[state], 1e-5, 1.0)\n",
    "        self.pi[state] /= np.sum(self.pi[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddf779",
   "metadata": {},
   "source": [
    "## 3 PYGAME MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e30676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 3: Stable UI with Accurate Sliders (2-decimal, Updated Beta + MC LR) -----------------\n",
    "def draw_button(screen, rect, text, font, color=(180,220,255), border=2):\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, border)\n",
    "    txt = font.render(text, True, (0,0,0))\n",
    "    screen.blit(txt, (rect.x + (rect.width - txt.get_width())//2, rect.y + (rect.height - txt.get_height())//2))\n",
    "\n",
    "def draw_slider(screen, x, y, w, val, min_val, max_val, label, font, knob_r=10):\n",
    "    \"\"\"\n",
    "    Draws a slider and returns the knob rectangle.\n",
    "    - Ensures knob is computed from a clamped value.\n",
    "    - Displays value with 2 decimals.\n",
    "    \"\"\"\n",
    "    range_span = max_val - min_val if (max_val - min_val) != 0 else 1e-6\n",
    "    clamped_val = max(min_val, min(max_val, float(val)))\n",
    "    frac = (clamped_val - min_val) / range_span\n",
    "    knob_x = int(x + frac * w)\n",
    "\n",
    "    # track\n",
    "    track_rect = pygame.Rect(x, y, w, 6)\n",
    "    pygame.draw.rect(screen, (220,220,220), track_rect)\n",
    "\n",
    "    # knob\n",
    "    pygame.draw.circle(screen, (80,80,200), (knob_x, y+3), knob_r)\n",
    "\n",
    "    # label (2 decimals)\n",
    "    txt = font.render(f\"{label}: {clamped_val:.2f}\", True, (0,0,0))\n",
    "    screen.blit(txt, (x, y - 28))\n",
    "\n",
    "    return pygame.Rect(knob_x - knob_r, y+3 - knob_r, knob_r*2, knob_r*2)\n",
    "\n",
    "def draw_input_box(screen, rect, text, font, active):\n",
    "    color = (200,255,200) if active else (255,255,255)\n",
    "    pygame.draw.rect(screen, color, rect)\n",
    "    pygame.draw.rect(screen, (0,0,0), rect, 2)\n",
    "    txt_surface = font.render(str(text), True, (0,0,0))\n",
    "    screen.blit(txt_surface, (rect.x+5, rect.y+5))\n",
    "\n",
    "def menu_and_params(initial_params=None):\n",
    "    pygame_safe_init()\n",
    "    w,h = 980, 640\n",
    "    screen = pygame.display.set_mode((w,h))\n",
    "    pygame.display.set_caption('Lock N Key — Setup')\n",
    "    \n",
    "    font = pygame.font.SysFont('verdana', 22)\n",
    "    small = pygame.font.SysFont('verdana', 16)\n",
    "    \n",
    "    header_font = pygame.font.SysFont('verdana', 36, bold=True)\n",
    "\n",
    "    algo_choices = ['Q-Learning', 'Monte Carlo', 'Actor-Critic']\n",
    "    algo_idx = 0\n",
    "    phase_idx = 1\n",
    "\n",
    "    # --- Default parameters (MC_LR included) ---\n",
    "    params = {\n",
    "        'episodes': '100',\n",
    "        'alpha': 0.10,\n",
    "        'beta': 0.10,\n",
    "        'gamma': 0.90,\n",
    "        'epsilon': 0.20,\n",
    "        'MC_LR': 0.05,\n",
    "        'phase': phase_idx\n",
    "    }\n",
    "    if initial_params:\n",
    "        params.update(initial_params)\n",
    "\n",
    "    stage = 0\n",
    "    dragging = None\n",
    "    drag_offset = 0\n",
    "    active_box = False\n",
    "    input_rect = pygame.Rect(280, 520, 420, 40)\n",
    "    clock = pygame.time.Clock()\n",
    "    start_sim = False\n",
    "    running = True\n",
    "    knobs = {}\n",
    "\n",
    "    def clamp(v, a, b): return max(a, min(b, v))\n",
    "    \n",
    "    \n",
    "    # Phase specific L/R buttons (positioned near Phase box)\n",
    "    phase_left_btn_rect = pygame.Rect(200, 160, 60, 30)\n",
    "    phase_right_btn_rect = pygame.Rect(720, 160, 60, 30)\n",
    "    \n",
    "    # Algo specific L/R buttons (positioned near Algo box)\n",
    "    algo_left_btn_rect = pygame.Rect(200, 250, 60, 30)\n",
    "    algo_right_btn_rect = pygame.Rect(720, 250, 60, 30)\n",
    "    \n",
    "    # Global stage navigation buttons\n",
    "    back_stage_btn_rect = pygame.Rect(720, 50, 100, 40)\n",
    "    next_stage_btn_rect = pygame.Rect(830, 50, 100, 40)\n",
    "\n",
    "    reset_btn_rect = pygame.Rect(720, 520, 100, 40)\n",
    "    start_btn_rect = pygame.Rect(830, 520, 100, 40)\n",
    "    \n",
    "    \n",
    "    def reset_params():\n",
    "        nonlocal params, phase_idx, algo_idx, stage\n",
    "        params = {\n",
    "            'episodes': '100',\n",
    "            'alpha': 0.10,\n",
    "            'beta': 0.10,\n",
    "            'gamma': 0.90,\n",
    "            'epsilon': 0.20,\n",
    "            'MC_LR': 0.05,\n",
    "            'phase': 1\n",
    "        }\n",
    "        phase_idx = 1\n",
    "        algo_idx = 0\n",
    "        stage = 0\n",
    "        \n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit(); raise SystemExit()\n",
    "\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_RIGHT:\n",
    "                    if stage == 0:\n",
    "                        phase_idx = min(5, phase_idx + 1)\n",
    "                    elif stage == 1:\n",
    "                        algo_idx = (algo_idx + 1) % len(algo_choices)\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    if stage == 0:\n",
    "                        phase_idx = max(1, phase_idx - 1)\n",
    "                    elif stage == 1:\n",
    "                        algo_idx = (algo_idx - 1) % len(algo_choices)\n",
    "                elif event.key == pygame.K_RETURN:\n",
    "                    if stage < 2:\n",
    "                        stage += 1\n",
    "                    else:\n",
    "                        start_sim = True\n",
    "                elif event.key == pygame.K_BACKSPACE:\n",
    "                    if active_box:\n",
    "                        params['episodes'] = params['episodes'][:-1]\n",
    "                    elif stage > 0:\n",
    "                        stage -= 1\n",
    "                elif active_box and event.unicode.isdigit():\n",
    "                    params['episodes'] += event.unicode\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                if input_rect.collidepoint(mx,my):\n",
    "                    active_box = True\n",
    "                else:\n",
    "                    active_box = False\n",
    "\n",
    "                # if event.button == 1:\n",
    "                #     for k, meta in knobs.items():\n",
    "                #         if meta['rect'].collidepoint(mx, my):\n",
    "                #             dragging = k\n",
    "                #             knob_center_x = meta['rect'].centerx\n",
    "                #             drag_offset = mx - knob_center_x\n",
    "                #             break\n",
    "                \n",
    "                # --- Button Clicks ---\n",
    "                if event.button == 1:\n",
    "                    # 1. Global Stage Navigation\n",
    "                    if stage < 2 and next_stage_btn_rect.collidepoint(mx, my):\n",
    "                        stage += 1\n",
    "                    elif stage > 0 and back_stage_btn_rect.collidepoint(mx, my):\n",
    "                        stage -= 1\n",
    "                    \n",
    "                    # 2. Start/Reset Buttons (Stage 2)\n",
    "                    elif stage == 2 and start_btn_rect.collidepoint(mx, my):\n",
    "                        start_sim = True\n",
    "                    elif reset_btn_rect.collidepoint(mx, my):\n",
    "                        reset_params()\n",
    "\n",
    "                    # 3. Phase L/R Buttons (Only active in Stage 0)\n",
    "                    elif stage == 0:\n",
    "                        if phase_left_btn_rect.collidepoint(mx, my):\n",
    "                            phase_idx = max(1, phase_idx - 1)\n",
    "                        elif phase_right_btn_rect.collidepoint(mx, my):\n",
    "                            phase_idx = min(5, phase_idx + 1)\n",
    "                            \n",
    "                    # 4. Algo L/R Buttons (Only active in Stage 1)\n",
    "                    elif stage == 1:\n",
    "                        if algo_left_btn_rect.collidepoint(mx, my):\n",
    "                            algo_idx = (algo_idx - 1) % len(algo_choices)\n",
    "                        elif algo_right_btn_rect.collidepoint(mx, my):\n",
    "                            algo_idx = (algo_idx + 1) % len(algo_choices)\n",
    "                        \n",
    "                    # 5. Slider drag activation\n",
    "                    for k, meta in knobs.items():\n",
    "                        if meta['rect'].collidepoint(mx, my):\n",
    "                            dragging = k\n",
    "                            knob_center_x = meta['rect'].centerx\n",
    "                            drag_offset = mx - knob_center_x\n",
    "                            break\n",
    "\n",
    "            elif event.type == pygame.MOUSEBUTTONUP:\n",
    "                dragging = None\n",
    "                drag_offset = 0\n",
    "\n",
    "            elif event.type == pygame.MOUSEMOTION and dragging:\n",
    "                mx,my = pygame.mouse.get_pos()\n",
    "                meta = knobs.get(dragging)\n",
    "                if meta is None:\n",
    "                    dragging = None\n",
    "                    continue\n",
    "\n",
    "                x, w = meta['x'], meta['w']\n",
    "                min_v, max_v = meta['min'], meta['max']\n",
    "                stick_x = clamp(mx - drag_offset, x, x + w)\n",
    "                rel = (stick_x - x) / float(w) if w != 0 else 0.0\n",
    "                new_val = round(min_v + rel * (max_v - min_v), 2)\n",
    "                new_val = clamp(new_val, min_v, max_v)\n",
    "                params[meta['param_key']] = new_val\n",
    "\n",
    "                knob_x = int(x + ((new_val - min_v) / (max_v - min_v if (max_v-min_v)!=0 else 1e-6)) * w)\n",
    "                knob_r = meta.get('knob_r', 10)\n",
    "                meta_rect = pygame.Rect(knob_x - knob_r, meta['y_center'] - knob_r, knob_r*2, knob_r*2)\n",
    "                meta['rect'] = meta_rect\n",
    "                knobs[dragging] = meta\n",
    "\n",
    "        # --- Draw UI ---\n",
    "        screen.fill((137,207,240))\n",
    "        \n",
    "        # 1. MAIN APPLICATION HEADER\n",
    "        main_header = header_font.render('LOCK N KEY', True, (20,20,20))\n",
    "        # Draw it at the top, slightly left-aligned (e.g., y=10)\n",
    "        screen.blit(main_header, (280, 10))\n",
    "        \n",
    "        # New logic to select and render the title based on the current stage\n",
    "        step_titles = [\n",
    "            'Step 1 - Select Phase (Difficulty)',\n",
    "            'Step 2 - Choose Algorithm',\n",
    "            'Step 3 - Adjust Parameters'  # The 'Episodes' and 'Parameters' are both in stage 2\n",
    "        ]\n",
    "        \n",
    "        # Use stage=0, 1, or 2 to index the list\n",
    "        current_title_text = step_titles[stage] if stage < len(step_titles) else 'Lock N Key — Setup'\n",
    "        title = font.render(current_title_text, True, (20,20,20))\n",
    "        \n",
    "        # This will now draw the correct, stage-dependent title\n",
    "        screen.blit(title, (280,55))\n",
    "\n",
    "        stage_texts = ['Select Phase (ENTER next)', 'Select Algorithm (ENTER next)', 'Tune Params / Start (ENTER start)']\n",
    "        screen.blit(small.render(stage_texts[stage], True, (60,60,60)), (280,90))\n",
    "        \n",
    "        # --- GLOBAL NAVIGATION BUTTONS ---\n",
    "        if stage < 2:\n",
    "            draw_button(screen, next_stage_btn_rect, \"Next\", font, color=(150, 255, 150))\n",
    "        if stage > 0:\n",
    "            draw_button(screen, back_stage_btn_rect, \"Back\", font, color=(255, 150, 150))\n",
    "      \n",
    "        # Phase box\n",
    "        phase_box = pygame.Rect(280, 130, 420, 80)\n",
    "\n",
    "        # 1. Determine the background color (already dark blue when active)\n",
    "        phase_fill_color = (29, 39, 168) if stage == 0 else (240, 240, 240)\n",
    "\n",
    "        # 2. Determine the text color (White when selected/active, Black otherwise)\n",
    "        phase_text_color = (255, 255, 255) if stage == 0 else (0, 0, 0)\n",
    "        desc_color = (255, 255, 255) if stage == 0 else (40, 40, 40)\n",
    "\n",
    "        pygame.draw.rect(screen, phase_fill_color, phase_box)\n",
    "        pygame.draw.rect(screen, (0, 0, 0), phase_box, 2)\n",
    "\n",
    "        # Use the determined text color for the Phase title\n",
    "        ptxt = font.render(f\"Phase: {phase_idx}\", True, phase_text_color)\n",
    "        screen.blit(ptxt, (phase_box.x + 20, phase_box.y + 20))\n",
    "\n",
    "        descs = {\n",
    "            1: \"Fixed positions (no enemy)\",\n",
    "            2: \"Random agent/key, fixed lock\",\n",
    "            3: \"Random agent/key/lock\",\n",
    "            4: \"Randomized tuning (like Phase 3)\",\n",
    "            5: \"Adds moving enemy hazard\"\n",
    "        }\n",
    "\n",
    "        screen.blit(small.render(descs[phase_idx], True, desc_color), (phase_box.x + 20, phase_box.y + 48))\n",
    "\n",
    "        # Phase L/R buttons: ONLY DRAWN IN STAGE 0\n",
    "        if stage == 0:\n",
    "            draw_button(screen, phase_left_btn_rect, \"<\", font, color=(200, 200, 255))\n",
    "            draw_button(screen, phase_right_btn_rect, \">\", font, color=(200, 200, 255))\n",
    "        \n",
    "        # Algorithm selection\n",
    "        algo_box = pygame.Rect(280, 220, 420, 80)\n",
    "\n",
    "        # 1. Determine the background color for the fill\n",
    "        algo_fill_color = (29, 39, 168) if stage == 1 else (240, 240, 240)\n",
    "\n",
    "        # 2. Determine the text color based on selection state (White when selected, Black when not)\n",
    "        algo_text_color = (255, 255, 255) if stage == 1 else (0, 0, 0) # White if selected, Black otherwise\n",
    "\n",
    "        pygame.draw.rect(screen, algo_fill_color, algo_box)\n",
    "        pygame.draw.rect(screen, (0, 0, 0), algo_box, 2)\n",
    "\n",
    "        # Use the determined text color here\n",
    "        algo_txt = font.render(f\"Algorithm: {algo_choices[algo_idx]}\", True, algo_text_color)\n",
    "        screen.blit(algo_txt, (algo_box.x + 20, algo_box.y + 20))\n",
    "        \n",
    "        # Algo L/R buttons: ONLY DRAWN IN STAGE 1\n",
    "        if stage == 1:\n",
    "            draw_button(screen, algo_left_btn_rect, \"<\", font, color=(200, 200, 255))\n",
    "            draw_button(screen, algo_right_btn_rect, \">\", font, color=(200, 200, 255))\n",
    "\n",
    "        # Sliders\n",
    "        algo = algo_choices[algo_idx]\n",
    "        knobs.clear()\n",
    "        if stage == 2:\n",
    "            y = 330\n",
    "            track_x = 250\n",
    "            track_w = 480\n",
    "\n",
    "            if algo == 'Q-Learning':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params['alpha'], 0.001, 1.0, \"Alpha (Learning Rate)\", font)\n",
    "                knobs['alpha'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'alpha', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['epsilon'], 0.0, 1.0, \"Epsilon (Exploration)\", font)\n",
    "                knobs['epsilon'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.0, 'max':1.0, 'param_key':'epsilon', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "            elif algo == 'Monte Carlo':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params.get('MC_LR', 0.05), 0.001, 1.0, \"MC Learning Rate\", font)\n",
    "                knobs['MC_LR'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'MC_LR', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['epsilon'], 0.0, 1.0, \"Epsilon (Exploration)\", font)\n",
    "                knobs['epsilon'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.0, 'max':1.0, 'param_key':'epsilon', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "            elif algo == 'Actor-Critic':\n",
    "                r = draw_slider(screen, track_x, y+20, track_w, params['alpha'], 0.001, 1.0, \"Alpha (Critic LR)\", font)\n",
    "                knobs['alpha'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'alpha', 'y_center': (y+20)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+80, track_w, params['beta'], 0.001, 1.0, \"Beta (Actor LR)\", font)\n",
    "                knobs['beta'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.001, 'max':1.0, 'param_key':'beta', 'y_center': (y+80)+3, 'knob_r':10}\n",
    "\n",
    "                r = draw_slider(screen, track_x, y+140, track_w, params['gamma'], 0.5, 0.99, \"Gamma (Discount)\", font)\n",
    "                knobs['gamma'] = {'rect': r, 'x': track_x, 'w': track_w, 'min':0.5, 'max':0.99, 'param_key':'gamma', 'y_center': (y+140)+3, 'knob_r':10}\n",
    "\n",
    "        # Episodes input\n",
    "        draw_input_box(screen, input_rect, params['episodes'], font, active_box)\n",
    "        screen.blit(small.render(\"Episodes:\", True, (0,0,0)), (280, 490))\n",
    "        \n",
    "        # Start Button\n",
    "        draw_button(screen, start_btn_rect, \"START\", font, color=(150, 255, 150))\n",
    "        \n",
    "        # Reset Button (Always visible)\n",
    "        draw_button(screen, reset_btn_rect, \"RESET\", font, color=(255, 150, 150))\n",
    "        \n",
    "        # Hint text\n",
    "        screen.blit(small.render('<-/-> to change • ENTER = Next/Start • BACKSPACE = Back • Drag sliders to adjust', True, (60,60,60)), (130, 600))\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(30)\n",
    "\n",
    "        if start_sim:\n",
    "            pygame.quit()\n",
    "            params['episodes'] = int(params['episodes']) if str(params['episodes']).isdigit() else 500\n",
    "            params['phase'] = phase_idx\n",
    "            return algo_choices[algo_idx], params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231edaf",
   "metadata": {},
   "source": [
    "## 4 ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642a05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 4: Simplified In-notebook Environment (with Image Placeholders + Speed Control + Distance Reward + Trailing Enemy + Random Obstacles Phase 4/5) -----------------\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os, sys, random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "# Safe init for pygame (avoids repeated initialization errors in Jupyter)\n",
    "def pygame_safe_init():\n",
    "    if not pygame.get_init():\n",
    "        pygame.init()\n",
    "        pygame.display.init()\n",
    "\n",
    "class LockKeyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Lock & Key Puzzle Environment\n",
    "    Phase 4–5 updated:\n",
    "      - Random obstacles spawn each reset (with restrictions)\n",
    "      - Obstacles never block or overlap key, door, agent, or zombie\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 5}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\", size=6, phase=1, seed=None):\n",
    "        super().__init__()\n",
    "        assert phase in (1,2,3,4,5), \"Phase must be 1–5\"\n",
    "        self.size = size\n",
    "        self.phase = phase\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Window and grid parameters\n",
    "        self.window_size = 800\n",
    "        self.grid_size = 500\n",
    "        self.info_width = self.window_size - self.grid_size\n",
    "        self.cell_size = self.grid_size // self.size\n",
    "\n",
    "        # Spaces\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Discrete(self.size**6)\n",
    "\n",
    "        # Default object states\n",
    "        self._default_agent_pos = np.array([5, 0])\n",
    "        self._default_key_pos = np.array([5, 3])\n",
    "        self._default_lock_pos = np.array([0, 5])\n",
    "        self.default_walls = {(0,2),(1,1),(1,4),(2,3),(3,0),(3,2),(4,4)}  # baseline pattern\n",
    "        self.walls = deepcopy(self.default_walls)\n",
    "\n",
    "        # Enemy\n",
    "        self.enemy_pos = None\n",
    "        self.enemy_active = (phase == 5)\n",
    "        self.trail = deque(maxlen=4)\n",
    "\n",
    "        # State variables\n",
    "        self.agent_pos = None\n",
    "        self.key_pos = None\n",
    "        self.lock_pos = None\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.last_reward = 0\n",
    "        self.current_episode = 0\n",
    "\n",
    "        # Distance trackers\n",
    "        self.prev_enemy_dist = None\n",
    "        self.prev_key_dist = None\n",
    "        self.prev_door_dist = None\n",
    "\n",
    "        # Rendering setup\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # Speed control\n",
    "        self.framerate = 5\n",
    "        self.unlimited_fps = False\n",
    "        self.speed_multiplier = 1.0\n",
    "        self.speed_levels = [0.5, 1, 2, 5, 10, 20, 30, 40, 50, 60]\n",
    "        self.current_speed_idx = 1\n",
    "\n",
    "        # Episode control\n",
    "        self.max_steps = 200\n",
    "\n",
    "        # Image placeholders\n",
    "        self.player_img = None\n",
    "        self.key_img = None\n",
    "        self.door_img = None\n",
    "        self.enemy_img = None\n",
    "        self.obstacle_img = None\n",
    "\n",
    "        # Reward config\n",
    "        self.STEP_PENALTY = -0.1\n",
    "        self.KEY_REWARD = 10.0\n",
    "        self.DOOR_REWARD = 20.0\n",
    "        self.CAUGHT_PENALTY = -15.0\n",
    "        self.SURVIVAL_BONUS = 0.00\n",
    "        self.DIST_SCALE = 0.05\n",
    "        self.APPROACH_SCALE = 0.1\n",
    "\n",
    "\n",
    "    # ---------- IMAGE LOADING ----------\n",
    "    def _load_images(self):\n",
    "        pygame_safe_init()\n",
    "        if not pygame.display.get_init() or pygame.display.get_surface() is None:\n",
    "            pygame.display.set_mode((1, 1))\n",
    "\n",
    "        cwd = os.getcwd()\n",
    "        print(f\"[INFO] Looking for images in: {cwd}\")\n",
    "\n",
    "        def load_img(name):\n",
    "            try:\n",
    "                if os.path.exists(name):\n",
    "                    img = pygame.image.load(name).convert_alpha()\n",
    "                    print(f\"[OK] Loaded {name}\")\n",
    "                    return img\n",
    "                else:\n",
    "                    print(f\"[WARNING] {name} not found — using placeholder.\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to load {name}: {e}\")\n",
    "                return None\n",
    "\n",
    "        self.player_img = load_img(\"player.png\")\n",
    "        self.key_img = load_img(\"key.png\")\n",
    "        self.door_img = load_img(\"door.png\")\n",
    "        self.enemy_img = load_img(\"enemy.png\")\n",
    "        self.obstacle_img = load_img(\"obstacle.png\")\n",
    "\n",
    "\n",
    "    # ---------- RANDOM OBSTACLE GENERATION ----------\n",
    "    def _generate_random_obstacles(self, exclude):\n",
    "        max_obstacles = 8  # adjustable\n",
    "        obstacles = set()\n",
    "        attempts = 0\n",
    "        while len(obstacles) < len(self.default_walls) and attempts < 100:\n",
    "            r, c = self._rng.integers(0, self.size, size=2)\n",
    "            if (r, c) not in exclude and (r, c) not in obstacles:\n",
    "                obstacles.add((r, c))\n",
    "            attempts += 1\n",
    "        return obstacles\n",
    "\n",
    "    def _is_path_clear(self, agent, key, lock, obstacles):\n",
    "        # BFS check that ensures key and door are reachable\n",
    "        grid = np.zeros((self.size, self.size), dtype=int)\n",
    "        for r, c in obstacles:\n",
    "            grid[r, c] = 1\n",
    "        directions = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "\n",
    "        def bfs(start, goal):\n",
    "            q = [tuple(start)]\n",
    "            visited = set(q)\n",
    "            while q:\n",
    "                r, c = q.pop(0)\n",
    "                if (r, c) == tuple(goal):\n",
    "                    return True\n",
    "                for dr, dc in directions:\n",
    "                    rr, cc = r+dr, c+dc\n",
    "                    if 0 <= rr < self.size and 0 <= cc < self.size and grid[rr, cc] == 0 and (rr, cc) not in visited:\n",
    "                        visited.add((rr, cc))\n",
    "                        q.append((rr, cc))\n",
    "            return False\n",
    "\n",
    "        return bfs(agent, key) and bfs(key, lock)\n",
    "\n",
    "\n",
    "    # ---------- RESET ----------\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Random placements depending on phase\n",
    "        if self.phase == 1:\n",
    "            self.walls = deepcopy(self.default_walls)\n",
    "            self.agent_pos = self._default_agent_pos.copy()\n",
    "            self.key_pos = self._default_key_pos.copy()\n",
    "            self.lock_pos = self._default_lock_pos.copy()\n",
    "\n",
    "        elif self.phase == 2:\n",
    "            self.walls = deepcopy(self.default_walls)\n",
    "            self.lock_pos = self._default_lock_pos.copy()\n",
    "            self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "            self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "\n",
    "        elif self.phase == 3:\n",
    "            self.walls = deepcopy(self.default_walls)\n",
    "            self.lock_pos = self._random_free_cell()\n",
    "            self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "            self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "\n",
    "        elif self.phase == 4:\n",
    "            # Random obstacles, agent, key, door (no zombie)\n",
    "            valid = False\n",
    "            while not valid:\n",
    "                self.lock_pos = self._random_free_cell()\n",
    "                self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "                self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "                exclude = {tuple(self.lock_pos), tuple(self.agent_pos), tuple(self.key_pos)}\n",
    "                self.walls = self._generate_random_obstacles(exclude)\n",
    "                valid = self._is_path_clear(self.agent_pos, self.key_pos, self.lock_pos, self.walls)\n",
    "\n",
    "        elif self.phase == 5:\n",
    "            # Random obstacles + zombie + ensure connectivity\n",
    "            valid = False\n",
    "            while not valid:\n",
    "                self.lock_pos = self._random_free_cell()\n",
    "                self.agent_pos = self._random_free_cell(exclude={tuple(self.lock_pos)})\n",
    "                self.key_pos = self._random_free_cell(exclude={tuple(self.lock_pos), tuple(self.agent_pos)})\n",
    "                exclude_main = {tuple(self.lock_pos), tuple(self.agent_pos), tuple(self.key_pos)}\n",
    "                self.walls = self._generate_random_obstacles(exclude_main)\n",
    "                valid = self._is_path_clear(self.agent_pos, self.key_pos, self.lock_pos, self.walls)\n",
    "            exclude_enemy = exclude_main.union(self.walls)\n",
    "            self.enemy_pos = self._random_free_cell(exclude=exclude_enemy)\n",
    "            self.enemy_active = True\n",
    "            self.trail.clear()\n",
    "\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.last_reward = 0\n",
    "        self.prev_enemy_dist = self._manhattan(self.agent_pos, self.enemy_pos) if self.enemy_active else None\n",
    "        self.prev_key_dist = self._manhattan(self.agent_pos, self.key_pos)\n",
    "        self.prev_door_dist = self._manhattan(self.agent_pos, self.lock_pos)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return obs, {}\n",
    "\n",
    "\n",
    "    # ---------- STEP ----------\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        reward = self.STEP_PENALTY\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        move_map = {0:(1,0), 1:(-1,0), 2:(0,1), 3:(0,-1)}\n",
    "        if action in move_map:\n",
    "            dr, dc = move_map[action]\n",
    "            new_pos = self.agent_pos + np.array([dr, dc])\n",
    "            if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and (tuple(new_pos) not in self.walls):\n",
    "                self.trail.appendleft(self.agent_pos.copy())\n",
    "                self.agent_pos = new_pos\n",
    "\n",
    "        key_dist = self._manhattan(self.agent_pos, self.key_pos)\n",
    "        door_dist = self._manhattan(self.agent_pos, self.lock_pos)\n",
    "\n",
    "        if not self.has_key:\n",
    "            delta = self.prev_key_dist - key_dist\n",
    "            reward += self.APPROACH_SCALE * delta\n",
    "            self.prev_key_dist = key_dist\n",
    "        else:\n",
    "            delta = self.prev_door_dist - door_dist\n",
    "            reward += self.APPROACH_SCALE * delta\n",
    "            self.prev_door_dist = door_dist\n",
    "\n",
    "        if not self.has_key and np.array_equal(self.agent_pos, self.key_pos):\n",
    "            self.has_key = True\n",
    "            reward += self.KEY_REWARD\n",
    "\n",
    "        if self.has_key and np.array_equal(self.agent_pos, self.lock_pos):\n",
    "            reward += self.DOOR_REWARD\n",
    "            terminated = True\n",
    "\n",
    "        if self.phase == 5 and self.enemy_active:\n",
    "            if len(self.trail) >= 3:\n",
    "                target = self.trail[2]\n",
    "                self.enemy_pos = target.copy()\n",
    "            else:\n",
    "                self._move_enemy_random()\n",
    "            dist_now = self._manhattan(self.agent_pos, self.enemy_pos)\n",
    "            if self.prev_enemy_dist is not None:\n",
    "                delta = dist_now - self.prev_enemy_dist\n",
    "                reward += self.DIST_SCALE * delta\n",
    "            self.prev_enemy_dist = dist_now\n",
    "            if np.array_equal(self.enemy_pos, self.agent_pos):\n",
    "                reward += self.CAUGHT_PENALTY\n",
    "                terminated = True\n",
    "\n",
    "        if not terminated:\n",
    "            reward += self.SURVIVAL_BONUS\n",
    "\n",
    "        info = {}\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "            info['timeout'] = True\n",
    "\n",
    "        if terminated:\n",
    "            if self.phase == 5 and self.enemy_active and np.array_equal(self.enemy_pos, self.agent_pos):\n",
    "                info['caught'] = True\n",
    "            elif self.has_key and np.array_equal(self.agent_pos, self.lock_pos):\n",
    "                info['unlocked'] = True\n",
    "\n",
    "        self.last_reward = reward\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    # ---------- HELPERS ----------\n",
    "    def _random_free_cell(self, exclude=None):\n",
    "        if exclude is None:\n",
    "            exclude = set()\n",
    "        while True:\n",
    "            r, c = self._rng.integers(0, self.size, size=2)\n",
    "            if (r, c) not in exclude and (r, c) not in self.walls:\n",
    "                return np.array([r, c])\n",
    "\n",
    "    def _manhattan(self, p1, p2):\n",
    "        if p1 is None or p2 is None:\n",
    "            return 0\n",
    "        return abs(p1[0]-p2[0]) + abs(p1[1]-p2[1])\n",
    "\n",
    "    def _move_enemy_random(self):\n",
    "        directions = [(1,0),(-1,0),(0,1),(0,-1),(0,0)]\n",
    "        self._rng.shuffle(directions)\n",
    "        for dr, dc in directions:\n",
    "            cand = self.enemy_pos + np.array([dr, dc])\n",
    "            rr, cc = int(cand[0]), int(cand[1])\n",
    "            if 0 <= rr < self.size and 0 <= cc < self.size and (rr, cc) not in self.walls:\n",
    "                self.enemy_pos = np.array([rr, cc])\n",
    "                return\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([*self.agent_pos, *self.key_pos, *self.lock_pos, int(self.has_key)])\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------- RENDER ----------\n",
    "    def _render_frame(self):\n",
    "        pygame_safe_init()\n",
    "        \n",
    "        if self.player_img is None:\n",
    "            self._load_images()\n",
    "        if self.window is None:\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.grid_size))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.grid_size, self.grid_size))\n",
    "        canvas.fill((137,207,240))\n",
    "\n",
    "        for x in range(self.size+1):\n",
    "            pygame.draw.line(canvas, (0,0,0), (0,x*self.cell_size), (self.grid_size,x*self.cell_size), 1)\n",
    "            pygame.draw.line(canvas, (0,0,0), (x*self.cell_size,0), (x*self.cell_size,self.grid_size), 1)\n",
    "\n",
    "        for r,c in self.walls:\n",
    "            rect = pygame.Rect(c*self.cell_size, r*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.obstacle_img:\n",
    "                scaled = pygame.transform.scale(self.obstacle_img, (self.cell_size,self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.rect(canvas,(100,100,100),rect)\n",
    "\n",
    "        if not self.has_key:\n",
    "            rect = pygame.Rect(self.key_pos[1]*self.cell_size, self.key_pos[0]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            if self.key_img:\n",
    "                scaled = pygame.transform.scale(self.key_img,(self.cell_size,self.cell_size))\n",
    "                canvas.blit(scaled, rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.circle(canvas,(255,215,0),rect.center,max(6,self.cell_size//4))\n",
    "\n",
    "        rect = pygame.Rect(self.lock_pos[1]*self.cell_size,self.lock_pos[0]*self.cell_size,self.cell_size,self.cell_size)\n",
    "        if self.door_img:\n",
    "            scaled = pygame.transform.scale(self.door_img,(self.cell_size,self.cell_size))\n",
    "            canvas.blit(scaled,rect.topleft)\n",
    "        else:\n",
    "            pygame.draw.rect(canvas,(200,50,50),rect)\n",
    "\n",
    "        rect = pygame.Rect(self.agent_pos[1]*self.cell_size,self.agent_pos[0]*self.cell_size,self.cell_size,self.cell_size)\n",
    "        if self.player_img:\n",
    "            scaled = pygame.transform.scale(self.player_img,(self.cell_size,self.cell_size))\n",
    "            canvas.blit(scaled,rect.topleft)\n",
    "        else:\n",
    "            pygame.draw.circle(canvas,(50,100,255),rect.center,max(6,self.cell_size//3))\n",
    "\n",
    "        if self.phase==5 and self.enemy_active and self.enemy_pos is not None:\n",
    "            rect = pygame.Rect(self.enemy_pos[1]*self.cell_size,self.enemy_pos[0]*self.cell_size,self.cell_size,self.cell_size)\n",
    "            if self.enemy_img:\n",
    "                scaled = pygame.transform.scale(self.enemy_img,(self.cell_size,self.cell_size))\n",
    "                canvas.blit(scaled,rect.topleft)\n",
    "            else:\n",
    "                pygame.draw.circle(canvas,(255,0,0),rect.center,max(6,self.cell_size//3))\n",
    "\n",
    "        info_panel = pygame.Surface((self.info_width,self.grid_size))\n",
    "        info_panel.fill((137,207,240))\n",
    "        \n",
    "        # Define fonts\n",
    "        font = pygame.font.SysFont(\"verdana\",18)\n",
    "        header_font = pygame.font.SysFont(\"verdana\", 24, bold=True) # New, larger font for the header\n",
    "\n",
    "        # --- DRAW HEADER ---\n",
    "        header_text = header_font.render(\"LOCK N KEY\", True, (0, 0, 0))\n",
    "        # Draw the header at the top of the info panel (e.g., at Y=10)\n",
    "        info_panel.blit(header_text, (10, 10))\n",
    "        \n",
    "        # Starting Y position for the status lines, accounting for the header\n",
    "        y_start_status = 50 \n",
    "        line_spacing = 30 # Use consistent line spacing\n",
    "        \n",
    "        lines=[\n",
    "            f\"Phase: {self.phase}\",\n",
    "            f\"Episode: {self.current_episode}\",\n",
    "            \"\",\n",
    "            f\"Steps: {self.steps}\",\n",
    "            f\"Reward: {round(self.last_reward,2)}\",\n",
    "            f\"Has Key: {'Yes' if self.has_key else 'No'}\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            f\"Speed: ×{self.speed_multiplier}\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"Controls:\",\n",
    "            \"  UP: Increase Speed\",\n",
    "            \"  DOWN: Decrease Speed\",\n",
    "            \"  Close Window/ESC to Quit\",\n",
    "        ]\n",
    "        \n",
    "        for i,text in enumerate(lines):\n",
    "            # Calculate Y position: start Y + index * spacing\n",
    "            y_pos = y_start_status + i * line_spacing\n",
    "            info_panel.blit(font.render(text,True,(0,0,0)),(10, y_pos))\n",
    "            \n",
    "        self.window.blit(canvas,(0,0))\n",
    "        self.window.blit(info_panel,(self.grid_size,0))\n",
    "        pygame.display.flip()\n",
    "\n",
    "        if not self.unlimited_fps:\n",
    "            effective_fps=max(1,int(self.framerate*self.speed_multiplier))\n",
    "            self.clock.tick(effective_fps)\n",
    "\n",
    "    # ---------- SPEED CONTROL ----------\n",
    "    def _increase_speed(self):\n",
    "        if self.current_speed_idx < len(self.speed_levels)-1:\n",
    "            self.current_speed_idx += 1\n",
    "            self.speed_multiplier = self.speed_levels[self.current_speed_idx]\n",
    "            print(f\"[INFO] Increased speed to ×{self.speed_multiplier}\")\n",
    "\n",
    "    def _decrease_speed(self):\n",
    "        if self.current_speed_idx > 0:\n",
    "            self.current_speed_idx -= 1\n",
    "            self.speed_multiplier = self.speed_levels[self.current_speed_idx]\n",
    "            print(f\"[INFO] Decreased speed to ×{self.speed_multiplier}\")\n",
    "            \n",
    "    # ---------- EVENT HANDLER ----------\n",
    "    def handle_events(self, events):\n",
    "        for event in events:\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    self._increase_speed()\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self._decrease_speed()\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1d161",
   "metadata": {},
   "source": [
    "## 5 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f0a1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CELL 5: Run the whole flow (menu -> train). Training loop no plotting. -----------------\n",
    "def train_and_visualize(algo_name, params, save_path=\"lockkey_policy.pkl\"):\n",
    "    pygame_safe_init()\n",
    "\n",
    "    # --- Environment setup (now using the in-notebook LockKeyEnv) ---\n",
    "    env = LockKeyEnv(render_mode='human', size=6, phase=int(params.get('phase', 5)))\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    episodes = int(params.get('episodes', 800))\n",
    "    gamma = float(params.get('gamma', 0.9))\n",
    "\n",
    "    # --- Agent creation ---\n",
    "    if algo_name == 'Q-Learning':\n",
    "        agent = QLearningAgent(n_states, n_actions,\n",
    "                               alpha=float(params.get('alpha', 0.1)),\n",
    "                               gamma=gamma,\n",
    "                               epsilon=float(params.get('epsilon', 0.2)))\n",
    "    elif algo_name == 'Monte Carlo':\n",
    "        agent = MonteCarloAgent(n_states, n_actions,\n",
    "                                gamma=gamma,\n",
    "                                epsilon=float(params.get('epsilon', 0.1)))\n",
    "        # if MC has a learning rate param, store it on agent (optional usage)\n",
    "        if 'MC_LR' in params:\n",
    "            agent.mc_lr = float(params.get('MC_LR', 0.05))\n",
    "    else:  # Actor-Critic\n",
    "        agent = ActorCriticAgent(n_states, n_actions,\n",
    "                                 alpha=float(params.get('alpha', 0.1)),   # critic lr\n",
    "                                 beta=float(params.get('beta', 0.01)),    # actor lr\n",
    "                                 gamma=gamma)\n",
    "\n",
    "    episode_rewards, successes, episode_lengths = [], [], []\n",
    "\n",
    "    pygame_safe_init()\n",
    "    font = pygame.font.SysFont('arial', 18)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # --- Main training loop ---\n",
    "        for ep in range(1, episodes + 1):\n",
    "            # fetch events once now; pass them into env later each frame\n",
    "            obs, _ = env.reset()\n",
    "            env.current_episode = ep\n",
    "            done = False\n",
    "            total_r = 0\n",
    "            steps = 0\n",
    "            episode_hist = []\n",
    "\n",
    "            if isinstance(obs, tuple) and len(obs) == 2:\n",
    "                obs = obs[0]\n",
    "\n",
    "            while not done and steps < 200:\n",
    "                # fetch events once per frame\n",
    "                events = pygame.event.get()\n",
    "                # handle quit/escape centrally (avoid consuming events)\n",
    "                for ev in events:\n",
    "                    if ev.type == pygame.QUIT:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "                    elif ev.type == pygame.KEYDOWN and ev.key == pygame.K_ESCAPE:\n",
    "                        env.close(); pygame.quit(); raise SystemExit()\n",
    "\n",
    "                # let environment consume events it needs (speed keys)\n",
    "                env.handle_events(events)\n",
    "\n",
    "                # Agent selects action\n",
    "                s = obs\n",
    "                a = agent.select_action(s) if hasattr(agent, 'select_action') else agent.policy(s)\n",
    "                obs2, r, done, trunc, info = env.step(a)\n",
    "                total_r += r\n",
    "                steps += 1\n",
    "\n",
    "                if isinstance(agent, MonteCarloAgent):\n",
    "                    episode_hist.append((s, a, r))\n",
    "                else:\n",
    "                    agent.update(s, a, r, obs2, done)\n",
    "\n",
    "                obs = obs2\n",
    "\n",
    "                # Render env (HUD handled by env._render_frame)\n",
    "                try:\n",
    "                    env.render()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Console updates occasionally\n",
    "                if steps % 30 == 0:\n",
    "                    print(f\"Ep {ep} | Step {steps:03d} | Reward={total_r:.2f} | Eps={getattr(agent, 'epsilon', 0):.2f}\", end='\\r')\n",
    "\n",
    "            # End of episode\n",
    "            if isinstance(agent, MonteCarloAgent):\n",
    "                agent.update(episode_hist)\n",
    "\n",
    "            if hasattr(agent, 'epsilon'):\n",
    "                agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
    "\n",
    "            episode_rewards.append(total_r)\n",
    "            successes.append(1 if done and total_r > 0 else 0)\n",
    "            episode_lengths.append(steps)\n",
    "            \n",
    "            # Detect episode outcome\n",
    "            if env.phase == 5 and env.enemy_active and np.array_equal(env.agent_pos, env.enemy_pos):\n",
    "                outcome = \"❌ Caught by Enemy\"\n",
    "            elif np.array_equal(env.agent_pos, env.lock_pos) and env.has_key:\n",
    "                outcome = \"🏁 Door Unlocked\"\n",
    "            elif not env.has_key and np.array_equal(env.agent_pos, env.key_pos):\n",
    "                outcome = \"🔑 Picked up Key\"\n",
    "            else:\n",
    "                outcome = \"🌀 Episode Ended (Timeout or Random Termination)\"\n",
    "\n",
    "            print(f\"✅ Episode {ep}/{episodes} finished | Reward={total_r:.2f} | Steps={steps} | {outcome}\")\n",
    "\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            env.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Save policy (unchanged) ---\n",
    "    try:\n",
    "        def safe_convert(obj):\n",
    "            if isinstance(obj, defaultdict):\n",
    "                obj = {k: safe_convert(v) for k, v in obj.items()}\n",
    "            return obj\n",
    "\n",
    "        data_to_save = None\n",
    "        if algo_name in ['Q-Learning', 'Monte Carlo']:\n",
    "            data_to_save = safe_convert(agent.Q)\n",
    "        else:\n",
    "            data_to_save = {'V': safe_convert(agent.V), 'pi': safe_convert(agent.pi)}\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(data_to_save, f)\n",
    "        print(f\"✅ Saved {algo_name} policy successfully to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not save policy due to {e}\")\n",
    "\n",
    "    return episode_rewards, successes, episode_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20c6e5",
   "metadata": {},
   "source": [
    "## 6 RUN WHOLE FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67d7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen algorithm: Q-Learning\n",
      "Parameters: {'episodes': 20, 'alpha': 0.1, 'beta': 0.1, 'gamma': 0.9, 'epsilon': 0.2, 'MC_LR': 0.05, 'phase': 1}\n",
      "[INFO] Looking for images in: c:\\Users\\renza\\Documents\\GitHub\\RL_PROJECT\\LOCK_KEY\n",
      "[OK] Loaded player.png\n",
      "[OK] Loaded key.png\n",
      "[OK] Loaded door.png\n",
      "[OK] Loaded enemy.png\n",
      "[OK] Loaded obstacle.png\n",
      "[INFO] Increased speed to ×2\n",
      "[INFO] Increased speed to ×50 | Eps=0.20\n",
      "[INFO] Increased speed to ×10\n",
      "[INFO] Increased speed to ×20\n",
      "✅ Episode 1/20 finished | Reward=26.50 | Steps=48 | 🏁 Door Unlocked\n",
      "[INFO] Increased speed to ×30\n",
      "✅ Episode 2/20 finished | Reward=30.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 3/20 finished | Reward=29.40 | Steps=19 | 🏁 Door Unlocked\n",
      "✅ Episode 4/20 finished | Reward=30.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 5/20 finished | Reward=29.50 | Steps=18 | 🏁 Door Unlocked\n",
      "✅ Episode 6/20 finished | Reward=30.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 7/20 finished | Reward=30.00 | Steps=13 | 🏁 Door Unlocked\n",
      "[INFO] Increased speed to ×40\n",
      "✅ Episode 8/20 finished | Reward=29.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 9/20 finished | Reward=30.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 10/20 finished | Reward=29.80 | Steps=15 | 🏁 Door Unlocked\n",
      "✅ Episode 11/20 finished | Reward=29.50 | Steps=18 | 🏁 Door Unlocked\n",
      "[INFO] Increased speed to ×50\n",
      "✅ Episode 12/20 finished | Reward=30.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 13/20 finished | Reward=30.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 14/20 finished | Reward=30.10 | Steps=12 | 🏁 Door Unlocked\n",
      "[INFO] Increased speed to ×60\n",
      "✅ Episode 15/20 finished | Reward=29.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Episode 16/20 finished | Reward=30.00 | Steps=13 | 🏁 Door Unlocked\n",
      "✅ Episode 17/20 finished | Reward=30.20 | Steps=11 | 🏁 Door Unlocked\n",
      "✅ Episode 18/20 finished | Reward=30.10 | Steps=12 | 🏁 Door Unlocked\n",
      "✅ Episode 19/20 finished | Reward=30.30 | Steps=10 | 🏁 Door Unlocked\n",
      "✅ Episode 20/20 finished | Reward=29.90 | Steps=14 | 🏁 Door Unlocked\n",
      "✅ Saved Q-Learning policy successfully to lockkey_policy.pkl\n",
      "Training complete. Saved policy.\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Run the menu and training\n",
    "# Run this cell to open the Pygame menu, choose phase & algorithm & params, then train.\n",
    "\n",
    "algo_name, params = menu_and_params()\n",
    "print(\"Chosen algorithm:\", algo_name)\n",
    "print(\"Parameters:\", params)\n",
    "\n",
    "# Visuals mapping placeholder (not currently used in the env code,\n",
    "# but retained if you want to pass custom colors or images later)\n",
    "visuals = {\n",
    "    'agent_color': (50,100,255),\n",
    "    'key_color': (255,215,0),\n",
    "    'lock_color': (200,50,50),\n",
    "    'wall_color': (80,80,80),\n",
    "    'enemy_color': (0,0,255),\n",
    "    'bg_color': (230,230,230)\n",
    "}\n",
    "\n",
    "# Train (this opens the environment window)\n",
    "rewards, successes, episode_lengths = train_and_visualize(algo_name, params, save_path='lockkey_policy.pkl')\n",
    "print(\"Training complete. Saved policy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51965fa",
   "metadata": {},
   "source": [
    "## 7 RESULTS GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd5646c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# === 1) Plot Learning Curves (Raw + Smoothed Rewards + Success Rate + Episode Lengths) ===\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# ----------------- CELL 7: Results & Comparison (plots + optional summary) -----------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# === 1) Plot Learning Curves (Raw + Smoothed Rewards + Success Rate + Episode Lengths) ===\n",
    "def plot_learning_curves(rewards, successes, episode_lengths, algo_name, smooth_window=10):\n",
    "    episodes = np.arange(1, len(rewards) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.suptitle(f\"{algo_name} - Training Performance\", fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 1️⃣ Raw + Smoothed Reward Curve\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(episodes, rewards, marker='o', alpha=0.6, label='Raw Reward')\n",
    "    if len(rewards) >= smooth_window:\n",
    "        smooth_rewards = np.convolve(rewards, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "        x = np.arange(smooth_window, len(rewards)+1)\n",
    "        plt.plot(x, smooth_rewards, color='orange', linewidth=2, label=f'Smoothed (window={smooth_window})')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Rewards (Raw + Smoothed)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # 2️⃣ Success Rate Curve\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if len(successes) > 0:\n",
    "        success_rate = np.cumsum(successes) / np.arange(1, len(successes)+1)\n",
    "        plt.plot(np.arange(1, len(successes)+1), success_rate, color='green', label='Success Rate')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Success Rate\")\n",
    "    plt.title(\"Success Rate Over Time\")\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 3️⃣ Episode Length Curve\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(episodes, episode_lengths, color='purple', marker='x', alpha=0.6, label='Episode Length')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.title(\"Episode Lengths\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === 2) Extract Best Metrics ===\n",
    "def analyze_best_metrics(rewards, successes, episode_lengths, algo_name, smooth_window=10):\n",
    "    # Raw rewards\n",
    "    best_raw = np.max(rewards)\n",
    "    best_raw_ep = np.argmax(rewards) + 1\n",
    "\n",
    "    # Smoothed rewards\n",
    "    if len(rewards) >= smooth_window:\n",
    "        smooth_rewards = np.convolve(rewards, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "        best_smooth = np.max(smooth_rewards)\n",
    "        best_smooth_ep = np.argmax(smooth_rewards) + smooth_window\n",
    "    else:\n",
    "        best_smooth = np.max(rewards)\n",
    "        best_smooth_ep = np.argmax(rewards) + 1\n",
    "\n",
    "    # Success rate\n",
    "    success_rate = np.cumsum(successes) / np.arange(1, len(successes)+1)\n",
    "    best_success = np.max(success_rate)\n",
    "    best_success_ep = np.argmax(success_rate) + 1\n",
    "\n",
    "    # Episode length\n",
    "    best_length = np.max(episode_lengths)\n",
    "    best_length_ep = np.argmax(episode_lengths) + 1\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "\n",
    "    print(f\"\\n📈 {algo_name} - Best Performance Metrics\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(f\"  🟦 Best Raw Reward: {best_raw:.3f}  (Episode {best_raw_ep})\")\n",
    "    print(f\"  🟧 Best Smoothed Reward: {best_smooth:.3f}  (Episode {best_smooth_ep})\")\n",
    "    print(f\"  🟩 Best Success Rate: {best_success:.3f}  (Episode {best_success_ep})\")\n",
    "    print(f\"  🟪 Longest Episode: {best_length} steps  (Episode {best_length_ep})\")\n",
    "    print(f\"  Average Episode Length: {avg_length:.2f} steps\")\n",
    "\n",
    "\n",
    "# === 3) Run plotting and metrics (use variables from CELL 6) ===\n",
    "plot_learning_curves(rewards, successes, episode_lengths, algo_name)\n",
    "analyze_best_metrics(rewards, successes, episode_lengths, algo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc90e5",
   "metadata": {},
   "source": [
    "# FOR PERCENTAGE OF EVAL (TO BE EDITED PER PHASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Converted Performance to Percentage Form ===\n",
      "       Setting  Best Raw Reward (%)  Best Smoothed Reward (%)  \\\n",
      "0      Default                99.35                     93.04   \n",
      "1  High LR (α)               100.00                     92.39   \n",
      "2   Low LR (α)                99.02                     93.56   \n",
      "3  High ER (ε)                96.08                     84.64   \n",
      "4   Low ER (ε)                99.35                     95.98   \n",
      "5  High DF (γ)               100.00                     94.74   \n",
      "6   Low DF (γ)                99.35                     92.88   \n",
      "\n",
      "   Best Success Rate (%)  Average Episode Length (%)  \n",
      "0                  100.0                       44.13  \n",
      "1                  100.0                       52.63  \n",
      "2                   89.0                       35.74  \n",
      "3                   76.0                        0.00  \n",
      "4                  100.0                       46.09  \n",
      "5                  100.0                       43.86  \n",
      "6                  100.0                       44.66  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Define your max constants (adjust if needed) ===\n",
    "MAX_REWARD = 30.600   # highest achievable reward observed\n",
    "MAX_STEPS = 200     # your environment's timeout step\n",
    "\n",
    "# === 2. Input your metrics per parameter setting ===\n",
    "data = [\n",
    "    {\n",
    "        \"Setting\": \"Default\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 28.470,\n",
    "        \"Best Success Rate\": 1.000,   # already 0–1\n",
    "        \"Average Episode Length\": 56.880\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High LR (α)\",\n",
    "        \"Best Raw Reward\": 30.600,\n",
    "        \"Best Smoothed Reward\": 28.270,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 48.22\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low LR (α)\",\n",
    "        \"Best Raw Reward\": 30.300,\n",
    "        \"Best Smoothed Reward\": 28.630,\n",
    "        \"Best Success Rate\": 0.890,\n",
    "        \"Average Episode Length\": 65.42\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High ER (ε)\",\n",
    "        \"Best Raw Reward\": 29.400,\n",
    "        \"Best Smoothed Reward\": 25.900,\n",
    "        \"Best Success Rate\": 0.760,\n",
    "        \"Average Episode Length\": 101.80\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low ER (ε)\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 29.370,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 54.88\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"High DF (γ)\",\n",
    "        \"Best Raw Reward\": 30.600,\n",
    "        \"Best Smoothed Reward\": 28.990,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 57.15\n",
    "    },\n",
    "    {\n",
    "        \"Setting\": \"Low DF (γ)\",\n",
    "        \"Best Raw Reward\": 30.400,\n",
    "        \"Best Smoothed Reward\": 28.420,\n",
    "        \"Best Success Rate\": 1.000,\n",
    "        \"Average Episode Length\": 56.34\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === 3. Convert to percentage values ===\n",
    "df[\"Best Raw Reward (%)\"] = (df[\"Best Raw Reward\"] / MAX_REWARD) * 100\n",
    "df[\"Best Smoothed Reward (%)\"] = (df[\"Best Smoothed Reward\"] / MAX_REWARD) * 100\n",
    "df[\"Best Success Rate (%)\"] = df[\"Best Success Rate\"] * 100\n",
    "df[\"Average Episode Length (%)\"] = (1 - (df[\"Average Episode Length\"] / MAX_STEPS)) * 100  # efficiency\n",
    "\n",
    "# === 4. Select and display percentage columns only ===\n",
    "percent_df = df[[\"Setting\",\n",
    "                 \"Best Raw Reward (%)\",\n",
    "                 \"Best Smoothed Reward (%)\",\n",
    "                 \"Best Success Rate (%)\",\n",
    "                 \"Average Episode Length (%)\"]]\n",
    "\n",
    "print(\"\\n=== Converted Performance to Percentage Form ===\")\n",
    "print(percent_df.round(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
